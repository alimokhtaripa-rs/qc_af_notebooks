{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== qc_forest2.py ============================\n",
    "\"\"\"\n",
    "Overall data health (Level-1 MWIR) — FOREST-2 + Side-by-Side Visuals\n",
    "--------------------------------------------------------------------\n",
    "QC metrics for a FOREST-2 MWIR Level-1 GeoTIFF + inline previews:\n",
    "\n",
    " • QC table: Metric | Value | Description\n",
    "     • Figures:\n",
    " • Left  panel = Sobel gradient magnitude (p99-stretched, with colorbar; title shows p50/p95/p99)\n",
    " • Right panel = Radiance (grayscale) with Hot pixels (red) and Cold pixels (blue), with grayscale colorbar\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ============================== Imports ===============================\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================== Configuration ============================\n",
    "\n",
    "# Additional NoData values commonly encountered in L1 MWIR GeoTIFFs\n",
    "NO_DATA_CANDIDATES = (-3.40282e38, 65555, 65535)\n",
    "\n",
    "\n",
    "# ============================== Utilities =============================\n",
    "\n",
    "def build_valid_mask(arr: np.ndarray, nodata_tag) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct a boolean mask of valid pixels.\n",
    "\n",
    "    Rules:\n",
    "      • Valid if finite\n",
    "      • Excludes declared nodata_tag (if present and not NaN)\n",
    "      • Excludes common values in NO_DATA_CANDIDATES\n",
    "      • Excludes extreme negative fillers (≤ -1e30)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Input radiance array.\n",
    "    nodata_tag : numeric or None\n",
    "        NoData value declared in the raster metadata.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray (bool)\n",
    "        True for valid pixels, False otherwise.\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(arr)\n",
    "    if nodata_tag is not None and not (isinstance(nodata_tag, float) and math.isnan(nodata_tag)):\n",
    "        mask &= ~(arr == nodata_tag)\n",
    "    for v in NO_DATA_CANDIDATES:\n",
    "        mask &= ~(arr == v)\n",
    "    mask &= ~(arr <= -1e30)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def robust_percentiles(x: np.ndarray, ps=(2, 98)):\n",
    "    \"\"\"Return robust percentiles of a 1D array; NaN-safe for empty inputs.\"\"\"\n",
    "    if x.size == 0:\n",
    "        return [np.nan for _ in ps]\n",
    "    return [float(np.percentile(x, p)) for p in ps]\n",
    "\n",
    "\n",
    "def zscore_1d(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute 1D z-scores with NaN-safe std handling.\"\"\"\n",
    "    m, s = np.nanmean(x), np.nanstd(x)\n",
    "    if not np.isfinite(s) or s == 0:\n",
    "        return (x - m)\n",
    "    return (x - m) / s\n",
    "\n",
    "\n",
    "def count_internal_holes(nodata_mask: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Count connected NoData components that do NOT touch the image border.\n",
    "    \"\"\"\n",
    "    lab, ncomp = ndi.label(nodata_mask)\n",
    "    if ncomp == 0:\n",
    "        return 0\n",
    "    h, w = nodata_mask.shape\n",
    "    touches = set()\n",
    "    touches.update(np.unique(lab[0, :]))\n",
    "    touches.update(np.unique(lab[-1, :]))\n",
    "    touches.update(np.unique(lab[:, 0]))\n",
    "    touches.update(np.unique(lab[:, -1]))\n",
    "    touches.discard(0)\n",
    "    holes = sum((i not in touches) for i in range(1, ncomp + 1))\n",
    "    return int(holes)\n",
    "\n",
    "\n",
    "def internal_holes_mask_and_sizes(nodata_mask: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Identify interior NoData components and return:\n",
    "      • holes_mask: boolean mask of interior NoData pixels\n",
    "      • hole_sizes: array of pixel counts per interior component\n",
    "    \"\"\"\n",
    "    lab, ncomp = ndi.label(nodata_mask)\n",
    "    if ncomp == 0:\n",
    "        return np.zeros_like(nodata_mask, dtype=bool), np.array([], dtype=int)\n",
    "    h, w = nodata_mask.shape\n",
    "    border_labels = set(np.unique(lab[0, :])) | set(np.unique(lab[-1, :])) \\\n",
    "                    | set(np.unique(lab[:, 0])) | set(np.unique(lab[:, -1]))\n",
    "    border_labels.discard(0)\n",
    "    holes_mask = nodata_mask & ~np.isin(lab, list(border_labels))\n",
    "    counts = np.bincount(lab.ravel())\n",
    "    hole_labels = np.setdiff1d(np.arange(1, counts.size),\n",
    "                               np.fromiter(border_labels, dtype=int, count=len(border_labels)))\n",
    "    hole_sizes = counts[hole_labels] if hole_labels.size else np.array([], dtype=int)\n",
    "    return holes_mask, hole_sizes\n",
    "\n",
    "\n",
    "def gradient_percentiles(arr: np.ndarray, mask_valid: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute Sobel gradient magnitude over valid pixels and return percentiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {'grad_p50', 'grad_p95', 'grad_p99'}\n",
    "    \"\"\"\n",
    "    filled = np.where(mask_valid, arr, np.nan)\n",
    "    filled = np.nan_to_num(filled, nan=0.0)\n",
    "    gx = ndi.sobel(filled, axis=1)\n",
    "    gy = ndi.sobel(filled, axis=0)\n",
    "    gm = np.hypot(gx, gy)\n",
    "    vals = gm[mask_valid]\n",
    "    if vals.size == 0:\n",
    "        return dict(grad_p50=np.nan, grad_p95=np.nan, grad_p99=np.nan)\n",
    "    return dict(\n",
    "        grad_p50=float(np.percentile(vals, 50)),\n",
    "        grad_p95=float(np.percentile(vals, 95)),\n",
    "        grad_p99=float(np.percentile(vals, 99)),\n",
    "    )\n",
    "\n",
    "\n",
    "# ====================== Gradient & Hot/Cold Layers =====================\n",
    "\n",
    "def compute_sobel_magnitude(arr: np.ndarray, mask_valid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sobel gradient magnitude (float32), masked to NaN outside valid pixels.\n",
    "    \"\"\"\n",
    "    safe = np.where(mask_valid, arr, np.nan)\n",
    "    safe = np.nan_to_num(safe, nan=0.0)\n",
    "    gx = ndi.sobel(safe, axis=1)\n",
    "    gy = ndi.sobel(safe, axis=0)\n",
    "    gm = np.hypot(gx, gy).astype(np.float32)\n",
    "    gm[~mask_valid] = np.nan\n",
    "    return gm\n",
    "\n",
    "\n",
    "def compute_hot_cold_mask(arr: np.ndarray, mask_valid: np.ndarray, z_thresh: float = 6.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flag hot/cold outliers relative to valid-pixel mean/std.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray (int)\n",
    "        1 for hot (> +z_thresh), -1 for cold (< -z_thresh), 0 otherwise.\n",
    "    \"\"\"\n",
    "    vals = arr[mask_valid]\n",
    "    if vals.size == 0:\n",
    "        return np.zeros_like(arr, dtype=int)\n",
    "    m, s = np.mean(vals), np.std(vals)\n",
    "    s = s if s != 0 else 1.0\n",
    "    z = (arr - m) / s\n",
    "    hot_mask = (z > z_thresh) & mask_valid\n",
    "    cold_mask = (z < -z_thresh) & mask_valid\n",
    "    out = np.zeros_like(arr, dtype=int)\n",
    "    out[hot_mask] = 1\n",
    "    out[cold_mask] = -1\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================== Plotting ===============================\n",
    "\n",
    "def imshow_side_by_side(arr: np.ndarray,\n",
    "                        mask_valid: np.ndarray,\n",
    "                        gm: np.ndarray,\n",
    "                        hc_mask: np.ndarray,\n",
    "                        hot_count: int,\n",
    "                        cold_count: int) -> None:\n",
    "    \"\"\"\n",
    "    Display two panels:\n",
    "      • Left: Sobel gradient magnitude (with colorbar; title shows p50/p95/p99)\n",
    "      • Right: Radiance grayscale with hot (red) / cold (blue) overlay (with grayscale colorbar)\n",
    "    \"\"\"\n",
    "    # Radiance background stretch (2–98%) over valid pixels\n",
    "    vals = arr[mask_valid]\n",
    "    if vals.size:\n",
    "        lo, hi = np.percentile(vals, [2, 98])\n",
    "    else:\n",
    "        lo, hi = 0, 1\n",
    "    normed = (np.clip(arr, lo, hi) - lo) / (hi - lo + 1e-6)\n",
    "    normed = np.where(mask_valid, normed, np.nan)\n",
    "\n",
    "    # RGB grayscale base\n",
    "    base = np.zeros((*arr.shape, 3), dtype=float)\n",
    "    for c in range(3):\n",
    "        base[..., c] = normed\n",
    "\n",
    "    # Overlay hot (red) and cold (blue)\n",
    "    base[hc_mask == 1] = [1, 0, 0]\n",
    "    base[hc_mask == -1] = [0, 0, 1]\n",
    "\n",
    "    # Gradient percentiles for left panel title\n",
    "    p50, p95, p99 = np.nanpercentile(gm, [50, 95, 99])\n",
    "\n",
    "    # Plot side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Left = Sobel gradient (heatmap)\n",
    "    lo_g, hi_g = 0.0, float(np.nanpercentile(gm, 99))\n",
    "    im0 = axes[0].imshow(gm, vmin=lo_g, vmax=hi_g, interpolation=\"nearest\")\n",
    "    axes[0].set_title(f\"Sobel gradient (p50={p50:.3f}, p95={p95:.3f}, p99={p99:.3f})\")\n",
    "    axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "    cbar0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    cbar0.set_label(\"Gradient magnitude\")\n",
    "\n",
    "    # Right = Radiance with hot/cold overlay\n",
    "    im1 = axes[1].imshow(base, interpolation=\"nearest\")\n",
    "    axes[1].set_title(f\"Radiance (gray) + Hot=red / Cold=blue\\nHot={hot_count}, Cold={cold_count}\")\n",
    "    axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "    cbar1 = fig.colorbar(plt.cm.ScalarMappable(cmap=\"gray\"),\n",
    "                         ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label(\"Radiance (normalized 2–98%)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================ QC Table Core ============================\n",
    "\n",
    "def overall_health_table(geotiff_path: str | Path) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute overall QC metrics and return (table_dataframe, extras_dict).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    • The returned DataFrame has one row and is later printed transposed.\n",
    "    • 'extras' contains arrays and counts used for visualization.\n",
    "    \"\"\"\n",
    "    geotiff_path = Path(geotiff_path)\n",
    "    assert geotiff_path.exists(), f\"Not found: {geotiff_path}\"\n",
    "\n",
    "    # ---- Read raster band & metadata ----\n",
    "    with rasterio.open(geotiff_path) as ds:\n",
    "        arr = ds.read(1)\n",
    "        nodata_tag = ds.nodata\n",
    "        h, w = ds.height, ds.width\n",
    "        crs = ds.crs.to_string() if ds.crs else \"None\"\n",
    "\n",
    "    # ---- Masks & hole analysis ----\n",
    "    mask_valid = build_valid_mask(arr, nodata_tag)\n",
    "    nodata = ~mask_valid\n",
    "    total = arr.size\n",
    "    valid_pixels = int(mask_valid.sum())\n",
    "\n",
    "    holes_mask, hole_sizes = internal_holes_mask_and_sizes(nodata)\n",
    "    internal_holes_count = hole_sizes.size\n",
    "    internal_holes_pixels = int(hole_sizes.sum()) if internal_holes_count else 0\n",
    "    internal_holes_ratio = float(internal_holes_pixels / total) if total else np.nan\n",
    "\n",
    "    # ---- Radiance stats (valid only) ----\n",
    "    vals = arr[mask_valid]\n",
    "    if vals.size:\n",
    "        vmin, vmax = float(np.min(vals)), float(np.max(vals))\n",
    "        vmean, vstd = float(np.mean(vals)), float(np.std(vals))\n",
    "        p2, p98 = robust_percentiles(vals, (2, 98))\n",
    "    else:\n",
    "        vmin = vmax = vmean = vstd = p2 = p98 = np.nan\n",
    "\n",
    "    # ---- Outlier counts (hot/cold) ----\n",
    "    if vals.size:\n",
    "        m, s = np.mean(vals), np.std(vals)\n",
    "        s = s if s != 0 else 1.0\n",
    "        z = (arr - m) / s\n",
    "        hot = int(np.sum((z > 6) & mask_valid))\n",
    "        cold = int(np.sum((z < -6) & mask_valid))\n",
    "    else:\n",
    "        hot = cold = 0\n",
    "\n",
    "    # ---- Drop-line candidates (row/col mean |z| > 5) ----\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        row_means = np.where(np.any(mask_valid, axis=1),\n",
    "                             np.nanmean(np.where(mask_valid, arr, np.nan), axis=1), np.nan)\n",
    "        col_means = np.where(np.any(mask_valid, axis=0),\n",
    "                             np.nanmean(np.where(mask_valid, arr, np.nan), axis=0), np.nan)\n",
    "    row_z = zscore_1d(row_means)\n",
    "    col_z = zscore_1d(col_means)\n",
    "    row_drops = int(np.nansum(np.abs(row_z) > 5))\n",
    "    col_drops = int(np.nansum(np.abs(col_z) > 5))\n",
    "\n",
    "    # ---- Gradient distribution percentiles ----\n",
    "    gstats = gradient_percentiles(arr, mask_valid)\n",
    "\n",
    "    # ---- Assemble table (single-row DataFrame) ----\n",
    "    table = {\n",
    "        \"CRS\": crs,\n",
    "        \"Image size\": f\"{h} × {w}\",\n",
    "        \"Valid pixels (count)\": valid_pixels,\n",
    "        \"NoData holes (count)\": internal_holes_count,\n",
    "        \"NoData hole pixels (total)\": internal_holes_pixels,\n",
    "        \"NoData hole ratio\": round(internal_holes_ratio, 6) if np.isfinite(internal_holes_ratio) else np.nan,\n",
    "        \"Radiance min\": vmin,\n",
    "        \"Radiance mean\": vmean,\n",
    "        \"Radiance max\": vmax,\n",
    "        \"Std. dev. (valid)\": vstd,\n",
    "        \"p2 (valid)\": p2,\n",
    "        \"p98 (valid)\": p98,\n",
    "        \"Hot pixels (z>6)\": hot,\n",
    "        \"Cold pixels (z<-6)\": cold,\n",
    "        \"Drop-line rows (|z|>5)\": row_drops,\n",
    "        \"Drop-line cols (|z|>5)\": col_drops,\n",
    "        \"Gradient p50\": gstats[\"grad_p50\"],\n",
    "        \"Gradient p95\": gstats[\"grad_p95\"],\n",
    "        \"Gradient p99\": gstats[\"grad_p99\"],\n",
    "    }\n",
    "\n",
    "    def _fmt(v):\n",
    "        if isinstance(v, (int, np.integer)):\n",
    "            return int(v)\n",
    "        if isinstance(v, float) and np.isfinite(v):\n",
    "            return float(np.round(v, 6))\n",
    "        return v\n",
    "\n",
    "    table = {k: _fmt(v) for k, v in table.items()}\n",
    "    df = pd.DataFrame([table])\n",
    "    extras = dict(arr=arr, mask_valid=mask_valid, hot=hot, cold=cold)\n",
    "    return df, extras\n",
    "\n",
    "\n",
    "# =========================== Descriptions ==============================\n",
    "\n",
    "METRIC_DESCRIPTIONS = {\n",
    "    \"CRS\": \"Coordinate Reference System of the raster (e.g., EPSG:4326).\",\n",
    "    \"Image size\": \"Raster height × width (pixels).\",\n",
    "    \"Valid pixels (count)\": \"Number of pixels considered valid (not NoData/≤-1e30).\",\n",
    "    \"NoData holes (count)\": \"Number of interior NoData components not touching any border.\",\n",
    "    \"NoData hole pixels (total)\": \"Total pixels belonging to all interior NoData holes.\",\n",
    "    \"NoData hole ratio\": \"Interior NoData pixels divided by total image pixels.\",\n",
    "    \"Radiance min\": \"Minimum radiance over valid pixels.\",\n",
    "    \"Radiance mean\": \"Mean radiance over valid pixels.\",\n",
    "    \"Radiance max\": \"Maximum radiance over valid pixels.\",\n",
    "    \"Std. dev. (valid)\": \"Standard deviation of radiance over valid pixels.\",\n",
    "    \"p2 (valid)\": \"2nd percentile of radiance over valid pixels (robust low).\",\n",
    "    \"p98 (valid)\": \"98th percentile of radiance over valid pixels (robust high).\",\n",
    "    \"Hot pixels (z>6)\": \"Pixels >6σ brighter than mean; often sensor outliers (red overlay).\",\n",
    "    \"Cold pixels (z<-6)\": \"Pixels <−6σ darker than mean; often sensor outliers (blue overlay).\",\n",
    "    \"Drop-line rows (|z|>5)\": \"Rows whose mean radiance has |z| > 5 (banding candidates).\",\n",
    "    \"Drop-line cols (|z|>5)\": \"Columns whose mean radiance has |z| > 5 (banding candidates).\",\n",
    "    \"Gradient p50\": \"Typical scene smoothness; low in smooth scenes (ocean ≈0.05–0.2), higher in rugged/urban.\",\n",
    "    \"Gradient p95\": \"Strength of common strong edges (coastlines/cloud edges); unusually high in smooth scenes ⇒ noise/striping.\",\n",
    "    \"Gradient p99\": \"Strongest 1% edges/artifacts (sharp land–sea/cloud fronts); very high ⇒ defects.\",\n",
    "}\n",
    "\n",
    "\n",
    "def describe_metric(metric_name: str) -> str:\n",
    "    \"\"\"Return human-readable description for a metric name.\"\"\"\n",
    "    return METRIC_DESCRIPTIONS.get(metric_name, \"\")\n",
    "\n",
    "\n",
    "def print_transposed(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print QC metrics as a three-column table: Metric | Value | Description.\n",
    "    (Keeps original functionality of printing to stdout.)\n",
    "    \"\"\"\n",
    "    assert df.shape[0] == 1\n",
    "    metrics = list(df.columns)\n",
    "    values = [df.iloc[0, i] for i in range(len(metrics))]\n",
    "    rows = [{\"Metric\": m, \"Value\": v, \"Description\": describe_metric(m)}\n",
    "            for m, v in zip(metrics, values)]\n",
    "    tdf = pd.DataFrame(rows, columns=[\"Metric\", \"Value\", \"Description\"])\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.width\", 170):\n",
    "        print(tdf.to_string(index=False))\n",
    "\n",
    "\n",
    "# ================================ CLI =================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example inputs (toggle as needed)\n",
    "    INPUT = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-08-12T212259.010953Z_2025-08-13T111644.731945Z_e81989f5_MWIR.tif\")\n",
    "\n",
    "    # ---- Compute QC table & print (transposed) ----\n",
    "    df, extras = overall_health_table(INPUT)\n",
    "    print_transposed(df)\n",
    "\n",
    "    # ---- Build layers & visualize side-by-side ----\n",
    "    gm = compute_sobel_magnitude(extras[\"arr\"], extras[\"mask_valid\"])\n",
    "    hc_mask = compute_hot_cold_mask(extras[\"arr\"], extras[\"mask_valid\"], z_thresh=6.0)\n",
    "    imshow_side_by_side(\n",
    "        extras[\"arr\"], extras[\"mask_valid\"], gm, hc_mask,\n",
    "        hot_count=extras[\"hot\"], cold_count=extras[\"cold\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc27600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ forest2_plus_reference_visualize.py ======================\n",
    "# FOREST-2 MWIR Radiance + Brightness Temperature (Manual Time Zone)\n",
    "# VIIRS/MODIS Comparison + Points Overlay\n",
    "#\n",
    "# Purpose\n",
    "#   • Reproject FOREST-2 MWIR radiance to EPSG:4326 and compute BT (λ_eff ≈ 3.8 µm)\n",
    "#   • Load VIIRS/MODIS Radiance/BT (already in GeoTIFF), crop all rasters by AOI\n",
    "#   • Compute AOI-based NoData shares\n",
    "#   • Parse acquisition timestamps (FOREST-2 ISO; VIIRS/MODIS AYYYYDDD.HHMM)\n",
    "#     and convert to a user-defined local timezone for figure stamps\n",
    "#   • Overlay detection points (GeoJSON) on map panels (black fill, white edge)\n",
    "#   • Produce paired maps with a single shared colorbar\n",
    "#   • Produce overlaid histograms (log-density) for Radiance & BT\n",
    "#\n",
    "# Requirements\n",
    "#   pip install rasterio geopandas shapely matplotlib pandas numpy tzdata\n",
    "#   (Shapely 1.x or 2.x supported)\n",
    "# ======================================================================\n",
    "\n",
    "# ============================== Imports ================================\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.transform import array_bounds\n",
    "from affine import Affine\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.ops import transform as shp_transform\n",
    "from shapely.geometry import Point\n",
    "import shapely\n",
    "\n",
    "\n",
    "# ============================ User Settings ============================\n",
    "# --- Input files (adjust as needed) ---\n",
    "# FOREST-2 L1 MWIR radiance (source CRS; will be reprojected to EPSG:4326)\n",
    "INPUT_FILE = r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR.tif\"\n",
    "\n",
    "# Reference satellite (example: MODIS Band 21 Radiance / Band 31 BT in GeoTIFF)\n",
    "REF_SAT_RAD_FILE = r\"path\\to\\MYD021KM.A2025010.2120.061.2025014191057_B21_Rad.tif\"\n",
    "REF_SAT_BT_FILE  = r\"path\\to\\MYD021KM.A2025010.2120.061.2025014191057_B31_BT_K.tif\"\n",
    "\n",
    "# Detection points and AOI boundary\n",
    "POINTS_GEOJSON = r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_model_detections.geojson\"\n",
    "BOUNDARY_SHP   = r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\"\n",
    "\n",
    "# --- Timezone controls ---\n",
    "USE_LOCAL_TZ_F2   = True\n",
    "USE_LOCAL_TZ_REF  = True\n",
    "TZ_NAME_LOCAL     = \"America/Los_Angeles\"  # e.g., \"America/Chicago\"\n",
    "\n",
    "# --- Processing & visualization parameters ---\n",
    "TARGET_CRS        = CRS.from_epsg(4326)   # output CRS for FOREST-2 reprojection\n",
    "PERC_STRETCH      = (2, 98)               # robust percentile stretch for quicklooks\n",
    "SAVE_COMPRESSION  = \"lzw\"                 # GeoTIFF compression for outputs (None to disable)\n",
    "WRITE_TIME_TAGS   = True                  # write acquisition time tags to outputs\n",
    "ALL_TOUCHED       = False                 # rasterization behavior for AOI masks\n",
    "ASSUME_POINTS_CRS = CRS.from_epsg(4326)   # assumed CRS if GeoJSON lacks CRS\n",
    "DRAW_AOI_OUTLINE  = False                 # set True to draw AOI outline on maps\n",
    "ADDITIONAL_NODATA_VALUES = (-3.40282e38, 65535, 65555)  # harden NoData detection\n",
    "\n",
    "\n",
    "# ============================ Helper Utils ============================\n",
    "def robust_percentiles(arr: np.ndarray, p_lo=2, p_hi=98):\n",
    "    \"\"\"Return robust low/high percentiles ignoring non-finite values.\"\"\"\n",
    "    f = np.isfinite(arr)\n",
    "    if f.any():\n",
    "        lo, hi = np.percentile(arr[f], [p_lo, p_hi])\n",
    "        return float(lo), float(hi)\n",
    "    return np.nan, np.nan\n",
    "\n",
    "\n",
    "def pooled_percentiles(a: np.ndarray, b: np.ndarray, p_lo=2, p_hi=98):\n",
    "    \"\"\"Pooled robust percentiles across two arrays.\"\"\"\n",
    "    a_lo, a_hi = robust_percentiles(a, p_lo, p_hi)\n",
    "    b_lo, b_hi = robust_percentiles(b, p_lo, p_hi)\n",
    "    return np.nanmin([a_lo, b_lo]), np.nanmax([a_hi, b_hi])\n",
    "\n",
    "\n",
    "def _parse_start_iso_from_name(name: str):\n",
    "    \"\"\"FOREST-2: first ISO-8601 timestamp in filename → aware UTC datetime, if present.\"\"\"\n",
    "    m = re.search(\n",
    "        r\"(?P<date>\\d{4}-\\d{2}-\\d{2})T(?P<hh>\\d{2})(?P<mm>\\d{2})(?P<ss>\\d{2})(?:\\.(?P<frac>\\d+))?Z\",\n",
    "        name\n",
    "    )\n",
    "    if not m:\n",
    "        return None\n",
    "    parts = m.groupdict()\n",
    "    micro = int((parts.get(\"frac\") or \"0\")[:6].ljust(6, \"0\"))\n",
    "    return datetime(\n",
    "        *map(int, parts[\"date\"].split(\"-\")),\n",
    "        int(parts[\"hh\"]), int(parts[\"mm\"]), int(parts[\"ss\"]),\n",
    "        microsecond=micro, tzinfo=timezone.utc\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_f2_datetime_utc(path: Path):\n",
    "    \"\"\"FOREST-2 UTC acquisition time from filename or GeoTIFF tags (best-effort).\"\"\"\n",
    "    dt = _parse_start_iso_from_name(path.name)\n",
    "    if dt:\n",
    "        return dt\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            tags = src.tags()\n",
    "        for key in (\"ACQ_DATETIME\", \"ACQUISITION_DATETIME\", \"ACQ_TIME\", \"DATETIME\", \"TIFFTAG_DATETIME\"):\n",
    "            val = tags.get(key)\n",
    "            if not val:\n",
    "                continue\n",
    "            if key == \"TIFFTAG_DATETIME\":\n",
    "                # e.g., 2025:01:10 21:54:12\n",
    "                return datetime.strptime(val, \"%Y:%m:%d %H:%M:%S\").replace(tzinfo=timezone.utc)\n",
    "            # ISO with potential trailing 'Z'\n",
    "            return datetime.fromisoformat(val.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_ref_utc_from_name(name: str):\n",
    "    \"\"\"VIIRS/MODIS: parse UTC from 'AYYYYDDD.HHMM' (e.g., A2025224.1954); return (dt_utc, code).\"\"\"\n",
    "    m = re.search(r\"A(?P<y>\\d{4})(?P<doy>\\d{3})\\.(?P<h>\\d{2})(?P<m>\\d{2})\", name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    y, doy, hh, mm = int(m[\"y\"]), int(m[\"doy\"]), int(m[\"h\"]), int(m[\"m\"])\n",
    "    code = f\"{y}{doy:03d}.{hh:02d}{mm:02d}\"\n",
    "    dt_utc = datetime(y, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "    return dt_utc, code\n",
    "\n",
    "\n",
    "def to_local_naive(dt_utc, use_local: bool, tz_name: str):\n",
    "    \"\"\"UTC → naive local datetime (with a label) or naive UTC if disabled/invalid.\"\"\"\n",
    "    if dt_utc is None:\n",
    "        return None, None\n",
    "    if not use_local or not tz_name:\n",
    "        return dt_utc.replace(tzinfo=None), \"UTC\"\n",
    "    try:\n",
    "        tz = ZoneInfo(tz_name)\n",
    "        return dt_utc.astimezone(tz).replace(tzinfo=None), tz.key\n",
    "    except Exception:\n",
    "        return dt_utc.replace(tzinfo=None), \"UTC\"\n",
    "\n",
    "\n",
    "def crop_array_and_transform(arr: np.ndarray, mask: np.ndarray | None, transform: Affine):\n",
    "    \"\"\"\n",
    "    Crop array/mask to the minimal bounding rectangle of True values in mask.\n",
    "    Returns: arr_crop, mask_crop, transform_crop, extent=(left, right, bottom, top)\n",
    "    \"\"\"\n",
    "    if mask is None or not np.any(mask):\n",
    "        h, w = arr.shape\n",
    "        left, bottom, right, top = array_bounds(h, w, transform)\n",
    "        return arr, mask, transform, (left, right, bottom, top)\n",
    "\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    r0, r1 = np.where(rows)[0][[0, -1]]\n",
    "    c0, c1 = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    arrc = arr[r0:r1 + 1, c0:c1 + 1]\n",
    "    maskc = mask[r0:r1 + 1, c0:c1 + 1]\n",
    "    trc = transform * Affine.translation(c0, r0)\n",
    "\n",
    "    h, w = arrc.shape\n",
    "    left, bottom, right, top = array_bounds(h, w, trc)\n",
    "    return arrc, maskc, trc, (left, right, bottom, top)\n",
    "\n",
    "\n",
    "def pixel_size_from_transform(tr: Affine) -> float:\n",
    "    \"\"\"Return a representative pixel size from an affine transform.\"\"\"\n",
    "    return max(abs(tr.a), abs(tr.e))\n",
    "\n",
    "\n",
    "def union_geoms(geoms):\n",
    "    \"\"\"\n",
    "    Shapely-compatible geometry union:\n",
    "      • Prefer shapely.ops.union_all (Shapely ≥ 2.0),\n",
    "      • Fall back to shapely.ops.unary_union (Shapely 1.x).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from shapely.ops import union_all as _union_all\n",
    "        return _union_all(list(geoms))\n",
    "    except Exception:\n",
    "        from shapely.ops import unary_union as _unary_union\n",
    "        return _unary_union(list(geoms))\n",
    "\n",
    "\n",
    "def _to_2d(x, y, z=None):\n",
    "    \"\"\"Helper for dropping any Z component with shapely.ops.transform.\"\"\"\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def force_points_2d_centroids(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Ensure point-like features:\n",
    "      • Explode multipart geometries,\n",
    "      • Make valid (fallback buffer(0) if needed),\n",
    "      • Force 2D,\n",
    "      • Convert non-point geometries to centroids.\n",
    "    \"\"\"\n",
    "    if gdf.empty:\n",
    "        return gdf\n",
    "    try:\n",
    "        gdf = gdf.explode(index_parts=False, ignore_index=True)\n",
    "    except TypeError:  # older geopandas\n",
    "        gdf = gdf.explode(ignore_index=True)\n",
    "\n",
    "    def _fix2d(g):\n",
    "        if g is None or g.is_empty:\n",
    "            return g\n",
    "        try:\n",
    "            g = shapely.make_valid(g) if hasattr(shapely, \"make_valid\") else g.buffer(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            g = shp_transform(_to_2d, g)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return g\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(_fix2d)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda g: g if isinstance(g, Point) else g.centroid)\n",
    "    return gdf[~gdf.geometry.is_empty & gdf.geometry.notnull()].copy()\n",
    "\n",
    "\n",
    "def select_points_for_panel(points_path, assume_crs_if_missing, target_crs, aoi_gdf,\n",
    "                            panel_transform: Affine, panel_extent: tuple, buffer_pixels=1.0):\n",
    "    \"\"\"\n",
    "    Select detection points for a given panel:\n",
    "      • Reproject points and AOI to panel CRS,\n",
    "      • Buffer the AOI by ~N pixels (map units),\n",
    "      • Keep points intersecting buffered AOI.\n",
    "    \"\"\"\n",
    "    pts_raw = gpd.read_file(points_path)\n",
    "    if pts_raw.crs is None:\n",
    "        pts_raw = pts_raw.set_crs(assume_crs_if_missing)\n",
    "    pts_raw = force_points_2d_centroids(pts_raw)\n",
    "\n",
    "    pts = pts_raw.to_crs(target_crs)\n",
    "    aoi = aoi_gdf.to_crs(target_crs)\n",
    "    px = pixel_size_from_transform(panel_transform)\n",
    "    buf = float(buffer_pixels) * (px if np.isfinite(px) else 0.0)\n",
    "\n",
    "    geom = union_geoms(aoi.geometry.values)\n",
    "    try:\n",
    "        geom_b = geom.buffer(buf) if buf > 0 else geom\n",
    "    except Exception:\n",
    "        geom_b = geom\n",
    "\n",
    "    kept = pts[pts.intersects(geom_b)]\n",
    "    if len(kept) == 0:\n",
    "        # Keep empty result predictable but non-crashing (optional behavior)\n",
    "        kept = pts.copy()\n",
    "        kept[\"__force__\"] = True\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ========================= Visualization Utils ========================\n",
    "def show_pair_with_right_cbar(arr_left, title_left, stamp_left, extent_left, pts_left, aoi_left,\n",
    "                              arr_right, title_right, stamp_right, extent_right, pts_right, aoi_right,\n",
    "                              vmin, vmax, cmap, cbar_label):\n",
    "    \"\"\"\n",
    "    Create two side-by-side map panels with a shared colorbar on the far right.\n",
    "    Titles/timestamps are placed above to avoid overlaying imagery.\n",
    "    \"\"\"\n",
    "    # Aspect ratios (map units)\n",
    "    lw = extent_left[1] - extent_left[0]\n",
    "    lh = extent_left[3] - extent_left[2]\n",
    "    rw = extent_right[1] - extent_right[0]\n",
    "    rh = extent_right[3] - extent_right[2]\n",
    "    ar_left = (lh / lw) if lw > 0 else 1.0\n",
    "    ar_right = (rh / rw) if rw > 0 else 1.0\n",
    "\n",
    "    # Balanced figure size from aspect ratios\n",
    "    base_h = 4.6\n",
    "    avg_inv = 0.5 * ((1.0 / max(ar_left, 1e-6)) + (1.0 / max(ar_right, 1e-6)))\n",
    "    fig_w = base_h * avg_inv * 2.0 + 0.9\n",
    "    fig_h = base_h + 0.9\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h))\n",
    "\n",
    "    # Grid: [title row; image row] x [left image, right image, spacer, colorbar]\n",
    "    gs = fig.add_gridspec(\n",
    "        2, 4,\n",
    "        height_ratios=[0.09, 0.91],\n",
    "        width_ratios=[1.0, 1.0, 0.004, 0.033],\n",
    "        wspace=0.01, hspace=0.01\n",
    "    )\n",
    "\n",
    "    # Title/timestamp areas\n",
    "    ax_stamp_l = fig.add_subplot(gs[0, 0])\n",
    "    ax_stamp_r = fig.add_subplot(gs[0, 1])\n",
    "    for ax in (ax_stamp_l, ax_stamp_r):\n",
    "        ax.axis(\"off\")\n",
    "    ax_stamp_l.text(0.0, 0.55, f\"{title_left}\\n{stamp_left}\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "    ax_stamp_r.text(0.0, 0.55, f\"{title_right}\\n{stamp_right}\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "    # Image axes and shared colorbar axis\n",
    "    ax1 = fig.add_subplot(gs[1, 0])\n",
    "    ax2 = fig.add_subplot(gs[1, 1])\n",
    "    cax = fig.add_subplot(gs[:, 3])  # shared colorbar at far right\n",
    "\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        for s in ax.spines.values():\n",
    "            s.set_visible(False)\n",
    "        ax.set_anchor('W')\n",
    "    ax1.set_box_aspect(ar_left)\n",
    "    ax2.set_box_aspect(ar_right)\n",
    "\n",
    "    # Draw rasters (extent = left, right, bottom, top)\n",
    "    im1 = ax1.imshow(arr_left,  vmin=vmin, vmax=vmax, cmap=cmap, extent=extent_left,  origin=\"upper\")\n",
    "    im2 = ax2.imshow(arr_right, vmin=vmin, vmax=vmax, cmap=cmap, extent=extent_right, origin=\"upper\")\n",
    "\n",
    "    # Lock extents\n",
    "    ax1.set_xlim(extent_left[0],  extent_left[1]);  ax1.set_ylim(extent_left[2],  extent_left[3])\n",
    "    ax2.set_xlim(extent_right[0], extent_right[1]); ax2.set_ylim(extent_right[2], extent_right[3])\n",
    "\n",
    "    # Optional AOI outline\n",
    "    if DRAW_AOI_OUTLINE:\n",
    "        try:\n",
    "            aoi_left.boundary.plot(ax=ax1,  color=\"white\", linewidth=0.8, alpha=0.9, zorder=5)\n",
    "            aoi_right.boundary.plot(ax=ax2, color=\"white\", linewidth=0.8, alpha=0.9, zorder=5)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Points overlay\n",
    "    if pts_left is not None and not pts_left.empty:\n",
    "        pts_left.plot(ax=ax1, color=\"black\", edgecolor=\"white\", markersize=70, linewidth=1.0, zorder=20)\n",
    "    if pts_right is not None and not pts_right.empty:\n",
    "        pts_right.plot(ax=ax2, color=\"black\", edgecolor=\"white\", markersize=70, linewidth=1.0, zorder=20)\n",
    "\n",
    "    # Shared colorbar\n",
    "    cb = fig.colorbar(im1, cax=cax, orientation=\"vertical\")\n",
    "    cb.set_label(cbar_label)\n",
    "    cb.ax.yaxis.set_tick_params(pad=2)\n",
    "\n",
    "    # Tight layout\n",
    "    fig.subplots_adjust(left=0.018, right=0.988, top=0.96, bottom=0.065, wspace=0.01, hspace=0.01)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================ I/O Utilities ===========================\n",
    "def load_single_band(path_str: str):\n",
    "    \"\"\"\n",
    "    Load a single-band GeoTIFF as float64 with NaN NoData.\n",
    "    Detects embedded NoData plus common artifact values (configurable).\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    assert p.exists(), f\"Not found: {p}\"\n",
    "    with rasterio.open(p) as ds:\n",
    "        arr = ds.read(1).astype(np.float64)\n",
    "        nd = ds.nodata\n",
    "\n",
    "        # Normalize NoData: declared NoData or known fill values → NaN\n",
    "        mask_declared = np.zeros_like(arr, dtype=bool)\n",
    "        if nd is not None and np.isfinite(nd):\n",
    "            mask_declared |= (arr == nd)\n",
    "\n",
    "        mask_artifacts = np.zeros_like(arr, dtype=bool)\n",
    "        for val in ADDITIONAL_NODATA_VALUES:\n",
    "            mask_artifacts |= (arr == val)\n",
    "\n",
    "        arr = np.where(mask_declared | mask_artifacts, np.nan, arr)\n",
    "\n",
    "        return arr, ds.crs, ds.transform, nd\n",
    "\n",
    "\n",
    "# ============================== Main Flow =============================\n",
    "def main():\n",
    "    # ---------- Reproject FOREST-2 radiance to TARGET_CRS ----------\n",
    "    in_path = Path(INPUT_FILE)\n",
    "    assert in_path.exists(), f\"Not found: {in_path}\"\n",
    "\n",
    "    f2_dt_utc = extract_f2_datetime_utc(in_path)  # aware UTC (or None)\n",
    "    with rasterio.open(in_path) as src:\n",
    "        assert src.crs, \"FOREST-2 input has no CRS.\"\n",
    "        out_transform, out_w, out_h = calculate_default_transform(\n",
    "            src.crs, TARGET_CRS, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        rad_f2 = np.full((out_h, out_w), np.nan, dtype=np.float64)\n",
    "        reproject(\n",
    "            source=src.read(1).astype(np.float64),\n",
    "            destination=rad_f2,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            src_nodata=src.nodata,\n",
    "            dst_transform=out_transform,\n",
    "            dst_crs=TARGET_CRS,\n",
    "            dst_nodata=np.nan,                # force NaN NoData in output\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "\n",
    "    # Optional: save reprojected FOREST-2 radiance\n",
    "    crs_tag = f\"epsg{TARGET_CRS.to_epsg()}\" if TARGET_CRS.to_epsg() else \"reproj\"\n",
    "    out_rad_path = in_path.with_name(in_path.stem + f\"_radiance_{crs_tag}.tif\")\n",
    "    if SAVE_COMPRESSION:\n",
    "        with rasterio.open(\n",
    "            out_rad_path, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=rad_f2.shape[0], width=rad_f2.shape[1],\n",
    "            count=1, dtype=rasterio.float32,\n",
    "            crs=TARGET_CRS, transform=out_transform,\n",
    "            nodata=np.float32(np.nan), compress=SAVE_COMPRESSION\n",
    "        ) as dst:\n",
    "            dst.write(rad_f2.astype(np.float32), 1)\n",
    "            if WRITE_TIME_TAGS and isinstance(f2_dt_utc, datetime):\n",
    "                dst.update_tags(ACQ_DATETIME_UTC=f2_dt_utc.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # Local time stamp for figure annotations\n",
    "    f2_dt_local, f2_tz_label = to_local_naive(f2_dt_utc, USE_LOCAL_TZ_F2, TZ_NAME_LOCAL)\n",
    "    f2_stamp = (\n",
    "        f2_dt_local.strftime(\"%Y-%m-%d %H:%M\") + (f\" {f2_tz_label}\" if f2_tz_label else \"\")\n",
    "    ) if isinstance(f2_dt_local, datetime) else \"Time: unknown\"\n",
    "\n",
    "    # ---------- AOI masks & crops ----------\n",
    "    gdf_raw = gpd.read_file(BOUNDARY_SHP)\n",
    "    assert gdf_raw.crs, \"Boundary shapefile has no CRS.\"\n",
    "\n",
    "    # FOREST-2 boundary/mask/crop\n",
    "    gdf_f2 = gdf_raw.to_crs(TARGET_CRS)\n",
    "    geom_f2 = union_geoms(gdf_f2.geometry.values)\n",
    "    mask_f2 = geometry_mask([geom_f2], out_shape=rad_f2.shape, transform=out_transform,\n",
    "                            invert=True, all_touched=ALL_TOUCHED)\n",
    "    rad_f2_crop, _, transform_f2_crop, extent_f2 = crop_array_and_transform(rad_f2, mask_f2, out_transform)\n",
    "\n",
    "    # ---------- Brightness Temperature (Planck inversion at λ_eff ≈ 3.8 µm) ----------\n",
    "    # CODATA constants (exact in SI):\n",
    "    h = 6.62607015e-34      # Planck constant [J·s]\n",
    "    c = 2.99792458e8        # speed of light [m/s]\n",
    "    k = 1.380649e-23        # Boltzmann constant [J/K]\n",
    "    lam_eff_um = 3.8\n",
    "    lam = lam_eff_um * 1e-6\n",
    "\n",
    "    # K1, K2 for monochromatic radiance at λ (per-µm units)\n",
    "    K1 = (2 * h * c**2) / (lam**5) * 1e-6\n",
    "    K2 = (h * c) / (k * lam)\n",
    "\n",
    "    # Brightness temperature from radiance: Tb = K2 / ln(1 + K1 / Lλ)\n",
    "    # Clip radiance to avoid division and log underflow\n",
    "    bt_f2 = K2 / np.log1p(K1 / np.clip(rad_f2, 1e-6, None))\n",
    "    bt_f2_crop, _, _, extent_f2_bt = crop_array_and_transform(bt_f2, mask_f2, out_transform)\n",
    "\n",
    "    # NoData shares (FOREST-2)\n",
    "    total_in_f2 = int(mask_f2.sum())\n",
    "    nodata_share_rad_f2 = (np.isnan(rad_f2)[mask_f2].sum() / total_in_f2) if total_in_f2 > 0 else np.nan\n",
    "    nodata_share_bt_f2  = ((~np.isfinite(bt_f2))[mask_f2].sum() / total_in_f2) if total_in_f2 > 0 else np.nan\n",
    "\n",
    "    # ---------- Reference satellite (VIIRS/MODIS) load & crop ----------\n",
    "    ref_rad, v_crs,  v_transform,  _ = load_single_band(REF_SAT_RAD_FILE)\n",
    "    ref_bt,  v_crs2, v_transform2, _ = load_single_band(REF_SAT_BT_FILE)\n",
    "\n",
    "    # Reference time (UTC) → local\n",
    "    v_dt_utc, v_timecode = parse_ref_utc_from_name(Path(REF_SAT_RAD_FILE).name)\n",
    "    if v_dt_utc is None:\n",
    "        v_dt_utc, v_timecode = parse_ref_utc_from_name(Path(REF_SAT_BT_FILE).name)\n",
    "    v_dt_local, v_tz_label = to_local_naive(v_dt_utc, USE_LOCAL_TZ_REF, TZ_NAME_LOCAL)\n",
    "    v_stamp = (\n",
    "        v_dt_local.strftime(\"%Y-%m-%d %H:%M\") + (f\" {v_tz_label}\" if v_tz_label else \"\")\n",
    "    ) if isinstance(v_dt_local, datetime) else \"Time: unknown\"\n",
    "\n",
    "    # Reference boundary/masks/crops (Radiance)\n",
    "    gdf_vr = gdf_raw.to_crs(v_crs)\n",
    "    geom_vr = union_geoms(gdf_vr.geometry.values)\n",
    "    mask_vr = geometry_mask([geom_vr], out_shape=ref_rad.shape, transform=v_transform,\n",
    "                            invert=True, all_touched=ALL_TOUCHED)\n",
    "    ref_rad_crop, _, _, extent_vr = crop_array_and_transform(ref_rad, mask_vr, v_transform)\n",
    "\n",
    "    # Reference boundary/masks/crops (BT)\n",
    "    gdf_vb = gdf_raw.to_crs(v_crs2)\n",
    "    geom_vb = union_geoms(gdf_vb.geometry.values)\n",
    "    mask_vb = geometry_mask([geom_vb], out_shape=ref_bt.shape, transform=v_transform2,\n",
    "                            invert=True, all_touched=ALL_TOUCHED)\n",
    "    ref_bt_crop, _, _, extent_vb = crop_array_and_transform(ref_bt, mask_vb, v_transform2)\n",
    "\n",
    "    # NoData shares (Reference)\n",
    "    total_in_vr = int(mask_vr.sum())\n",
    "    total_in_vb = int(mask_vb.sum())\n",
    "    nodata_share_rad_ref = (np.isnan(ref_rad)[mask_vr].sum() / total_in_vr) if total_in_vr > 0 else np.nan\n",
    "    nodata_share_bt_ref  = ((~np.isfinite(ref_bt))[mask_vb].sum() / total_in_vb) if total_in_vb > 0 else np.nan\n",
    "\n",
    "    # ---------- Points for overlays ----------\n",
    "    pts_f2_for_rad = select_points_for_panel(\n",
    "        POINTS_GEOJSON, ASSUME_POINTS_CRS, TARGET_CRS, gdf_raw, transform_f2_crop, extent_f2, buffer_pixels=1.0\n",
    "    )\n",
    "    pts_f2_for_bt  = pts_f2_for_rad\n",
    "    pts_ref_for_rad = select_points_for_panel(\n",
    "        POINTS_GEOJSON, ASSUME_POINTS_CRS, v_crs,  gdf_raw, v_transform,  extent_vr, buffer_pixels=1.0\n",
    "    )\n",
    "    pts_ref_for_bt  = select_points_for_panel(\n",
    "        POINTS_GEOJSON, ASSUME_POINTS_CRS, v_crs2, gdf_raw, v_transform2, extent_vb, buffer_pixels=1.0\n",
    "    )\n",
    "\n",
    "    # ---------- Stretches for display ----------\n",
    "    def pooled(v1, v2):\n",
    "        vmin = np.nanmin([\n",
    "            np.nanpercentile(v1[np.isfinite(v1)], PERC_STRETCH[0]) if np.isfinite(v1).any() else np.nan,\n",
    "            np.nanpercentile(v2[np.isfinite(v2)], PERC_STRETCH[0]) if np.isfinite(v2).any() else np.nan\n",
    "        ])\n",
    "        vmax = np.nanmax([\n",
    "            np.nanpercentile(v1[np.isfinite(v1)], PERC_STRETCH[1]) if np.isfinite(v1).any() else np.nan,\n",
    "            np.nanpercentile(v2[np.isfinite(v2)], PERC_STRETCH[1]) if np.isfinite(v2).any() else np.nan\n",
    "        ])\n",
    "        return vmin, vmax\n",
    "\n",
    "    rad_vmin, rad_vmax = pooled(rad_f2_crop, ref_rad_crop)\n",
    "    bt_vmin,  bt_vmax  = pooled(bt_f2_crop,  ref_bt_crop)\n",
    "\n",
    "    # ============================ Visuals =============================\n",
    "    # Radiance paired maps\n",
    "    show_pair_with_right_cbar(\n",
    "        rad_f2_crop, \"FOREST-2 MWIR Radiance\", f2_stamp, extent_f2, pts_f2_for_rad, gdf_f2,\n",
    "        ref_rad_crop, \"Reference MWIR Radiance\",  v_stamp, extent_vr, pts_ref_for_rad, gdf_vr,\n",
    "        rad_vmin, rad_vmax, \"plasma\", \"Radiance (W m$^{-2}$ sr$^{-1}$ µm$^{-1}$)\"\n",
    "    )\n",
    "\n",
    "    # Radiance histogram (overlaid)\n",
    "    f2r_f = np.isfinite(rad_f2_crop); vr_f = np.isfinite(ref_rad_crop)\n",
    "    lo = np.nanmin([np.nanmin(rad_f2_crop[f2r_f]) if f2r_f.any() else np.nan,\n",
    "                    np.nanmin(ref_rad_crop[vr_f]) if vr_f.any() else np.nan])\n",
    "    hi = np.nanmax([np.nanmax(rad_f2_crop[f2r_f]) if f2r_f.any() else np.nan,\n",
    "                    np.nanmax(ref_rad_crop[vr_f]) if vr_f.any() else np.nan])\n",
    "    bins = np.linspace(lo, hi, 200) if (np.isfinite(lo) and np.isfinite(hi) and hi > lo) else 200\n",
    "\n",
    "    plt.figure(figsize=(8.4, 5.0))\n",
    "    if f2r_f.any():\n",
    "        plt.hist(rad_f2_crop[f2r_f].ravel(), bins=bins, density=True, label=\"FOREST-2\", alpha=0.85)\n",
    "    if vr_f.any():\n",
    "        plt.hist(ref_rad_crop[vr_f].ravel(), bins=bins, density=True, label=\"Reference MWIR\", color=\"red\", alpha=0.7)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Radiance (W m$^{-2}$ sr$^{-1}$ µm$^{-1}$)\")\n",
    "    plt.ylabel(\"Density (log)\")\n",
    "    plt.title(\"Radiance Distribution\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # BT paired maps\n",
    "    show_pair_with_right_cbar(\n",
    "        bt_f2_crop, \"FOREST-2 Brightness Temperature\", f2_stamp, extent_f2_bt, pts_f2_for_bt, gdf_f2,\n",
    "        ref_bt_crop, \"Reference Brightness Temperature\",  v_stamp, extent_vb, pts_ref_for_bt, gdf_vb,\n",
    "        bt_vmin, bt_vmax, \"Spectral_r\", \"Brightness Temperature (K)\"\n",
    "    )\n",
    "\n",
    "    # BT histogram (overlaid)\n",
    "    f2b_f = np.isfinite(bt_f2_crop); vb_f = np.isfinite(ref_bt_crop)\n",
    "    lo = np.nanmin([np.nanmin(bt_f2_crop[f2b_f]) if f2b_f.any() else np.nan,\n",
    "                    np.nanmin(ref_bt_crop[vb_f]) if vb_f.any() else np.nan])\n",
    "    hi = np.nanmax([np.nanmax(bt_f2_crop[f2b_f]) if f2b_f.any() else np.nan,\n",
    "                    np.nanmax(ref_bt_crop[vb_f]) if vb_f.any() else np.nan])\n",
    "    bins = np.linspace(lo, hi, 200) if (np.isfinite(lo) and np.isfinite(hi) and hi > lo) else 200\n",
    "\n",
    "    plt.figure(figsize=(8.4, 5.0))\n",
    "    if f2b_f.any():\n",
    "        plt.hist(bt_f2_crop[f2b_f].ravel(), bins=bins, density=True, label=\"FOREST-2\", alpha=0.85)\n",
    "    if vb_f.any():\n",
    "        plt.hist(ref_bt_crop[vb_f].ravel(), bins=bins, density=True, label=\"Reference MWIR\", color=\"red\", alpha=0.7)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Brightness Temperature (K)\")\n",
    "    plt.ylabel(\"Density (log)\")\n",
    "    plt.title(\"Brightness Temperature Distribution\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- Optional: save FOREST-2 BT ----------\n",
    "    out_bt_path = in_path.with_name(in_path.stem + f\"_BT_K_{crs_tag}.tif\")\n",
    "    if SAVE_COMPRESSION:\n",
    "        with rasterio.open(\n",
    "            out_bt_path, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=bt_f2.shape[0], width=bt_f2.shape[1],\n",
    "            count=1, dtype=rasterio.float32,\n",
    "            crs=TARGET_CRS, transform=out_transform,\n",
    "            nodata=np.float32(np.nan), compress=SAVE_COMPRESSION\n",
    "        ) as dst:\n",
    "            dst.write(bt_f2.astype(np.float32), 1)\n",
    "            if WRITE_TIME_TAGS and isinstance(f2_dt_utc, datetime):\n",
    "                dst.update_tags(\n",
    "                    ACQ_DATETIME_UTC=f2_dt_utc.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    ACQ_TZ=(f2_tz_label or \"UTC\"),\n",
    "                    ACQ_DATETIME_LOCAL=(f2_dt_local.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                        if isinstance(f2_dt_local, datetime) else \"unknown\")\n",
    "                )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3226a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ air_temperature.py ======================\n",
    "\"\"\"\n",
    "Daily near-surface air temperature (Ta_C) and shortwave radiation (Rs)\n",
    "over an AOI from NOAA CFSv2 FOR6H, aggregated to daily means via a\n",
    "time-warp helper, and returned as a pandas DataFrame `df_daily`.\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• `df_daily`  (pandas.DataFrame): columns = ['date', 'Ta_C', 'Rs']\n",
    "   - 'date' is a pandas datetime64[ns] (no timezone) sorted ascending\n",
    "   - Ta_C in °C, Rs in W m^-2 (6-hour average band daily-mean)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "• Requires an authenticated Earth Engine account.\n",
    "\n",
    "Dependencies\n",
    "----\n",
    "Install earthengine-api geemap pandas\n",
    "\"\"\"\n",
    "\n",
    "# ============================== Imports ===============================\n",
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "from geemap import shp_to_ee  # noqa: F401  (kept for reference in docstring)\n",
    "\n",
    "# ======================== Earth Engine Init ===========================\n",
    "# Authenticate once if needed, then initialize. This keeps behavior intact.\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# =========================== Configuration ============================\n",
    "# AOI shapefile (single or multi-part polygon)\n",
    "shapefile_path = r'path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp'\n",
    "\n",
    "# Analysis window (inclusive start, exclusive end in EE filterDate semantics)\n",
    "startDate = ee.Date.fromYMD(2025, 1, 4)\n",
    "endDate   = ee.Date.fromYMD(2025, 1, 13)\n",
    "\n",
    "# ============================== AOI ================================\n",
    "# Convert shapefile to an EE geometry and ensure we use the geometry only (no properties)\n",
    "AOI = geemap.shp_to_ee(shapefile_path).geometry()\n",
    "\n",
    "# =========================== Time Helper ============================\n",
    "def time_warp(collection: ee.ImageCollection,\n",
    "              start: ee.Date,\n",
    "              count: ee.Number,\n",
    "              interval: int,\n",
    "              units: str) -> ee.ImageCollection:\n",
    "    \"\"\"\n",
    "    Build a daily (or generic) sequence of mean images with aligned timestamps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    collection : ee.ImageCollection\n",
    "        Source images to aggregate (already filtered by date/bounds).\n",
    "    start : ee.Date\n",
    "        Start date of the first interval.\n",
    "    count : ee.Number\n",
    "        Number of intervals to produce.\n",
    "    interval : int\n",
    "        Size of each interval (e.g., 1 for daily).\n",
    "    units : str\n",
    "        Units for 'interval' (e.g., 'day').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.ImageCollection\n",
    "        Collection where each image is the mean over one interval with:\n",
    "        - 'system:time_start' set to the interval start\n",
    "        - 'date_daily', 'system:id', 'system:index' set to YYYY-MM-DD\n",
    "    \"\"\"\n",
    "    sequence = ee.List.sequence(0, ee.Number(count).subtract(1))\n",
    "    origin_date = ee.Date(start)\n",
    "\n",
    "    def _each(i):\n",
    "        i = ee.Number(i)\n",
    "        start_date = origin_date.advance(i.multiply(interval), units)\n",
    "        end_date   = origin_date.advance(i.add(1).multiply(interval), units)\n",
    "        img = (collection\n",
    "               .filterDate(start_date, end_date)\n",
    "               .mean()\n",
    "               .set('system:time_start', start_date.millis())\n",
    "               .set('date_daily', start_date.format('YYYY-MM-dd'))\n",
    "               .set('system:id', start_date.format('YYYY-MM-dd'))\n",
    "               .set('system:index', start_date.format('YYYY-MM-dd')))\n",
    "        return img\n",
    "\n",
    "    return ee.ImageCollection(sequence.map(_each))\n",
    "\n",
    "# ======================= Source Dataset (CFSv2) ======================\n",
    "# NOAA/CFSV2/FOR6H: 6-hourly forecast fields\n",
    "NCEP_raw = (ee.ImageCollection(\"NOAA/CFSV2/FOR6H\")\n",
    "            .filterDate(startDate, endDate)\n",
    "            .filterBounds(AOI)\n",
    "            .select([\n",
    "                \"Temperature_height_above_ground\",\n",
    "                \"Downward_Short-Wave_Radiation_Flux_surface_6_Hour_Average\"\n",
    "            ]))\n",
    "\n",
    "# ================== Daily Aggregation (Ta_C, Rs) =====================\n",
    "# Build daily bins using time_warp, convert Kelvin to °C, clip to AOI, keep metadata.\n",
    "NCEP_daily = (time_warp(\n",
    "                    NCEP_raw,\n",
    "                    start=startDate,\n",
    "                    count=endDate.difference(startDate, 'day'),\n",
    "                    interval=1,\n",
    "                    units='day')\n",
    "              .map(lambda image:\n",
    "                   (image\n",
    "                    .select(\"Downward_Short-Wave_Radiation_Flux_surface_6_Hour_Average\")\n",
    "                    .rename(\"Rs\")\n",
    "                    .addBands(\n",
    "                        image.select(\"Temperature_height_above_ground\")\n",
    "                             .subtract(273.15)  # K → °C\n",
    "                             .rename(\"Ta_C\")\n",
    "                    )\n",
    "                    .clip(AOI)\n",
    "                    .copyProperties(image, image.propertyNames())\n",
    "                    .set(\"date_daily\", image.date().format(\"YYYY-MM-dd\"))\n",
    "                    .set(\"system:id\", image.date().format(\"YYYY-MM-dd H:mm\"))\n",
    "                    ))\n",
    "              )\n",
    "\n",
    "# Bands to export (kept identical to original behavior)\n",
    "bands_weather_daily = [\"Ta_C\", \"Rs\"]\n",
    "\n",
    "# ======================= Region Reduce to AOI ========================\n",
    "def _reduce_daily_means(image: ee.Image) -> ee.Feature:\n",
    "    \"\"\"\n",
    "    Reduce selected bands over AOI to daily means.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ee.Feature\n",
    "        Feature with properties {'date', 'Ta_C', 'Rs'}\n",
    "    \"\"\"\n",
    "    stats = image.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=AOI,\n",
    "        scale=30,          # kept as in original script to preserve behavior\n",
    "        bestEffort=True,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "    # Attach the date string for tabular export\n",
    "    return ee.Feature(None, stats.set('date', image.date().format('YYYY-MM-dd')))\n",
    "\n",
    "features_daily = NCEP_daily.select(bands_weather_daily).map(_reduce_daily_means)\n",
    "\n",
    "# ========================= Client-Side Table ==========================\n",
    "# Pull results to client and assemble pandas DataFrame (no prints)\n",
    "_feature_collection = features_daily.getInfo()  # triggers server → client transfer\n",
    "\n",
    "_records = []\n",
    "for feat in _feature_collection['features']:\n",
    "    props = feat['properties']\n",
    "    row = [props.get('date')] + [props.get(k, None) for k in bands_weather_daily]\n",
    "    _records.append(row)\n",
    "\n",
    "df_daily = pd.DataFrame(_records, columns=['date'] + bands_weather_daily)\n",
    "df_daily['date'] = pd.to_datetime(df_daily['date'], errors='coerce')\n",
    "df_daily = df_daily.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Final air temperature DataFrame\n",
    "df_daily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ef0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== viirs_timeseries.py ======================\n",
    "\"\"\"\n",
    "Per-polygon time series of VIIRS I04 Radiance Δ(max − P25)/Δ(max + P25), with DAY & NIGHT lines,\n",
    "and daily near-surface air temperature (Ta_C) bars from an external DataFrame `df_daily`, generated in air_temperature.py.\n",
    "\n",
    "Metric per polygon & timestamp\n",
    "------------------------------\n",
    "  delta = max(R) − P25(R)        [R in W m^-2 sr^-1 µm^-1]\n",
    "  R     = pixel radiances within the polygon (NaN-aware)\n",
    "  P25   = 25th percentile via np.nanpercentile(..., 25)\n",
    "\n",
    "IMPORTANT: This implementation returns a normalized value:\n",
    "  delta_norm = (max − P25) / (max + P25)   ∈ [0, 1]\n",
    "This preserves your current behavior without altering functionality.\n",
    "\n",
    "Day/Night split\n",
    "---------------\n",
    "For each polygon centroid and timestamp, compute Solar Zenith Angle (SZA):\n",
    "  • Day   if SZA < SZA_DAY_THRESHOLD_DEG (default 88°)\n",
    "  • Night otherwise\n",
    "\n",
    "I/O layout\n",
    "----------\n",
    "Inputs searched in BASEDIR/YYYY-MM-DD/bt/:  *_I04_Rad.tif\n",
    "Outputs per polygon:\n",
    "  • CSV (UTC): datetime_utc, period{day|night}, delta_I04_Wm2_sr_um\n",
    "  • Inline plot (local time): Day=red solid, Night=blue dashed; points labeled HH:MM\n",
    "  • Optional background bars (secondary y-axis): daily Ta_C from `df_daily` (date, Ta_C)\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "  Install numpy pandas rasterio fiona shapely pyproj matplotlib tzdata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Imports ===============================\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import transform as shp_transform\n",
    "import fiona\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from zoneinfo import ZoneInfo  # Python 3.9+\n",
    "\n",
    "\n",
    "# =========================== Configuration ============================\n",
    "\n",
    "# Base directory containing day folders like YYYY-MM-DD/bt/*.tif\n",
    "BASEDIR: Path    = Path(r\"path\\to\\base\\directory\")\n",
    "START_DATE: date = date(2025, 8, 9)\n",
    "END_DATE:   date = date(2025, 8, 12)\n",
    "\n",
    "# Local time zone for plotting (lines and bar labels)\n",
    "LOCAL_TZNAME: str = \"America/Chicago\" # e.g., \"America/Los_Angeles\"\n",
    "LOCAL_TZ: ZoneInfo = ZoneInfo(LOCAL_TZNAME)\n",
    "\n",
    "# Day/night threshold (Day if SZA < threshold)\n",
    "SZA_DAY_THRESHOLD_DEG: float = 88.0\n",
    "\n",
    "# Polygons shapefile (must contain an integer 'id' or 'ID' attribute)\n",
    "POLY_SHP: Path = Path(\n",
    "    r\"path\\to\\F002_L1__IR__L2L1M0__2025-08-12T212259.010953Z_2025-08-13T111644.731945Z_e81989f5_model_detections_Polygon.shp\"\n",
    ")\n",
    "\n",
    "# Radiance file pattern (inside per-day \"bt\" folder)\n",
    "PAT_I04: str = \"*_I04_Rad.tif\"\n",
    "\n",
    "# Output directories for per-polygon CSVs\n",
    "OUTDIR: Path  = BASEDIR / \"timeseries\"\n",
    "OUT_CSV: Path = OUTDIR / \"csv\"\n",
    "OUT_CSV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================== Date Utilities ===========================\n",
    "\n",
    "def date_iter(d0: date, d1: date):\n",
    "    \"\"\"Inclusive day iterator from d0 to d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def timestamp_key_from_name(name: str) -> str | None:\n",
    "    \"\"\"Extract '.Ayyyyddd.HHMM.' key from a filename (VIIRS convention).\"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def dt_from_key(key: str) -> datetime | None:\n",
    "    \"\"\"'.Ayyyyddd.HHMM' → aware UTC datetime.\"\"\"\n",
    "    m = re.match(r\"A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})$\", key)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "def gather_i04_index() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect timestamps and file paths for I04 Radiance granules\n",
    "    across BASEDIR/YYYY-MM-DD/bt/ folders that match PAT_I04.\n",
    "    Returns a list sorted by UTC datetime.\n",
    "    \"\"\"\n",
    "    recs: Dict[str, Dict[str, Any]] = {}\n",
    "    for day in date_iter(START_DATE, END_DATE):\n",
    "        btdir = BASEDIR / day.strftime(\"%Y-%m-%d\") / \"bt\"\n",
    "        if not btdir.exists():\n",
    "            continue\n",
    "        for p in btdir.glob(PAT_I04):\n",
    "            key = timestamp_key_from_name(p.name)\n",
    "            if not key:\n",
    "                continue\n",
    "            dt = dt_from_key(key)\n",
    "            if not dt:\n",
    "                continue\n",
    "            recs[key] = {\"dt\": dt, \"i04\": p}\n",
    "    # Sort by datetime (ascending)\n",
    "    return [recs[k] for k in sorted(recs.keys(), key=lambda kk: recs[kk][\"dt\"])]\n",
    "\n",
    "\n",
    "# =========================== Polygon Handling =========================\n",
    "\n",
    "def read_polygons_wgs84(shp_path: Path) -> List[Tuple[int, Any, float, float]]:\n",
    "    \"\"\"\n",
    "    Read polygon features and return them in EPSG:4326.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        (poly_id: int, geometry: shapely in EPSG:4326, centroid_lat, centroid_lon)\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"Polygon shapefile not found: {shp_path}\")\n",
    "\n",
    "    polys: List[Tuple[int, Any, float, float]] = []\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        src_crs = src.crs\n",
    "        if not src_crs:\n",
    "            raise ValueError(\"Input shapefile has no CRS.\")\n",
    "        src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "        dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "\n",
    "        if src_crs_obj != dst_crs_obj:\n",
    "            transformer = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "            reproj = lambda x, y, z=None: transformer.transform(x, y)\n",
    "        else:\n",
    "            reproj = None\n",
    "\n",
    "        for feat in src:\n",
    "            props = feat.get(\"properties\", {})\n",
    "            poly_id = int(props.get(\"id\", props.get(\"ID\", len(polys) + 1)))\n",
    "            geom = shape(feat[\"geometry\"]).buffer(0)  # make valid\n",
    "            if reproj:\n",
    "                geom = shp_transform(reproj, geom)\n",
    "            c = geom.centroid\n",
    "            polys.append((poly_id, geom, float(c.y), float(c.x)))  # (id, geom, lat, lon)\n",
    "\n",
    "    if not polys:\n",
    "        raise ValueError(\"No polygons found in shapefile.\")\n",
    "    return polys\n",
    "\n",
    "\n",
    "# ========================== Solar Geometry ============================\n",
    "\n",
    "def solar_zenith_deg(lat_deg: float, lon_deg: float, dt_utc: datetime) -> float:\n",
    "    \"\"\"\n",
    "    Approximate Solar Zenith Angle (SZA) in degrees. Sufficient for day/night split.\n",
    "    \"\"\"\n",
    "    y, m, d = dt_utc.year, dt_utc.month, dt_utc.day\n",
    "    hh, mm = dt_utc.hour, dt_utc.minute\n",
    "    ss = dt_utc.second + dt_utc.microsecond / 1e6\n",
    "\n",
    "    # Julian Day (Meeus-like)\n",
    "    a = math.floor((14 - m) / 12)\n",
    "    y2 = y + 4800 - a\n",
    "    m2 = m + 12 * a - 3\n",
    "    jd = d + math.floor((153 * m2 + 2) / 5) + 365 * y2 + math.floor(y2 / 4) \\\n",
    "         - math.floor(y2 / 100) + math.floor(y2 / 400) - 32045\n",
    "    frac_day = (hh + mm / 60 + ss / 3600) / 24.0\n",
    "    JD = jd + frac_day\n",
    "    T = (JD - 2451545.0) / 36525.0\n",
    "\n",
    "    # Solar coordinates\n",
    "    L0 = (280.46646 + 36000.76983 * T + 0.0003032 * T * T) % 360\n",
    "    M = math.radians((357.52911 + 35999.05029 * T - 0.0001537 * T * T) % 360)\n",
    "    e = 0.016708634 - 0.000042037 * T - 0.0000001267 * T * T\n",
    "    C = (1.914602 - 0.004817 * T - 0.000014 * T * T) * math.sin(M) + \\\n",
    "        (0.019993 - 0.000101 * T) * math.sin(2 * M) + 0.000289 * math.sin(3 * M)\n",
    "    lam_true = math.radians((L0 + C) % 360)\n",
    "    omega = math.radians(125.04 - 1934.136 * T)\n",
    "    lam_app = lam_true - math.radians(0.00569) - math.radians(0.00478) * math.sin(omega)\n",
    "\n",
    "    eps0 = 23 + (26 + (21.448 - T * (46.815 + T * (0.00059 - 0.001813 * T))) / 60) / 60\n",
    "    eps = math.radians(eps0 + 0.00256 * math.cos(omega))\n",
    "\n",
    "    # Declination\n",
    "    sin_delta = math.sin(eps) * math.sin(lam_app)\n",
    "    delta = math.asin(sin_delta)\n",
    "\n",
    "    # Equation of Time (minutes)\n",
    "    y_eps = math.tan(eps / 2) ** 2\n",
    "    L0r = math.radians(L0)\n",
    "    E = 4 * math.degrees(\n",
    "        y_eps * math.sin(2 * L0r) - 2 * e * math.sin(M) + 4 * e * y_eps * math.sin(M) * math.cos(2 * L0r)\n",
    "        - 0.5 * y_eps * y_eps * math.sin(4 * L0r) - 1.25 * e * e * math.sin(2 * M)\n",
    "    )\n",
    "\n",
    "    # True Solar Time (minutes)\n",
    "    utc_min = hh * 60 + mm + ss / 60\n",
    "    TST = (utc_min + E + 4 * lon_deg) % 1440\n",
    "    H = math.radians(TST / 4 - 180)  # hour angle\n",
    "\n",
    "    # Zenith angle\n",
    "    phi = math.radians(lat_deg)\n",
    "    cos_zen = math.sin(phi) * math.sin(delta) + math.cos(phi) * math.cos(delta) * math.cos(H)\n",
    "    cos_zen = max(-1.0, min(1.0, cos_zen))\n",
    "    return math.degrees(math.acos(cos_zen))\n",
    "\n",
    "\n",
    "# ===================== Polygon Metric (Δ(max−P25)) ====================\n",
    "\n",
    "def _polygon_values(raster_path: Path, polygon_geom):\n",
    "    \"\"\"\n",
    "    Return all pixel values inside polygon as a 1D array (finite only).\n",
    "    Reprojects polygon to raster CRS if needed.\n",
    "    \"\"\"\n",
    "    if raster_path is None:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    with rasterio.open(raster_path) as ds:\n",
    "        arr = ds.read(1).astype(np.float32)\n",
    "        # Reproject polygon → raster CRS if necessary\n",
    "        if ds.crs and ds.crs.to_epsg() != 4326:\n",
    "            transformer = Transformer.from_crs(PJCRS.from_epsg(4326), ds.crs, always_xy=True)\n",
    "            geom = shp_transform(lambda x, y, z=None: transformer.transform(x, y), polygon_geom)\n",
    "        else:\n",
    "            geom = polygon_geom\n",
    "        mask = geometry_mask([mapping(geom)], out_shape=arr.shape, transform=ds.transform, invert=True)\n",
    "        vals = arr[mask]\n",
    "        return vals[np.isfinite(vals)]\n",
    "\n",
    "\n",
    "def poly_delta_max_p25(raster_path: Path, polygon_geom) -> float:\n",
    "    \"\"\"\n",
    "    Compute normalized Δ(max−P25) for pixel radiances within a polygon.\n",
    "\n",
    "    NOTE: Preserves existing behavior — returns normalized value in [0,1]:\n",
    "          (max - P25) / (max + P25). Returns NaN if no valid pixels.\n",
    "    \"\"\"\n",
    "    v = _polygon_values(raster_path, polygon_geom)\n",
    "    if v.size == 0:\n",
    "        return np.nan\n",
    "    p25 = np.nanpercentile(v, 25)\n",
    "    vmax = np.nanmax(v)\n",
    "    return float((vmax - p25) / (vmax + p25))\n",
    "\n",
    "\n",
    "# ================================ Main ================================\n",
    "\n",
    "def main() -> None:\n",
    "    # ---- Gather inputs (I04 granules & polygons) ----\n",
    "    recs = gather_i04_index()\n",
    "    if not recs:\n",
    "        raise SystemExit(\"No I04 Radiance granules found in the given range.\")\n",
    "    poly_list = read_polygons_wgs84(POLY_SHP)  # (id, geom, lat, lon)\n",
    "    print(f\"[INFO] Found {len(poly_list)} polygons and {len(recs)} I04 timestamps.\")\n",
    "\n",
    "    # Per-polygon containers (UTC CSV + local plotting series)\n",
    "    series: Dict[int, Dict[str, List[Any]]] = {\n",
    "        pid: {\n",
    "            # CSV (UTC, union day+night)\n",
    "            \"dt\": [], \"period\": [], \"delta\": [],\n",
    "            # Plotting (local timezone, separate series)\n",
    "            \"day_dt_loc\": [], \"day_delta\": [],\n",
    "            \"ngt_dt_loc\": [], \"ngt_delta\": [],\n",
    "        }\n",
    "        for pid, _, _, _ in poly_list\n",
    "    }\n",
    "\n",
    "    # ---- Iterate timestamps; compute deltas; split day/night ----\n",
    "    for rec in recs:\n",
    "        dt_utc: datetime = rec[\"dt\"]   # aware UTC\n",
    "        p04: Path = rec[\"i04\"]\n",
    "        for pid, geom, plat, plon in poly_list:\n",
    "            sza = solar_zenith_deg(plat, plon, dt_utc)\n",
    "            if not np.isfinite(sza):\n",
    "                continue\n",
    "            d04 = poly_delta_max_p25(p04, geom) if p04 else np.nan\n",
    "            if not np.isfinite(d04):\n",
    "                continue\n",
    "\n",
    "            period = \"day\" if sza < SZA_DAY_THRESHOLD_DEG else \"night\"\n",
    "\n",
    "            # CSV (UTC)\n",
    "            series[pid][\"dt\"].append(dt_utc)\n",
    "            series[pid][\"period\"].append(period)\n",
    "            series[pid][\"delta\"].append(d04)\n",
    "\n",
    "            # Plotting (local)\n",
    "            dt_loc = dt_utc.astimezone(LOCAL_TZ)\n",
    "            if period == \"day\":\n",
    "                series[pid][\"day_dt_loc\"].append(dt_loc)\n",
    "                series[pid][\"day_delta\"].append(d04)\n",
    "            else:\n",
    "                series[pid][\"ngt_dt_loc\"].append(dt_loc)\n",
    "                series[pid][\"ngt_delta\"].append(d04)\n",
    "\n",
    "    # ---- Prepare daily temperature bars from global df_daily (if provided) ----\n",
    "    def prepare_daily_bars() -> Tuple[List[datetime] | None, np.ndarray | None]:\n",
    "        \"\"\"\n",
    "        Build bar positions (local-noon datetimes) and values (°C) from a global DataFrame `df_daily`\n",
    "        with columns {'date', 'Ta_C'}. If absent or invalid, returns (None, None).\n",
    "        \"\"\"\n",
    "        if \"df_daily\" not in globals():\n",
    "            return None, None\n",
    "        df = globals()[\"df_daily\"]\n",
    "        if not isinstance(df, pd.DataFrame) or not {\"date\", \"Ta_C\"}.issubset(df.columns):\n",
    "            return None, None\n",
    "\n",
    "        # Parse and localize to plotting TZ\n",
    "        date_ser = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        if date_ser.isna().all():\n",
    "            return None, None\n",
    "        if date_ser.dt.tz is None:\n",
    "            date_ser = date_ser.dt.tz_localize(LOCAL_TZ)\n",
    "        else:\n",
    "            date_ser = date_ser.dt.tz_convert(LOCAL_TZ)\n",
    "\n",
    "        # Center bars at local noon for each date\n",
    "        noon_ser = date_ser.dt.normalize() + pd.Timedelta(hours=12)\n",
    "        bar_times = noon_ser.dt.to_pydatetime().tolist()\n",
    "\n",
    "        # Clean temperature values\n",
    "        ta = pd.to_numeric(df[\"Ta_C\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "        # Keep finite temps and valid datetimes\n",
    "        mask = np.isfinite(ta.values) & pd.notna(noon_ser.values)\n",
    "        bar_times = [bar_times[i] for i in range(len(bar_times)) if mask[i]]\n",
    "        bar_vals = ta.values[mask]\n",
    "        return bar_times, bar_vals\n",
    "\n",
    "    bar_times, bar_vals = prepare_daily_bars()\n",
    "\n",
    "    # ===================== CSV export + plotting =======================\n",
    "    for pid, data in series.items():\n",
    "        if (not data[\"day_dt_loc\"]) and (not data[\"ngt_dt_loc\"]):\n",
    "            print(f\"[WARN] Polygon id={pid}: no valid samples.\")\n",
    "            continue\n",
    "\n",
    "        # ---- CSV (UTC), sorted by datetime ----\n",
    "        order = np.argsort(data[\"dt\"])\n",
    "        dts_utc = [data[\"dt\"][i] for i in order]\n",
    "        per     = [data[\"period\"][i] for i in order]\n",
    "        delt    = [data[\"delta\"][i] for i in order]\n",
    "\n",
    "        csv_path = OUT_CSV / f\"poly_{pid:03d}_timeseries_deltaP25_I04_day_night.csv\"\n",
    "        with csv_path.open(\"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"datetime_utc\", \"period\", \"delta_I04_Wm2_sr_um\"])  # Δ(max−P25), normalized in this script\n",
    "            for t, p, x in zip(dts_utc, per, delt):\n",
    "                xs = \"\" if not np.isfinite(x) else f\"{x:.6f}\"\n",
    "                w.writerow([t.strftime(\"%Y-%m-%dT%H:%MZ\"), p, xs])\n",
    "        print(f\"[OK] {csv_path}\")\n",
    "\n",
    "        # ---- Build plotting series (local), each sorted independently ----\n",
    "        day_t: List[datetime] = []\n",
    "        day_v: List[float] = []\n",
    "        ngt_t: List[datetime] = []\n",
    "        ngt_v: List[float] = []\n",
    "\n",
    "        if data[\"day_dt_loc\"]:\n",
    "            d_ord = np.argsort(data[\"day_dt_loc\"])\n",
    "            day_t = [data[\"day_dt_loc\"][i] for i in d_ord]\n",
    "            day_v = [data[\"day_delta\"][i]  for i in d_ord]\n",
    "\n",
    "        if data[\"ngt_dt_loc\"]:\n",
    "            n_ord = np.argsort(data[\"ngt_dt_loc\"])\n",
    "            ngt_t = [data[\"ngt_dt_loc\"][i] for i in n_ord]\n",
    "            ngt_v = [data[\"ngt_delta\"][i]  for i in n_ord]\n",
    "\n",
    "        # ---- Plot: lines (day/night) + optional Ta_C bars ----\n",
    "        fig, ax = plt.subplots(figsize=(11, 4.8))\n",
    "\n",
    "        # Background Ta_C bars on secondary y-axis (if provided)\n",
    "        ax2 = None\n",
    "        if bar_times is not None and bar_vals is not None and len(bar_times) > 0:\n",
    "            ax2 = ax.twinx()\n",
    "            # Ensure lines render above bars\n",
    "            ax.set_zorder(2); ax.patch.set_alpha(0.0)\n",
    "            ax2.set_zorder(1)\n",
    "            ax2.bar(bar_times, bar_vals, width=0.8, alpha=0.25, color=\"gray\",\n",
    "                    label=\"Ta (°C)\", zorder=0, align=\"center\")\n",
    "            ax2.set_ylabel(\"Air temperature (°C)\")\n",
    "            ax2.set_ylim(20, 35)  # keep your current fixed range\n",
    "            ax2.grid(False)\n",
    "\n",
    "        # Day series (solid red, circle markers)\n",
    "        if day_t:\n",
    "            ax.plot(day_t, day_v, marker=\"o\", linewidth=1.6, color=\"red\", label=\"Day\", zorder=3)\n",
    "            for t, v in zip(day_t, day_v):\n",
    "                ax.annotate(t.strftime(\"%H:%M\"), (t, v), textcoords=\"offset points\",\n",
    "                            xytext=(0, 6), ha=\"center\", fontsize=8, color=\"black\")\n",
    "\n",
    "        # Night series (blue dashed, square markers)\n",
    "        if ngt_t:\n",
    "            ax.plot(ngt_t, ngt_v, marker=\"s\", linestyle=\"--\", linewidth=1.6, color=\"blue\", label=\"Night\", zorder=3)\n",
    "            for t, v in zip(ngt_t, ngt_v):\n",
    "                ax.annotate(t.strftime(\"%H:%M\"), (t, v), textcoords=\"offset points\",\n",
    "                            xytext=(0, -10), ha=\"center\", fontsize=8, color=\"black\")\n",
    "\n",
    "        # Fixed y-axis for normalized metric\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel(f\"Local time ({LOCAL_TZNAME})\")\n",
    "        ax.set_ylabel(r\"Normalized $\\Delta(\\max - P25)$ (−)\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\", tz=LOCAL_TZ))\n",
    "        fig.autofmt_xdate()\n",
    "\n",
    "        # Combined legend (include Ta bars if present)\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        if ax2 is not None:\n",
    "            handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "            ax.legend(handles1 + handles2, labels1 + labels2, loc=\"upper left\")\n",
    "        else:\n",
    "            ax.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.title(f\"Polygon id={pid} — VIIRS I04 Normalized Δ(max−P25)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================== main =================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= modis_timeseries.py =======================\n",
    "\"\"\"\n",
    "Per-polygon time series of MODIS B21 Radiance Δ(max − P25), normalized,\n",
    "with DAY & NIGHT lines and optional daily Ta_C bars from a global `df_daily` from air_temperature.py.\n",
    "\n",
    "Metric per polygon & timestamp\n",
    "------------------------------\n",
    "  Δ_norm = (max(R) − P25(R)) / (max(R) + P25(R))     ∈ [0, 1]\n",
    "  R      = pixel radiances within polygon (NaN-aware)\n",
    "  P25    = 25th percentile via np.nanpercentile(..., 25)\n",
    "\n",
    "Inputs searched: BASEDIR / YYYY-MM-DD / bt / *_B21_Rad.tif\n",
    "Outputs (per polygon)\n",
    "---------------------\n",
    "  • CSV (UTC): datetime_utc, period{day|night}, delta_norm_B21\n",
    "  • Inline plot (local time): Day=red solid, Night=blue dashed; y-lim [0, 1]\n",
    "  • Optional bars (secondary y-axis): daily Ta_C from global `df_daily` (columns: date, Ta_C)\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "Install numpy pandas rasterio fiona shapely pyproj matplotlib tzdata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Imports ===============================\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import transform as shp_transform\n",
    "import fiona\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "\n",
    "# =========================== Configuration ============================\n",
    "\n",
    "# Root folder containing dated subfolders: YYYY-MM-DD/bt/*.tif\n",
    "BASEDIR: Path = Path(\n",
    "    r\"path\\to\\base\\directory\"\n",
    ")\n",
    "\n",
    "# Date range (inclusive)\n",
    "START_DATE: date = date(2025, 1, 4)\n",
    "END_DATE:   date = date(2025, 1, 11)\n",
    "\n",
    "# Plotting timezone (used for line timestamps and x-axis labels)\n",
    "LOCAL_TZNAME: str     = \"America/Los_Angeles\" # e.g., \"America/Chicago\"\n",
    "LOCAL_TZ: ZoneInfo    = ZoneInfo(LOCAL_TZNAME)\n",
    "\n",
    "# Day/night threshold (Day if SZA < threshold, otherwise Night)\n",
    "SZA_DAY_THRESHOLD_DEG: float = 88.0\n",
    "\n",
    "# Polygons shapefile (must have integer 'id' or 'ID' attribute)\n",
    "POLY_SHP: Path = Path(\n",
    "    r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Polygon.shp\"\n",
    ")\n",
    "\n",
    "# Radiance band/pattern (inside each day's \"bt\" folder)\n",
    "BAND_LABEL:  str = \"B21\"\n",
    "PATTERN_RAD: str = f\"*_{BAND_LABEL}_Rad.tif\"  # -> \"*_B21_Rad.tif\"\n",
    "\n",
    "# Output directories for per-polygon CSVs\n",
    "OUTDIR: Path  = BASEDIR / \"timeseries_modis\"\n",
    "OUT_CSV: Path = OUTDIR / \"csv\"\n",
    "OUT_CSV.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================== Utilities =============================\n",
    "\n",
    "def date_iter(d0: date, d1: date):\n",
    "    \"\"\"Inclusive iterator from d0 to d1 (step=1 day).\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def timestamp_key_from_name(name: str) -> str | None:\n",
    "    \"\"\"Extract MODIS/VIIRS-like timestamp key: '.Ayyyyddd.HHMM.' from filename.\"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def dt_from_key(key: str) -> datetime | None:\n",
    "    \"\"\"Convert 'Ayyyyddd.HHMM' → aware UTC datetime.\"\"\"\n",
    "    m = re.match(r\"A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})$\", key)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "def gather_rad_index() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect timestamps & paths for MODIS Radiance granules in per-day bt/ folders.\n",
    "    Returns a list of records sorted by UTC datetime: {'dt': datetime, 'rad': Path}.\n",
    "    \"\"\"\n",
    "    recs: Dict[str, Dict[str, Any]] = {}\n",
    "    for day in date_iter(START_DATE, END_DATE):\n",
    "        btdir = BASEDIR / day.strftime(\"%Y-%m-%d\") / \"bt\"\n",
    "        if not btdir.exists():\n",
    "            continue\n",
    "        for p in btdir.glob(PATTERN_RAD):\n",
    "            key = timestamp_key_from_name(p.name)\n",
    "            if not key:\n",
    "                continue\n",
    "            dt = dt_from_key(key)\n",
    "            if not dt:\n",
    "                continue\n",
    "            recs[key] = {\"dt\": dt, \"rad\": p}\n",
    "    return [recs[k] for k in sorted(recs.keys(), key=lambda kk: recs[kk][\"dt\"])]\n",
    "\n",
    "\n",
    "def read_polygons_wgs84(shp_path: Path) -> List[Tuple[int, Any, float, float]]:\n",
    "    \"\"\"\n",
    "    Read polygons and return them in EPSG:4326 with centroid lat/lon.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[(poly_id, shapely_geom_4326, centroid_lat, centroid_lon)]\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"Polygon shapefile not found: {shp_path}\")\n",
    "\n",
    "    polys: List[Tuple[int, Any, float, float]] = []\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        src_crs = src.crs\n",
    "        if not src_crs:\n",
    "            raise ValueError(\"Input shapefile has no CRS.\")\n",
    "        src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "        dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "\n",
    "        if src_crs_obj != dst_crs_obj:\n",
    "            transformer = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "            reproj = lambda x, y, z=None: transformer.transform(x, y)\n",
    "        else:\n",
    "            reproj = None\n",
    "\n",
    "        for feat in src:\n",
    "            props = feat.get(\"properties\", {})\n",
    "            poly_id = int(props.get(\"id\", props.get(\"ID\", len(polys) + 1)))\n",
    "            geom = shape(feat[\"geometry\"]).buffer(0)  # make valid if needed\n",
    "            if reproj:\n",
    "                geom = shp_transform(reproj, geom)\n",
    "            c = geom.centroid\n",
    "            polys.append((poly_id, geom, float(c.y), float(c.x)))\n",
    "\n",
    "    if not polys:\n",
    "        raise ValueError(\"No polygons found in shapefile.\")\n",
    "    return polys\n",
    "\n",
    "\n",
    "# =========================== Solar Geometry ===========================\n",
    "\n",
    "def solar_zenith_deg(lat_deg: float, lon_deg: float, dt_utc: datetime) -> float:\n",
    "    \"\"\"\n",
    "    Approximate solar zenith angle (degrees) for a given UTC datetime and lat/lon.\n",
    "    Accuracy is sufficient for day/night classification.\n",
    "    \"\"\"\n",
    "    y, m, d = dt_utc.year, dt_utc.month, dt_utc.day\n",
    "    hh, mm = dt_utc.hour, dt_utc.minute\n",
    "    ss = dt_utc.second + dt_utc.microsecond / 1e6\n",
    "\n",
    "    # Julian Day\n",
    "    a = math.floor((14 - m) / 12)\n",
    "    y2 = y + 4800 - a\n",
    "    m2 = m + 12 * a - 3\n",
    "    jd = d + math.floor((153 * m2 + 2) / 5) + 365 * y2 + math.floor(y2 / 4) \\\n",
    "         - math.floor(y2 / 100) + math.floor(y2 / 400) - 32045\n",
    "    frac_day = (hh + mm / 60 + ss / 3600) / 24.0\n",
    "    JD = jd + frac_day\n",
    "    T = (JD - 2451545.0) / 36525.0\n",
    "\n",
    "    # Solar coordinates\n",
    "    L0 = (280.46646 + 36000.76983 * T + 0.0003032 * T * T) % 360\n",
    "    M = math.radians((357.52911 + 35999.05029 * T - 0.0001537 * T * T) % 360)\n",
    "    e = 0.016708634 - 0.000042037 * T - 0.0000001267 * T * T\n",
    "    C = (1.914602 - 0.004817 * T - 0.000014 * T * T) * math.sin(M) + \\\n",
    "        (0.019993 - 0.000101 * T) * math.sin(2 * M) + 0.000289 * math.sin(3 * M)\n",
    "    lam_true = math.radians((L0 + C) % 360)\n",
    "    omega = math.radians(125.04 - 1934.136 * T)\n",
    "    lam_app = lam_true - math.radians(0.00569) - math.radians(0.00478) * math.sin(omega)\n",
    "\n",
    "    eps0 = 23 + (26 + (21.448 - T * (46.815 + T * (0.00059 - 0.001813 * T))) / 60) / 60\n",
    "    eps = math.radians(eps0 + 0.00256 * math.cos(omega))\n",
    "\n",
    "    # Declination\n",
    "    sin_delta = math.sin(eps) * math.sin(lam_app)\n",
    "    delta = math.asin(sin_delta)\n",
    "\n",
    "    # Equation of Time (minutes)\n",
    "    y_eps = math.tan(eps / 2) ** 2\n",
    "    L0r = math.radians(L0)\n",
    "    E = 4 * math.degrees(\n",
    "        y_eps * math.sin(2 * L0r) - 2 * e * math.sin(M) + 4 * e * y_eps * math.sin(M) * math.cos(2 * L0r)\n",
    "        - 0.5 * y_eps * y_eps * math.sin(4 * L0r) - 1.25 * e * e * math.sin(2 * M)\n",
    "    )\n",
    "\n",
    "    # True Solar Time (minutes) & hour angle\n",
    "    utc_min = hh * 60 + mm + ss / 60\n",
    "    TST = (utc_min + E + 4 * lon_deg) % 1440\n",
    "    H = math.radians(TST / 4 - 180)\n",
    "\n",
    "    # Zenith angle\n",
    "    phi = math.radians(lat_deg)\n",
    "    cos_zen = math.sin(phi) * math.sin(delta) + math.cos(phi) * math.cos(delta) * math.cos(H)\n",
    "    cos_zen = max(-1.0, min(1.0, cos_zen))\n",
    "    return math.degrees(math.acos(cos_zen))\n",
    "\n",
    "\n",
    "# ==================== Polygon Metric: Δ_norm ==========================\n",
    "\n",
    "def _polygon_values(raster_path: Path, polygon_geom):\n",
    "    \"\"\"\n",
    "    Return all finite pixel values inside polygon as 1D array.\n",
    "    Reprojects polygon → raster CRS if necessary.\n",
    "    \"\"\"\n",
    "    if raster_path is None:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    with rasterio.open(raster_path) as ds:\n",
    "        arr = ds.read(1).astype(np.float32)\n",
    "        if ds.crs and ds.crs.to_epsg() != 4326:\n",
    "            transformer = Transformer.from_crs(PJCRS.from_epsg(4326), ds.crs, always_xy=True)\n",
    "            geom = shp_transform(lambda x, y, z=None: transformer.transform(x, y), polygon_geom)\n",
    "        else:\n",
    "            geom = polygon_geom\n",
    "        mask = geometry_mask([mapping(geom)], out_shape=arr.shape, transform=ds.transform, invert=True)\n",
    "        vals = arr[mask]\n",
    "        return vals[np.isfinite(vals)]\n",
    "\n",
    "\n",
    "def poly_delta_norm(raster_path: Path, polygon_geom) -> float:\n",
    "    \"\"\"\n",
    "    Δ_norm = (max − P25) / (max + P25)\n",
    "    Returns NaN if no valid pixels or denominator ≤ 0.\n",
    "    \"\"\"\n",
    "    v = _polygon_values(raster_path, polygon_geom)\n",
    "    if v.size == 0:\n",
    "        return np.nan\n",
    "    vmax = np.nanmax(v)\n",
    "    p25  = np.nanpercentile(v, 25)\n",
    "    denom = vmax + p25\n",
    "    if not np.isfinite(denom) or denom <= 0:\n",
    "        return np.nan\n",
    "    return float((vmax - p25) / denom)\n",
    "\n",
    "\n",
    "# ================================ Main ================================\n",
    "\n",
    "def main() -> None:\n",
    "    # ---- Gather inputs ----\n",
    "    recs = gather_rad_index()\n",
    "    if not recs:\n",
    "        raise SystemExit(f\"No MODIS {BAND_LABEL} Radiance granules found in the given range.\")\n",
    "    poly_list = read_polygons_wgs84(POLY_SHP)  # (id, geom, lat, lon)\n",
    "    print(f\"[INFO] Found {len(poly_list)} polygons and {len(recs)} timestamps.\")\n",
    "\n",
    "    # Per-polygon storage (UTC CSV + local plotting series)\n",
    "    series: Dict[int, Dict[str, List[Any]]] = {\n",
    "        pid: {\n",
    "            \"dt\": [], \"period\": [], \"delta\": [],\n",
    "            \"day_dt_loc\": [], \"day_delta\": [],\n",
    "            \"ngt_dt_loc\": [], \"ngt_delta\": [],\n",
    "        } for pid, _, _, _ in poly_list\n",
    "    }\n",
    "\n",
    "    # ---- Iterate timestamps; compute Δ_norm; split day/night ----\n",
    "    for rec in recs:\n",
    "        dt_utc: datetime = rec[\"dt\"]  # aware UTC\n",
    "        p_rad: Path      = rec[\"rad\"]\n",
    "        for pid, geom, plat, plon in poly_list:\n",
    "            sza = solar_zenith_deg(plat, plon, dt_utc)\n",
    "            if not np.isfinite(sza):\n",
    "                continue\n",
    "\n",
    "            dval = poly_delta_norm(p_rad, geom) if p_rad else np.nan\n",
    "            if not np.isfinite(dval):\n",
    "                continue\n",
    "\n",
    "            period = \"day\" if sza < SZA_DAY_THRESHOLD_DEG else \"night\"\n",
    "\n",
    "            # CSV (UTC)\n",
    "            series[pid][\"dt\"].append(dt_utc)\n",
    "            series[pid][\"period\"].append(period)\n",
    "            series[pid][\"delta\"].append(dval)\n",
    "\n",
    "            # Plotting (local)\n",
    "            dt_loc = dt_utc.astimezone(LOCAL_TZ)\n",
    "            if period == \"day\":\n",
    "                series[pid][\"day_dt_loc\"].append(dt_loc)\n",
    "                series[pid][\"day_delta\"].append(dval)\n",
    "            else:\n",
    "                series[pid][\"ngt_dt_loc\"].append(dt_loc)\n",
    "                series[pid][\"ngt_delta\"].append(dval)\n",
    "\n",
    "    # ---- Optional: daily temperature bars from global df_daily ----\n",
    "    def prepare_daily_bars():\n",
    "        \"\"\"\n",
    "        Return (bar_times_local_noon, bar_vals) from a global `df_daily` with\n",
    "        columns {'date', 'Ta_C'}. If absent/invalid → (None, None).\n",
    "        \"\"\"\n",
    "        if \"df_daily\" not in globals():\n",
    "            return None, None\n",
    "        df = globals()[\"df_daily\"]\n",
    "        if not isinstance(df, pd.DataFrame) or not {\"date\", \"Ta_C\"}.issubset(df.columns):\n",
    "            return None, None\n",
    "\n",
    "        date_ser = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        if date_ser.isna().all():\n",
    "            return None, None\n",
    "        if date_ser.dt.tz is None:\n",
    "            date_ser = date_ser.dt.tz_localize(LOCAL_TZ)\n",
    "        else:\n",
    "            date_ser = date_ser.dt.tz_convert(LOCAL_TZ)\n",
    "\n",
    "        noon_ser = date_ser.dt.normalize() + pd.Timedelta(hours=12)\n",
    "        t_mid = noon_ser.dt.to_pydatetime().tolist()\n",
    "        ta = pd.to_numeric(df[\"Ta_C\"], errors=\"coerce\").astype(float).values\n",
    "        mask = np.isfinite(ta) & pd.notna(noon_ser).values\n",
    "        bar_times = [t_mid[i] for i in range(len(t_mid)) if mask[i]]\n",
    "        bar_vals  = ta[mask]\n",
    "        return bar_times, bar_vals\n",
    "\n",
    "    bar_times, bar_vals = prepare_daily_bars()\n",
    "\n",
    "    # ===================== CSV export + plotting ======================\n",
    "    for pid, data in series.items():\n",
    "        if (not data[\"day_dt_loc\"]) and (not data[\"ngt_dt_loc\"]):\n",
    "            print(f\"[WARN] Polygon id={pid}: no valid samples.\")\n",
    "            continue\n",
    "\n",
    "        # ---- CSV (UTC), sorted by datetime ----\n",
    "        order = np.argsort(data[\"dt\"])\n",
    "        dts_utc = [data[\"dt\"][i]     for i in order]\n",
    "        per     = [data[\"period\"][i] for i in order]\n",
    "        delt    = [data[\"delta\"][i]  for i in order]\n",
    "\n",
    "        csv_path = OUT_CSV / f\"poly_{pid:03d}_timeseries_deltaNorm_{BAND_LABEL}_day_night.csv\"\n",
    "        with csv_path.open(\"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"datetime_utc\", \"period\", f\"delta_norm_{BAND_LABEL}\"])\n",
    "            for t, p, x in zip(dts_utc, per, delt):\n",
    "                xs = \"\" if not np.isfinite(x) else f\"{x:.6f}\"\n",
    "                w.writerow([t.strftime(\"%Y-%m-%dT%H:%MZ\"), p, xs])\n",
    "        print(f\"[OK] {csv_path}\")\n",
    "\n",
    "        # ---- Build plotting series (local), each sorted independently ----\n",
    "        day_t: List[datetime] = []\n",
    "        day_v: List[float] = []\n",
    "        ngt_t: List[datetime] = []\n",
    "        ngt_v: List[float] = []\n",
    "\n",
    "        if data[\"day_dt_loc\"]:\n",
    "            d_ord = np.argsort(data[\"day_dt_loc\"])\n",
    "            day_t = [data[\"day_dt_loc\"][i] for i in d_ord]\n",
    "            day_v = [data[\"day_delta\"][i]  for i in d_ord]\n",
    "\n",
    "        if data[\"ngt_dt_loc\"]:\n",
    "            n_ord = np.argsort(data[\"ngt_dt_loc\"])\n",
    "            ngt_t = [data[\"ngt_dt_loc\"][i] for i in n_ord]\n",
    "            ngt_v = [data[\"ngt_delta\"][i]  for i in n_ord]\n",
    "\n",
    "        # ---- Plot: day/night lines + optional Ta_C bars ----\n",
    "        fig, ax = plt.subplots(figsize=(11, 4.8))\n",
    "\n",
    "        ax2 = None\n",
    "        if bar_times is not None and bar_vals is not None and len(bar_times) > 0:\n",
    "            ax2 = ax.twinx()\n",
    "            ax.set_zorder(2); ax.patch.set_alpha(0.0)\n",
    "            ax2.set_zorder(1)\n",
    "            ax2.bar(bar_times, bar_vals, width=0.8, alpha=0.25, color=\"gray\",\n",
    "                    label=\"Ta (°C)\", zorder=0, align=\"center\")\n",
    "            ax2.set_ylabel(\"Air temperature (°C)\")\n",
    "            ax2.grid(False)\n",
    "\n",
    "        # Day series (solid red)\n",
    "        if day_t:\n",
    "            ax.plot(day_t, day_v, marker=\"o\", linewidth=1.6, color=\"red\", label=\"Day\")\n",
    "            for t, v in zip(day_t, day_v):\n",
    "                ax.annotate(t.strftime(\"%H:%M\"), (t, v), textcoords=\"offset points\",\n",
    "                            xytext=(0, 6), ha=\"center\", fontsize=8, color=\"black\")\n",
    "\n",
    "        # Night series (blue dashed)\n",
    "        if ngt_t:\n",
    "            ax.plot(ngt_t, ngt_v, marker=\"s\", linestyle=\"--\", linewidth=1.6, color=\"blue\", label=\"Night\")\n",
    "            for t, v in zip(ngt_t, ngt_v):\n",
    "                ax.annotate(t.strftime(\"%H:%M\"), (t, v), textcoords=\"offset points\",\n",
    "                            xytext=(0, -10), ha=\"center\", fontsize=8, color=\"black\")\n",
    "\n",
    "        # Axes/labels/legend\n",
    "        ax.set_ylim(0.0, 1.0)  # normalized metric\n",
    "        ax.set_xlabel(f\"Local time ({LOCAL_TZNAME}) — day if SZA < {SZA_DAY_THRESHOLD_DEG}°\")\n",
    "        ax.set_ylabel(r\"Normalized Radiance (−)\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\", tz=LOCAL_TZ))\n",
    "        fig.autofmt_xdate()\n",
    "\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        if ax2 is not None:\n",
    "            handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "            ax.legend(handles1 + handles2, labels1 + labels2, loc=\"upper left\")\n",
    "        else:\n",
    "            ax.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.title(f\"Polygon id={pid} — MODIS {BAND_LABEL} Normalized Δ(max−P25)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================== main =================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad195ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== false_positive.py =================================\n",
    "\"\"\"\n",
    "OroraTech False Positives vs Reference (VIIRS or MODIS)\n",
    "----------------------------------------------------------------------------\n",
    "Behavior\n",
    "  • Builds a false-positive (FP) table using a 7×7 kernel tested against reference\n",
    "    detection rasters.\n",
    "  • Skips rasters with no/partial coverage (requires ALL OroraTech points covered).\n",
    "  • Sorts by local time; deduplicates by path/name/time; exports a public subset to Excel.\n",
    "  • Plots, per row:\n",
    "      - Esri World Imagery basemap\n",
    "      - Enlarged yellow rectangles at detected pixels (>= threshold), black edges\n",
    "      - Points: FP=red (white edge), TP=green (white edge)\n",
    "      - Scalebar (meters)\n",
    "      - Zoom framing via ZOOM_OUT_FACTOR + MIN_HALF_SPAN_M\n",
    "\n",
    "Dependencies\n",
    "  Install numpy pandas rasterio fiona shapely pyproj matplotlib tzdata\n",
    "          contextily xyzservices matplotlib-scalebar\n",
    "\n",
    "\"\"\"\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Imports ======================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.crs import CRS as RCRS\n",
    "from rasterio.transform import xy as rio_xy\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, Point\n",
    "from pyproj import Transformer, CRS as PJCRS\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import contextily as ctx\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "\n",
    "# ========================= User Configuration ================================\n",
    "\n",
    "# --- Satellite & temporal window (inclusive dates as 'YYYY-MM-DD') ---\n",
    "SATELLITE   = \"MODIS\" # \"VIIRS\" or \"MODIS\"\n",
    "START_DATE  = \"2025-01-10\"\n",
    "END_DATE    = \"2025-01-11\"\n",
    "\n",
    "# --- Paths (toggle the pair you need) ---\n",
    "REF_BASEDIR   = Path(r\"path\\to\\satellite\\reference\\directory\")\n",
    "ORORA_GEOJSON = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_model_detections.geojson\")\n",
    "out_path      = Path(r\"save\\to\\ororatech_detection_results.xlsx\")\n",
    "\n",
    "# --- Detection & geometry parameters ---\n",
    "KERNEL_SIZE       = 7          # size of square kernel (K×K) used for reference detection check\n",
    "DETECT_THRESHOLD  = 0.5        # pixel >= threshold ⇒ detected\n",
    "INPUT_POINTS_CRS  = PJCRS.from_epsg(4326)\n",
    "\n",
    "# --- Local timezone for reporting/sorting ---\n",
    "# LOCAL_TZ = ZoneInfo(\"America/Chicago\")   # Texas\n",
    "LOCAL_TZ = ZoneInfo(\"America/Los_Angeles\") # California\n",
    "\n",
    "# --- Display & drawing parameters (EPSG:3857) ---\n",
    "PIXEL_RECT_SCALE  = 15.0       # rectangle scale relative to pixel size (in meters via 3857)\n",
    "MAX_RECT_PATCHES  = 15000      # thin rectangles if detections exceed this cap\n",
    "ZOOM_PAD_M        = 5000.0     # padding meters on each side\n",
    "ZOOM_OUT_FACTOR   = 1.4        # inflate half-width/height by this factor\n",
    "MIN_HALF_SPAN_M   = 8000.0     # min half-span (meters) to avoid over-zooming\n",
    "\n",
    "\n",
    "# =============================== Utilities ===================================\n",
    "\n",
    "def _date_iter(d0: date, d1: date):\n",
    "    \"\"\"Inclusive date iterator from d0 to d1 (step = 1 day).\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def _suffix_for_sat(sat: str) -> str:\n",
    "    \"\"\"\n",
    "    Expected suffix for reference detection rasters by satellite.\n",
    "    VIIRS → *_detectmask.tif, MODIS → *_detect.tif\n",
    "    \"\"\"\n",
    "    s = sat.upper()\n",
    "    if s == \"VIIRS\":\n",
    "        return \"_detectmask.tif\"\n",
    "    if s == \"MODIS\":\n",
    "        return \"_detect.tif\"\n",
    "    raise ValueError(\"SATELLITE must be 'VIIRS' or 'MODIS'\")\n",
    "\n",
    "\n",
    "def _gather_reference_files_for_date(day: date, sat: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Find all detection rasters for a given date under .../<YYYY-MM-DD>/af/.\n",
    "    Only files with the satellite-specific suffix are returned.\n",
    "    \"\"\"\n",
    "    daydir = REF_BASEDIR / day.strftime(\"%Y-%m-%d\") / \"af\"\n",
    "    if not daydir.exists():\n",
    "        return []\n",
    "    suf = _suffix_for_sat(sat)\n",
    "    found = set()\n",
    "    for p in daydir.rglob(\"*.tif\"):\n",
    "        if p.name.lower().endswith(suf):\n",
    "            try:\n",
    "                found.add(p.resolve())\n",
    "            except Exception:\n",
    "                found.add(p)\n",
    "    return list(found)\n",
    "\n",
    "\n",
    "def _parse_dt_utc_from_name(name: str) -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    Parse UTC from filenames containing '.Ayyyyddd.HHMM.' (e.g., MODIS/VIIRS convention).\n",
    "    Returns an aware UTC datetime or None if not found.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\.A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})\\.\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "def _count_reference_detections(raster_path: Path) -> int:\n",
    "    \"\"\"Count pixels meeting the detection threshold (>= DETECT_THRESHOLD).\"\"\"\n",
    "    with rasterio.open(raster_path) as ds:\n",
    "        band = ds.read(1, masked=True).filled(np.nan)\n",
    "        return int(np.nansum(band >= DETECT_THRESHOLD))\n",
    "\n",
    "\n",
    "def _load_orora_points(geojson_path: Path) -> List[Point]:\n",
    "    \"\"\"\n",
    "    Load OroraTech detections as shapely Points from a GeoJSON.\n",
    "    Only Point geometries are considered (multipoints ignored).\n",
    "    \"\"\"\n",
    "    if not geojson_path.exists():\n",
    "        raise FileNotFoundError(f\"OroraTech GeoJSON not found: {geojson_path}\")\n",
    "    pts: List[Point] = []\n",
    "    with fiona.open(geojson_path, \"r\") as src:\n",
    "        for feat in src:\n",
    "            g = shape(feat[\"geometry\"])\n",
    "            if isinstance(g, Point):\n",
    "                pts.append(g)\n",
    "    if not pts:\n",
    "        print(\"[WARN] No point detections found in the GeoJSON.\")\n",
    "    return pts\n",
    "\n",
    "\n",
    "def _point_fp_mask_and_coverage(\n",
    "    raster_path: Path, pts: List[Point], kernel: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each OroraTech point, test a K×K neighborhood in the reference raster.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fp_mask : bool array\n",
    "        True  → False Positive (no detection in K×K neighborhood).\n",
    "        False → True Positive  (≥ threshold exists in K×K neighborhood).\n",
    "    covered_mask : bool array\n",
    "        True if the point is fully covered by the raster & the read window is valid.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    • Points not covered (covered_mask=False) are not counted toward FP/TP.\n",
    "    • The caller enforces \"all points must be covered\"; otherwise that raster is skipped.\n",
    "    \"\"\"\n",
    "    half = kernel // 2\n",
    "    fp_mask = np.zeros(len(pts), dtype=bool)\n",
    "    covered = np.zeros(len(pts), dtype=bool)\n",
    "\n",
    "    with rasterio.open(raster_path) as ds:\n",
    "        ds_crs = ds.crs or RCRS.from_epsg(4326)\n",
    "        tf: Optional[Transformer] = None\n",
    "        # Build a transformer only if raster CRS differs from input points CRS\n",
    "        try:\n",
    "            if (ds_crs.to_epsg() or 4326) != 4326:\n",
    "                tf = Transformer.from_crs(INPUT_POINTS_CRS, PJCRS.from_wkt(ds_crs.to_wkt()), always_xy=True)\n",
    "        except Exception:\n",
    "            # Fall back to assuming same CRS if EPSG resolution fails\n",
    "            tf = None\n",
    "\n",
    "        for i, pt in enumerate(pts):\n",
    "            x, y = (tf.transform(pt.x, pt.y) if tf else (pt.x, pt.y))\n",
    "            try:\n",
    "                row, col = ds.index(x, y)\n",
    "            except Exception:\n",
    "                covered[i] = False\n",
    "                continue\n",
    "\n",
    "            r0 = max(0, row - half); r1 = min(ds.height, row + half + 1)\n",
    "            c0 = max(0, col - half); c1 = min(ds.width,  col + half + 1)\n",
    "            if (r1 <= r0) or (c1 <= c0):\n",
    "                covered[i] = False\n",
    "                continue\n",
    "\n",
    "            win = Window.from_slices((r0, r1), (c0, c1))\n",
    "            arr = ds.read(1, window=win, boundless=False, masked=True).filled(np.nan)\n",
    "            if not np.isfinite(arr).any():\n",
    "                covered[i] = False\n",
    "                continue\n",
    "\n",
    "            covered[i] = True\n",
    "            det = (np.nanmax(arr) >= DETECT_THRESHOLD)\n",
    "            fp_mask[i] = (not det)\n",
    "\n",
    "    return fp_mask, covered\n",
    "\n",
    "\n",
    "# =============================== Plotting =====================================\n",
    "\n",
    "def _plot_overlay_detect_mask_merc_esri(\n",
    "    raster_path: Path,\n",
    "    pts: List[Point],\n",
    "    fp_mask: np.ndarray,\n",
    "    title: str,\n",
    "    zoom_to_points: bool = True,\n",
    "    pad_m: float = ZOOM_PAD_M,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot detection rectangles and FP/TP points on an Esri World Imagery basemap in EPSG:3857.\n",
    "\n",
    "    Layers (z-order)\n",
    "      • Basemap (contextily, EPSG:3857)\n",
    "      • Enlarged yellow rectangles centered on detected pixels (black edges)\n",
    "      • FP points (red, white edge), TP points (green, white edge)\n",
    "      • Scalebar (meters), legend, title\n",
    "\n",
    "    Framing\n",
    "      • Either fit to all points or to raster bounds, then expand by:\n",
    "          hx, hy = max(halfspan * ZOOM_OUT_FACTOR + pad, MIN_HALF_SPAN_M)\n",
    "    \"\"\"\n",
    "    EPSG3857 = PJCRS.from_epsg(3857)\n",
    "\n",
    "    with rasterio.open(raster_path) as ds:\n",
    "        ds_crs = ds.crs or RCRS.from_epsg(4326)\n",
    "        to_merc = Transformer.from_crs(PJCRS.from_wkt(ds_crs.to_wkt()) if ds_crs else PJCRS.from_epsg(4326),\n",
    "                                       EPSG3857, always_xy=True)\n",
    "        from_wgs84_to_merc = Transformer.from_crs(PJCRS.from_epsg(4326), EPSG3857, always_xy=True)\n",
    "\n",
    "        # --- Detection mask ---\n",
    "        arr = ds.read(1).astype(np.float32)\n",
    "        det = (arr >= DETECT_THRESHOLD) & np.isfinite(arr)\n",
    "\n",
    "        # --- Raster extent → 3857 ---\n",
    "        left, bottom, right, top = ds.bounds\n",
    "        bx0, by0 = to_merc.transform(left,  bottom)\n",
    "        bx1, by1 = to_merc.transform(right, top)\n",
    "        x_min, x_max = min(bx0, bx1), max(bx0, bx1)\n",
    "        y_min, y_max = min(by0, by1), max(by0, by1)\n",
    "\n",
    "        # --- Init figure/axes (no ticks/labels) ---\n",
    "        fig, ax = plt.subplots(figsize=(10, 8.5))\n",
    "        ax.set_xlim(x_min, x_max); ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "        # --- Basemap (Esri World Imagery) ---\n",
    "        ctx.add_basemap(ax, crs=\"EPSG:3857\", source=ctx.providers.Esri.WorldImagery)\n",
    "\n",
    "        # --- Detection rectangles (thin if too many) ---\n",
    "        rr, cc = np.where(det)\n",
    "        n = rr.size\n",
    "        if n > 0:\n",
    "            # Pixel metric size estimated near scene center (in 3857)\n",
    "            px_dx = abs(ds.transform.a); px_dy = abs(ds.transform.e)\n",
    "            cx_src = (left + right) / 2.0; cy_src = (bottom + top) / 2.0\n",
    "            cxm, cym = to_merc.transform(cx_src, cy_src)\n",
    "            dxm, _   = to_merc.transform(cx_src + px_dx, cy_src)\n",
    "            _,   dym = to_merc.transform(cx_src, cy_src + px_dy)\n",
    "            px_w_m = abs(dxm - cxm); px_h_m = abs(dym - cym)\n",
    "            w = px_w_m * float(PIXEL_RECT_SCALE)\n",
    "            h = px_h_m * float(PIXEL_RECT_SCALE)\n",
    "\n",
    "            stride = 1\n",
    "            if n > MAX_RECT_PATCHES:\n",
    "                stride = int(np.ceil(np.sqrt(n / MAX_RECT_PATCHES)))\n",
    "            rr = rr[::stride]; cc = cc[::stride]\n",
    "\n",
    "            xs_src, ys_src = rio_xy(ds.transform, rr, cc, offset=\"center\")\n",
    "            xs_m, ys_m = to_merc.transform(xs_src, ys_src)\n",
    "\n",
    "            patches = [Rectangle((x - w/2, y - h/2), w, h) for x, y in zip(xs_m, ys_m)]\n",
    "            coll = PatchCollection(patches, facecolor=\"yellow\", edgecolor=\"black\", linewidth=1, zorder=2)\n",
    "            ax.add_collection(coll)\n",
    "\n",
    "        # --- Points (WGS84 → 3857) with white borders ---\n",
    "        xs_fp: List[float]; ys_fp: List[float]; xs_tp: List[float]; ys_tp: List[float]\n",
    "        xs_fp, ys_fp, xs_tp, ys_tp, xs_all, ys_all = [], [], [], [], [], []\n",
    "        for is_fp, p in zip(fp_mask, pts):\n",
    "            xm, ym = from_wgs84_to_merc.transform(p.x, p.y)\n",
    "            xs_all.append(xm); ys_all.append(ym)\n",
    "            if is_fp: xs_fp.append(xm); ys_fp.append(ym)\n",
    "            else:     xs_tp.append(xm); ys_tp.append(ym)\n",
    "\n",
    "        if xs_fp:\n",
    "            ax.scatter(xs_fp, ys_fp, s=150, c=\"red\",   edgecolors=\"white\", linewidths=1.1,\n",
    "                       label=\"False Positive\", zorder=5)\n",
    "        if xs_tp:\n",
    "            ax.scatter(xs_tp, ys_tp, s=150, c=\"green\", edgecolors=\"white\", linewidths=1.1,\n",
    "                       label=\"True Positive\",  zorder=6)\n",
    "\n",
    "        # --- Framing / zoom ---\n",
    "        if zoom_to_points and len(xs_all) > 0:\n",
    "            x0, x1 = min(xs_all), max(xs_all)\n",
    "            y0, y1 = min(ys_all), max(ys_all)\n",
    "        else:\n",
    "            x0, x1 = x_min, x_max; y0, y1 = y_min, y_max\n",
    "\n",
    "        xc = 0.5 * (x0 + x1); yc = 0.5 * (y0 + y1)\n",
    "        hx = max(0.5 * (x1 - x0), 1.0); hy = max(0.5 * (y1 - y0), 1.0)\n",
    "        hx = max(hx * ZOOM_OUT_FACTOR + pad_m, MIN_HALF_SPAN_M)\n",
    "        hy = max(hy * ZOOM_OUT_FACTOR + pad_m, MIN_HALF_SPAN_M)\n",
    "        ax.set_xlim(xc - hx, xc + hx); ax.set_ylim(yc - hy, yc + hy)\n",
    "\n",
    "        # --- Title, legend, scalebar ---\n",
    "        ax.set_title(title, fontsize=26, fontweight=\"bold\")\n",
    "        ax.legend(loc=\"lower right\", frameon=True, fontsize=18, title_fontsize=15)\n",
    "        try:\n",
    "            scalebar = ScaleBar(dx=1, units='m', location='upper right', box_alpha=0.6, scale_loc='top')\n",
    "            ax.add_artist(scalebar)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not add scalebar: {e}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============================== Core Pipeline ================================\n",
    "\n",
    "def _build_table_and_masks(\n",
    "    SATELLITE: str,\n",
    "    START_DATE: str,\n",
    "    END_DATE: str,\n",
    ") -> tuple[pd.DataFrame, List[Point], Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Build the false-positive summary table across the requested date window and\n",
    "    compute per-raster FP masks (for plotting), enforcing full coverage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Full table including hidden plotting columns (paths, datetimes).\n",
    "    orora_pts : list[Point]\n",
    "        OroraTech points as shapely Points (WGS84).\n",
    "    FP_MASK_BY_PATH : dict[str, np.ndarray]\n",
    "        Map from resolved raster path → pointwise FP mask (bool array).\n",
    "    \"\"\"\n",
    "    D0 = datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    D1 = datetime.strptime(END_DATE,   \"%Y-%m-%d\").date()\n",
    "    if D1 < D0:\n",
    "        raise SystemExit(\"END_DATE must be >= START_DATE\")\n",
    "\n",
    "    orora_pts = _load_orora_points(ORORA_GEOJSON)\n",
    "    N_POINTS = len(orora_pts)\n",
    "    print(f\"[INFO] Loaded {N_POINTS} OroraTech detections from {ORORA_GEOJSON.name}\")\n",
    "    print(f\"[INFO] Local timezone for reporting: {getattr(LOCAL_TZ, 'key', None) or LOCAL_TZ.tzname(datetime.now())}\")\n",
    "\n",
    "    rows: List[dict] = []\n",
    "    FP_MASK_BY_PATH: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # ---- Enumerate reference rasters by day ----\n",
    "    for day in _date_iter(D0, D1):\n",
    "        ref_paths = _gather_reference_files_for_date(day, SATELLITE)\n",
    "        if not ref_paths:\n",
    "            continue\n",
    "\n",
    "        # Sort: by timestamp (if parseable), then name\n",
    "        ref_paths = sorted(\n",
    "            ref_paths,\n",
    "            key=lambda p: (\n",
    "                (dt := _parse_dt_utc_from_name(p.name)) is None,\n",
    "                (dt.strftime(\"%H%M\") if dt else \"\"),\n",
    "                p.name,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        for rp in ref_paths:\n",
    "            dt_utc = _parse_dt_utc_from_name(rp.name) or datetime.combine(day, datetime.min.time(), tzinfo=timezone.utc)\n",
    "            dt_loc = dt_utc.astimezone(LOCAL_TZ)\n",
    "\n",
    "            # Skip unreadable rasters\n",
    "            try:\n",
    "                ref_det = _count_reference_detections(rp)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # Build FP mask & coverage; enforce FULL coverage across all points\n",
    "            fp_mask, covered = _point_fp_mask_and_coverage(rp, orora_pts, KERNEL_SIZE)\n",
    "            if (covered.size == 0) or (covered.sum() != N_POINTS):\n",
    "                continue\n",
    "\n",
    "            fp = int(fp_mask.sum())\n",
    "            if N_POINTS == 0:\n",
    "                continue\n",
    "            fp_rate = fp / N_POINTS if N_POINTS > 0 else np.nan\n",
    "\n",
    "            key_path = str(rp.resolve())\n",
    "            FP_MASK_BY_PATH[key_path] = fp_mask\n",
    "\n",
    "            try:\n",
    "                raster_rel = str(rp.resolve().relative_to(REF_BASEDIR.resolve()))\n",
    "            except Exception:\n",
    "                raster_rel = rp.name\n",
    "\n",
    "            rows.append({\n",
    "                \"satellite\": SATELLITE,\n",
    "                \"local_date\": dt_loc.date(),\n",
    "                \"local_time\": dt_loc.strftime(\"%H:%M\"),\n",
    "                \"tz\": dt_loc.tzname(),\n",
    "                \"false_positives\": fp,\n",
    "                \"fp_rate\": round(fp_rate, 3) if np.isfinite(fp_rate) else np.nan,\n",
    "                \"ref_detections\": int(ref_det),\n",
    "                \"raster_name\": rp.name,\n",
    "                \"_raster_path\": key_path,     # for plotting\n",
    "                \"_raster_relpath\": raster_rel,\n",
    "                \"_local_dt\": dt_loc,          # sort key\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No rasters with full coverage found in the requested date range.\")\n",
    "        return df, orora_pts, FP_MASK_BY_PATH\n",
    "\n",
    "    # ---- Deduplicate & sort ----\n",
    "    df = df.drop_duplicates(subset=[\"_raster_relpath\"], keep=\"first\").copy()\n",
    "    df = df.sort_values([\"_local_dt\", \"ref_detections\"], ascending=[True, False])\n",
    "    df = df.drop_duplicates(subset=[\"raster_name\", \"local_date\", \"local_time\"], keep=\"first\")\n",
    "    df = df.sort_values(\"_local_dt\").reset_index(drop=True)\n",
    "    return df, orora_pts, FP_MASK_BY_PATH\n",
    "\n",
    "\n",
    "# ============================ Execute & Plot ================================\n",
    "\n",
    "# Build table + masks\n",
    "df, orora_pts, FP_MASK_BY_PATH = _build_table_and_masks(SATELLITE, START_DATE, END_DATE)\n",
    "\n",
    "# Export public subset & display; then iterate plots row-by-row\n",
    "if not df.empty:\n",
    "    df_public = df[[\n",
    "        \"satellite\", \"local_date\", \"local_time\", \"tz\",\n",
    "        \"false_positives\", \"fp_rate\", \"ref_detections\", \"raster_name\"\n",
    "    ]].copy()\n",
    "\n",
    "    # Export to Excel\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_public.to_excel(out_path, index=False)\n",
    "    print(f\"[INFO] Results saved to: {out_path.resolve()}\")\n",
    "\n",
    "    # Show the public view inline\n",
    "    display(df_public)\n",
    "\n",
    "    # Per-row overlays (Esri basemap; EPSG:3857; framed zoom)\n",
    "    for _, row in df.iterrows():\n",
    "        rpath = Path(row[\"_raster_path\"])\n",
    "        fp_mask = FP_MASK_BY_PATH.get(str(rpath.resolve()))\n",
    "        if (not rpath.exists()) or (fp_mask is None):\n",
    "            print(f\"[SKIP] Missing raster or FP mask for {row['raster_name']}\")\n",
    "            continue\n",
    "\n",
    "        title = f\"{row['satellite']} Detection — {row['local_date']} {row['local_time']} {row['tz']}\"\n",
    "        _plot_overlay_detect_mask_merc_esri(rpath, orora_pts, fp_mask, title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
