{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4efea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== modis_download.py ==============================\n",
    "\"\"\"\n",
    "MODIS Downloader (Terra/Aqua) via Earthdata/CMR (earthaccess)\n",
    "===============================================================================\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "• Authenticates with NASA Earthdata:\n",
    "    - Prefers token: EARTHDATA_TOKEN or EARTHACCESS_TOKEN\n",
    "    - Else EARTHDATA_USERNAME / EARTHDATA_PASSWORD (env)\n",
    "    - Else interactive prompt (safe for local runs)\n",
    "• Reads an AOI shapefile (any CRS), computes its WGS84 bounding box.\n",
    "• For each calendar day in START_DATE..END_DATE:\n",
    "    - Creates BASEDIR/YYYY-MM-DD/raw_modis/\n",
    "    - (Optional) Downloads Thermal L1B (MOD021KM/MYD021KM) and GEO (MOD03/MYD03)\n",
    "      and writes a pairing manifest: manifest_pairs_modis.csv\n",
    "    - (Optional) Downloads Active Fire L2 (MOD14/MYD14)\n",
    "      and writes AF manifest: manifest_active_fire_modis.csv\n",
    "\n",
    "Outputs (per-day)\n",
    "-----------------\n",
    "• Thermal manifest: BASEDIR/YYYY-MM-DD/manifest_pairs_modis.csv\n",
    "    columns: l1b_path, geo_path, platform, timestamp_key\n",
    "• AF manifest:      BASEDIR/YYYY-MM-DD/manifest_active_fire_modis.csv\n",
    "    columns: product, file_path, timestamp_key, has_internal_geo, paired_geo_path\n",
    "\n",
    "Notes / Assumptions\n",
    "-------------------\n",
    "• Pairing uses the MODIS/VIIRS-like timestamp key found in file names: \".Ayyyyddd.HHMM.\"\n",
    "• For AF products, an internal lat/lon check is attempted (HDF4 via optional pyhdf).\n",
    "  If missing, we provide a GEO path when available (from downloaded Thermal pairs).\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "pip install earthaccess fiona shapely pyproj pyhdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Standard libs =================================\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "# ============================== Third-party ===================================\n",
    "import earthaccess as ea\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "# ================================ SWITCHES ====================================\n",
    "# Toggle downloads (preserved defaults)\n",
    "DOWNLOAD_THERMAL = True   # L1B+GEO (radiance/BT workflows)\n",
    "DOWNLOAD_AF      = True    # Active Fire swath L2 (MOD14/MYD14)\n",
    "\n",
    "# Platforms to fetch (preserved defaults)\n",
    "GET_TERRA = True           # Terra: MOD*\n",
    "GET_AQUA  = True           # Aqua : MYD*\n",
    "\n",
    "# ================================= CONFIG =====================================\n",
    "BASEDIR    = Path(r\"path\\to\\base\\directory\")\n",
    "AOI_SHP    = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\")\n",
    "START_DATE = date(2025, 1, 1)\n",
    "END_DATE   = date(2025, 1, 12)\n",
    "\n",
    "# Network safety: avoid long hangs\n",
    "socket.setdefaulttimeout(20)\n",
    "\n",
    "# =============================== Auth & AOI ====================================\n",
    "def ensure_login() -> str:\n",
    "    \"\"\"\n",
    "    Authenticate with Earthdata/CMR via earthaccess.\n",
    "\n",
    "    Order of preference\n",
    "    -------------------\n",
    "    1) Token in env: EARTHDATA_TOKEN or EARTHACCESS_TOKEN (length > 50)\n",
    "    2) Username/password in env: EARTHDATA_USERNAME / EARTHDATA_PASSWORD\n",
    "    3) Interactive prompt (username + password)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str: \"token\" | \"userpass\" | \"prompt\"\n",
    "    \"\"\"\n",
    "    tok = os.getenv(\"EARTHDATA_TOKEN\") or os.getenv(\"EARTHACCESS_TOKEN\")\n",
    "    if tok and len(tok) > 50:\n",
    "        os.environ[\"EARTHDATA_TOKEN\"] = tok\n",
    "        ea.login(strategy=\"environment\")\n",
    "        return \"token\"\n",
    "\n",
    "    user = os.getenv(\"EARTHDATA_USERNAME\")\n",
    "    pwd  = os.getenv(\"EARTHDATA_PASSWORD\")\n",
    "    if user and pwd:\n",
    "        ea.login(strategy=\"environment\")\n",
    "        return \"userpass\"\n",
    "\n",
    "    # Fallback prompt (safe for local, not for unattended environments)\n",
    "    import getpass\n",
    "    user = input(\"Earthdata username: \").strip()\n",
    "    pwd  = getpass.getpass(\"Earthdata password: \")\n",
    "    os.environ[\"EARTHDATA_USERNAME\"] = user\n",
    "    os.environ[\"EARTHDATA_PASSWORD\"] = pwd\n",
    "    ea.login(strategy=\"environment\")\n",
    "    return \"prompt\"\n",
    "\n",
    "\n",
    "def read_aoi_bbox_wgs84(shp_path: Path) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Read AOI shapefile (any CRS) and return its (minx, miny, maxx, maxy) in WGS84.\n",
    "\n",
    "    Clamps to valid lon/lat ranges.\n",
    "    \"\"\"\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(f[\"geometry\"]) for f in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != dst_crs_obj:\n",
    "        transformer = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: transformer.transform(x, y), aoi)\n",
    "    minx, miny, maxx, maxy = aoi.bounds\n",
    "    return (max(minx, -180.0), max(miny, -90.0), min(maxx, 180.0), min(maxy, 90.0))\n",
    "\n",
    "\n",
    "def day_iter(d0: date, d1: date) -> Iterable[date]:\n",
    "    \"\"\"Inclusive date iterator: d0, d0+1, …, d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "# ============================== Pairing utilities ==============================\n",
    "def timestamp_key_from_name(name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract MODIS-style timestamp key: '.Ayyyyddd.HHMM.' from a file name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str | None\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def pair_by_timestamp(l1b_paths: list[str], geo_paths: list[str]) -> list[tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Pair L1B ↔ GEO by the timestamp key; only exact matches are kept.\n",
    "    \"\"\"\n",
    "    geo_map = {timestamp_key_from_name(Path(g).name): Path(g) for g in geo_paths}\n",
    "    pairs: list[tuple[Path, Path]] = []\n",
    "    for l1b in l1b_paths:\n",
    "        key = timestamp_key_from_name(Path(l1b).name)\n",
    "        if key in geo_map and geo_map[key] is not None:\n",
    "            pairs.append((Path(l1b), geo_map[key]))\n",
    "    return pairs\n",
    "\n",
    "# ================================ Thermal =====================================\n",
    "def search_download_thermal(day: date,\n",
    "                            bbox: tuple[float, float, float, float],\n",
    "                            rawdir: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search and download MODIS L1B & GEO for Terra/Aqua, pair by timestamp,\n",
    "    and return manifest rows.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "    prods: list[tuple[str, str, str]] = []\n",
    "    if GET_TERRA:\n",
    "        prods.append((\"MOD021KM\", \"MOD03\", \"TERRA\"))\n",
    "    if GET_AQUA:\n",
    "        prods.append((\"MYD021KM\", \"MYD03\", \"AQUA\"))\n",
    "\n",
    "    for L1B, GEO, plat in prods:\n",
    "        print(f\"[INFO] {day} — searching {plat}: {L1B}/{GEO}\")\n",
    "        items_l1b = ea.search_data(short_name=L1B, temporal=(day, day), bounding_box=bbox)\n",
    "        items_geo = ea.search_data(short_name=GEO, temporal=(day, day), bounding_box=bbox)\n",
    "        print(f\"       found {len(items_l1b)} L1B, {len(items_geo)} GEO\")\n",
    "\n",
    "        l1b_paths = ea.download(items_l1b, rawdir.as_posix()) if items_l1b else []\n",
    "        geo_paths = ea.download(items_geo, rawdir.as_posix()) if items_geo else []\n",
    "        print(f\"       downloaded {len(l1b_paths)} L1B, {len(geo_paths)} GEO\")\n",
    "\n",
    "        pairs = pair_by_timestamp(l1b_paths, geo_paths)\n",
    "        print(f\"       paired {len(pairs)} {plat} granules\")\n",
    "\n",
    "        for l1b_p, geo_p in pairs:\n",
    "            rows.append({\n",
    "                \"l1b_path\": str(l1b_p),\n",
    "                \"geo_path\": str(geo_p),\n",
    "                \"platform\": plat,\n",
    "                \"timestamp_key\": timestamp_key_from_name(Path(l1b_p).name) or \"\"\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# ============================= Active Fire (AF) ================================\n",
    "def _af_has_internal_geo(hdf_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Best-effort check for internal lat/lon in MOD14/MYD14 HDF4.\n",
    "    Returns True if datasets 'Latitude' and 'Longitude' exist.\n",
    "    If pyhdf is not available, returns False (no failure).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyhdf.SD import SD, SDC  # optional dependency\n",
    "    except Exception:\n",
    "        return False\n",
    "    try:\n",
    "        sd = SD(str(hdf_path), SDC.READ)\n",
    "        names = [sd.select(idx).info()[0] for idx in range(len(sd.datasets()))]\n",
    "        sd.end()\n",
    "        names_l = [n.lower() for n in names]\n",
    "        return (\"latitude\" in names_l) and (\"longitude\" in names_l)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def search_download_active_fire(day: date,\n",
    "                                bbox: tuple[float, float, float, float],\n",
    "                                rawdir: Path,\n",
    "                                geo_rows_for_pairing: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search and download MODIS Active Fire L2 (MOD14/MYD14).\n",
    "    Attempt to pair with GEO if needed (using Thermal rows), but AF may carry internal geo.\n",
    "    Returns AF manifest rows.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "    prods: list[tuple[str, str]] = []\n",
    "    if GET_TERRA:\n",
    "        prods.append((\"MOD14\", \"TERRA\"))\n",
    "    if GET_AQUA:\n",
    "        prods.append((\"MYD14\", \"AQUA\"))\n",
    "\n",
    "    # Quick lookup for potential GEO pairing (from thermal rows)\n",
    "    geo_map = {r.get(\"timestamp_key\"): r.get(\"geo_path\") for r in geo_rows_for_pairing}\n",
    "\n",
    "    for AF, plat in prods:\n",
    "        print(f\"[INFO] {day} — searching {plat}: {AF} (Active Fire)\")\n",
    "        items_af = ea.search_data(short_name=AF, temporal=(day, day), bounding_box=bbox)\n",
    "        print(f\"       found {len(items_af)} AF items\")\n",
    "\n",
    "        af_paths = ea.download(items_af, rawdir.as_posix()) if items_af else []\n",
    "        print(f\"       downloaded {len(af_paths)} AF granules\")\n",
    "\n",
    "        for p in af_paths:\n",
    "            p = Path(p)\n",
    "            key = timestamp_key_from_name(p.name) or \"\"\n",
    "            has_geo = 1 if _af_has_internal_geo(p) else 0\n",
    "            paired_geo = geo_map.get(key, \"\") if key in geo_map else \"\"\n",
    "            rows.append({\n",
    "                \"product\": AF,\n",
    "                \"file_path\": str(p),\n",
    "                \"timestamp_key\": key,\n",
    "                \"has_internal_geo\": has_geo,\n",
    "                \"paired_geo_path\": paired_geo\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "# ================================== Main ======================================\n",
    "def main() -> None:\n",
    "    \"\"\"Entry point: authenticate, compute AOI bbox, iterate days, write manifests.\"\"\"\n",
    "    if not DOWNLOAD_THERMAL and not DOWNLOAD_AF:\n",
    "        raise SystemExit(\"Both DOWNLOAD_THERMAL and DOWNLOAD_AF are False. Nothing to do.\")\n",
    "\n",
    "    auth_mode = ensure_login()\n",
    "    print(f\"[INFO] Auth mode: {auth_mode}\")\n",
    "\n",
    "    bbox = read_aoi_bbox_wgs84(AOI_SHP)\n",
    "    print(f\"[INFO] AOI bbox (WGS84): {bbox}\")\n",
    "\n",
    "    for day in day_iter(START_DATE, END_DATE):\n",
    "        daydir = BASEDIR / day.strftime(\"%Y-%m-%d\")\n",
    "        rawdir = daydir / \"raw_modis\"\n",
    "        rawdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        thermal_rows: list[dict] = []\n",
    "        af_rows: list[dict] = []\n",
    "\n",
    "        # ---- Thermal (L1B + GEO) ----\n",
    "        if DOWNLOAD_THERMAL:\n",
    "            thermal_rows = search_download_thermal(day, bbox, rawdir)\n",
    "            if thermal_rows:\n",
    "                manifest_path = daydir / \"manifest_pairs_modis.csv\"\n",
    "                with manifest_path.open(\"w\", newline=\"\") as f:\n",
    "                    w = csv.DictWriter(f, fieldnames=[\"l1b_path\", \"geo_path\", \"platform\", \"timestamp_key\"])\n",
    "                    w.writeheader()\n",
    "                    for r in thermal_rows:\n",
    "                        w.writerow(r)\n",
    "                print(f\"[OK] Thermal manifest written: {manifest_path}\")\n",
    "            else:\n",
    "                print(f\"[WARN] {day}: no L1B/GEO pairs found.\")\n",
    "\n",
    "        # ---- Active Fire (MOD14/MYD14) ----\n",
    "        if DOWNLOAD_AF:\n",
    "            af_rows = search_download_active_fire(day, bbox, rawdir, thermal_rows)\n",
    "            if af_rows:\n",
    "                manifest_af = daydir / \"manifest_active_fire_modis.csv\"\n",
    "                with manifest_af.open(\"w\", newline=\"\") as f:\n",
    "                    w = csv.DictWriter(\n",
    "                        f,\n",
    "                        fieldnames=[\"product\", \"file_path\", \"timestamp_key\", \"has_internal_geo\", \"paired_geo_path\"]\n",
    "                    )\n",
    "                    w.writeheader()\n",
    "                    for r in af_rows:\n",
    "                        w.writerow(r)\n",
    "                print(f\"[OK] AF manifest written: {manifest_af}\")\n",
    "            else:\n",
    "                print(f\"[WARN] {day}: no AF granules found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea03427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================= modis_process_rad_bt.py =============================\n",
    "\"\"\"\n",
    "MODIS L1B → Radiance & Brightness Temperature\n",
    "===============================================================================\n",
    "\n",
    "Bands: 21 (~3.96 µm) and 31 (~11.0 µm)\n",
    "\n",
    "Pipeline (per day)\n",
    "------------------\n",
    "Inputs (created by modis_download.py):\n",
    "  BASEDIR/YYYY-MM-DD/raw_modis/*.hdf\n",
    "  BASEDIR/YYYY-MM-DD/manifest_pairs_modis.csv\n",
    "\n",
    "Processing:\n",
    "  1) Read L1B (MOD021KM/MYD021KM) EV_1KM_Emissive for Bands 21 & 31\n",
    "  2) Scale to radiance (W·m^-2·sr^-1·µm^-1)\n",
    "  3) Convert to BT (K)\n",
    "  4) Read geolocation from MOD03/MYD03\n",
    "  5) Resample swath to a fixed WGS84 grid (EPSG:4326), intersected with AOI\n",
    "  6) Apply cloud mask: Band 31 BT < CLOUD_BT_K = 265 °K → invalid\n",
    "  7) AOI clip\n",
    "  8) Write GeoTIFFs: *_B21_Rad.tif, *_B21_BT_K.tif, *_B31_Rad.tif, *_B31_BT_K.tif\n",
    "  9) Append global CSV log: BASEDIR/processing_log_modis.csv (UTC/local times & stats)\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "• Install pyhdf gdal rasterio pyresample fiona shapely pyproj tzdata\n",
    "\n",
    "Notes\n",
    "-----\n",
    "• Cloud mask is deliberately simple (cold-cloud threshold on Band 31 BT).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ------------------------------ Standard library ------------------------------\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "# --------------------------------- Numerics -----------------------------------\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------- Raster IO ----------------------------------\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ----------------------------- Vector / geometry ------------------------------\n",
    "import fiona\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "# ----------------------------- Resampling (swath) -----------------------------\n",
    "from pyresample import geometry, kd_tree\n",
    "\n",
    "# --------------------------------- Timezone -----------------------------------\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ---------------------------------- HDF4 --------------------------------------\n",
    "from pyhdf.SD import SD, SDC  # MODIS L1B/geo reader\n",
    "\n",
    "# ==============================================================================\n",
    "#                                 CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "BASEDIR    = Path(r\"path\\to\\base\\directory\")\n",
    "AOI_SHP    = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\")\n",
    "START_DATE = date(2025, 1, 8)\n",
    "END_DATE   = date(2025, 1, 12)\n",
    "\n",
    "# Local time zone for logging\n",
    "LOCAL_TZNAME = \"America/Los_Angeles\"  # California: \"America/Los_Angeles\", Texas: \"America/Chicago\"\n",
    "LOCAL_TZ     = ZoneInfo(LOCAL_TZNAME)\n",
    "\n",
    "# Output grid & cloud mask parameters\n",
    "GRID_RES_DEG = 0.008333  # ≈ 1 km\n",
    "CLOUD_BT_K   = 265.0     # cold-cloud threshold on Band 31 BT\n",
    "\n",
    "# ==============================================================================\n",
    "#                         PHYSICS & BAND CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# MCST effective central wavenumbers (cm^-1) — as used by Satpy, etc.\n",
    "MCST_CWN_CM1: dict[str, float] = {\n",
    "    \"20\": 2641.775, \"21\": 2505.277, \"22\": 2518.028, \"23\": 2465.428,\n",
    "    \"24\": 2235.815, \"25\": 2200.346,\n",
    "    \"27\": 1477.967, \"28\": 1362.737, \"29\": 1173.190, \"30\": 1027.715,\n",
    "    \"31\": 908.0884, \"32\": 831.5399, \"33\": 748.3394, \"34\": 730.8963,\n",
    "    \"35\": 718.8681, \"36\": 704.5367,\n",
    "}\n",
    "\n",
    "# Physical constants\n",
    "_H = 6.62607015e-34   # J·s\n",
    "_C = 2.99792458e8     # m·s^-1\n",
    "_K = 1.380649e-23     # J·K^-1\n",
    "\n",
    "def k1k2_from_cwn_cm1(cwn_cm1: float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute (K1, K2) for spectral radiance in W·m^-2·sr^-1·µm^-1 given ν_eff (cm^-1).\n",
    "    \"\"\"\n",
    "    lam_m = 1.0 / (cwn_cm1 * 100.0)             # cm^-1 → m\n",
    "    K1 = (2.0 * _H * _C**2) / (lam_m**5) * 1e-6  # per µm\n",
    "    K2 = (_H * _C) / (_K * lam_m)\n",
    "    return K1, K2\n",
    "\n",
    "def bt_from_radiance_with_bandnum(L_um: np.ndarray | None, band_num: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Brightness temperature (K) from spectral radiance (W·m^-2·sr^-1·µm^-1),\n",
    "    using MCST ν_eff (band_num: \"21\", \"31\").\n",
    "    \"\"\"\n",
    "    if L_um is None:\n",
    "        return None\n",
    "    cwn = MCST_CWN_CM1.get(str(band_num))\n",
    "    if cwn is None:\n",
    "        raise ValueError(f\"No MCST ν_eff for band {band_num}.\")\n",
    "    K1, K2 = k1k2_from_cwn_cm1(cwn)\n",
    "    L = np.asarray(L_um, dtype=np.float64)\n",
    "    L = np.clip(L, 1e-9, np.inf)                 # guard against log singularities\n",
    "    return (K2 / np.log1p(K1 / L)).astype(np.float32)\n",
    "\n",
    "# ==============================================================================\n",
    "#                           IO HELPERS & TIME UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def day_iter(d0: date, d1: date) -> Iterable[date]:\n",
    "    \"\"\"Inclusive date iterator: d0, d0+1, …, d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "def read_aoi_wgs84(shp_path: Path):\n",
    "    \"\"\"\n",
    "    Read AOI polygons (any CRS) and return a unified geometry in EPSG:4326.\n",
    "    \"\"\"\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(f[\"geometry\"]) for f in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != dst_crs_obj:\n",
    "        transformer = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: transformer.transform(x, y), aoi)\n",
    "    return aoi\n",
    "\n",
    "def timestamp_key_from_name(name: str) -> Optional[str]:\n",
    "    \"\"\"Extract '.Ayyyyddd.HHMM.' timestamp key from a MODIS filename.\"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def dt_from_timestamp_key(key: str) -> Optional[datetime]:\n",
    "    \"\"\"Convert 'Ayyyyddd.HHMM' to aware UTC datetime.\"\"\"\n",
    "    m = re.match(r\"A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})$\", key)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "# ==============================================================================\n",
    "#                             MODIS HDF4 READERS\n",
    "# ==============================================================================\n",
    "\n",
    "# Fallback band order for EV_1KM_Emissive when metadata is missing\n",
    "_BAND_ORDER_DEFAULT = [20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
    "\n",
    "def _parse_band_names(attr_val) -> list[int] | None:\n",
    "    \"\"\"\n",
    "    Parse 'band_names' attribute into a list of integer band IDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = attr_val.decode(\"utf-8\") if isinstance(attr_val, bytes) else str(attr_val)\n",
    "        parts = [p.strip() for p in s.replace(\"Band_\", \"\").split(\",\")]\n",
    "        nums: list[int] = []\n",
    "        for p in parts:\n",
    "            m = re.match(r\"(\\d+)\", p)\n",
    "            if m:\n",
    "                nums.append(int(m.group(1)))\n",
    "        return nums or None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def read_modis_emissive(mod021_path: Path, want_bands: tuple[int, int] = (21, 31)) -> dict[int, Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Read EV_1KM_Emissive and scale to radiance for requested bands.\n",
    "    Returns a dict {band_num: radiance(float32) | None}.\n",
    "    Units: W·m^-2·sr^-1·µm^-1\n",
    "    \"\"\"\n",
    "    sd = SD(str(mod021_path), SDC.READ)\n",
    "    sds = sd.select(\"EV_1KM_Emissive\")\n",
    "    arr = sds.get()  # shape: (nbands, lines, samples), int16\n",
    "    attrs = sds.attributes()\n",
    "\n",
    "    band_names = _parse_band_names(attrs.get(\"band_names\", attrs.get(\"Band_names\", \"\"))) or _BAND_ORDER_DEFAULT\n",
    "    band_to_idx = {b: i for i, b in enumerate(band_names)}\n",
    "\n",
    "    scales  = np.array(attrs.get(\"radiance_scales\"),  dtype=np.float64)\n",
    "    offsets = np.array(attrs.get(\"radiance_offsets\"), dtype=np.float64)\n",
    "    badval  = attrs.get(\"bad_data_value\", None)\n",
    "    fillval = attrs.get(\"_FillValue\", None)\n",
    "\n",
    "    out: dict[int, Optional[np.ndarray]] = {}\n",
    "    for b in want_bands:\n",
    "        if b not in band_to_idx:\n",
    "            out[b] = None\n",
    "            continue\n",
    "        i = band_to_idx[b]\n",
    "        dn = arr[i, :, :].astype(np.float64)\n",
    "\n",
    "        # Replace sentinel/fill with NaN before scaling\n",
    "        if fillval is not None:\n",
    "            dn[dn == float(fillval)] = np.nan\n",
    "        if badval is not None:\n",
    "            dn[dn == float(badval)] = np.nan\n",
    "\n",
    "        rad = (dn - offsets[i]) * scales[i]           # scale to radiance\n",
    "        rad = rad.astype(np.float32)\n",
    "        rad[~np.isfinite(rad)] = np.nan\n",
    "        out[b] = rad\n",
    "\n",
    "    sds.endaccess()\n",
    "    sd.end()\n",
    "    return out\n",
    "\n",
    "def read_modis_geo(mod03_path: Path) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read MOD03/MYD03 geolocation: Latitude, Longitude (float32).\n",
    "    \"\"\"\n",
    "    sd = SD(str(mod03_path), SDC.READ)\n",
    "    lat = sd.select(\"Latitude\").get().astype(np.float32)\n",
    "    lon = sd.select(\"Longitude\").get().astype(np.float32)\n",
    "    sd.end()\n",
    "    lat[(lat < -90) | (lat > 90)] = np.nan\n",
    "    lon[(lon < -180) | (lon > 180)] = np.nan\n",
    "    return lat, lon\n",
    "\n",
    "# ==============================================================================\n",
    "#                        GRIDDING / RESAMPLING (WGS84)\n",
    "# ==============================================================================\n",
    "\n",
    "def define_area_wgs84_intersection(lat: np.ndarray,\n",
    "                                   lon: np.ndarray,\n",
    "                                   aoi_geom,\n",
    "                                   res_deg: float = GRID_RES_DEG,\n",
    "                                   pad: float = 0.0):\n",
    "    \"\"\"\n",
    "    Create a pyresample AreaDefinition for the intersection of the swath bbox and the AOI.\n",
    "    Returns (area_def, transform, width, height) or None if no intersection.\n",
    "    \"\"\"\n",
    "    sw_lon_min, sw_lon_max = float(np.nanmin(lon)), float(np.nanmax(lon))\n",
    "    sw_lat_min, sw_lat_max = float(np.nanmin(lat)), float(np.nanmax(lat))\n",
    "    aoi_lon_min, aoi_lat_min, aoi_lon_max, aoi_lat_max = aoi_geom.bounds\n",
    "\n",
    "    lon_min = max(sw_lon_min, aoi_lon_min) - pad\n",
    "    lon_max = min(sw_lon_max, aoi_lon_max) + pad\n",
    "    lat_min = max(sw_lat_min, aoi_lat_min) - pad\n",
    "    lat_max = min(sw_lat_max, aoi_lat_max) + pad\n",
    "    if not (lon_min < lon_max and lat_min < lat_max):\n",
    "        return None\n",
    "\n",
    "    width  = int(np.ceil((lon_max - lon_min) / res_deg))\n",
    "    height = int(np.ceil((lat_max - lat_min) / res_deg))\n",
    "    transform = from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "\n",
    "    proj_dict = {\"proj\": \"longlat\", \"datum\": \"WGS84\"}\n",
    "    area_def = geometry.AreaDefinition(\n",
    "        \"wgs84\", \"WGS84 latlon\", \"epsg4326\",\n",
    "        proj_dict, width, height, (lon_min, lat_min, lon_max, lat_max)\n",
    "    )\n",
    "    return area_def, transform, width, height\n",
    "\n",
    "def resample_swath_to_grid(lat: np.ndarray,\n",
    "                           lon: np.ndarray,\n",
    "                           data: np.ndarray,\n",
    "                           area_def) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nearest-neighbor resampling with a 6 km radius of influence (typical for 1 km MODIS).\n",
    "    \"\"\"\n",
    "    swath_def = geometry.SwathDefinition(lons=lon, lats=lat)\n",
    "    out = kd_tree.resample_nearest(\n",
    "        swath_def, data, area_def,\n",
    "        radius_of_influence=6000, fill_value=np.nan\n",
    "    )\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def write_geotiff(path: Path,\n",
    "                  arr: np.ndarray,\n",
    "                  transform,\n",
    "                  crs=CRS.from_epsg(4326),\n",
    "                  nodata=np.float32(np.nan),\n",
    "                  band_tags: dict | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Write a single-band GeoTIFF with LZW compression and NaN nodata.\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": arr.shape[0],\n",
    "        \"width\":  arr.shape[1],\n",
    "        \"count\":  1,\n",
    "        \"dtype\":  rasterio.float32,\n",
    "        \"crs\":    crs,\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": nodata,\n",
    "        \"compress\": \"lzw\",\n",
    "        \"tiled\": True\n",
    "    }\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(arr, 1)\n",
    "        if band_tags:\n",
    "            dst.update_tags(1, **band_tags)\n",
    "\n",
    "# ==============================================================================\n",
    "#                                   LOGGING\n",
    "# ==============================================================================\n",
    "\n",
    "LOG_CSV = BASEDIR / \"processing_log_modis.csv\"\n",
    "\n",
    "def append_log(row: dict) -> None:\n",
    "    \"\"\"\n",
    "    Append one processing record to the global MODIS log (creates file on first call).\n",
    "    \"\"\"\n",
    "    exists = LOG_CSV.exists()\n",
    "    with LOG_CSV.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\n",
    "            \"l1b_file\",\"geo_file\",\"platform\",\"timestamp_key\",\n",
    "            \"acq_date_utc\",\"acq_time_utc\",\"acq_date_local\",\"acq_time_local\",\"local_tz\",\n",
    "            \"cloud_thresh_K\",\n",
    "            \"bt_B21_min\",\"bt_B21_max\",\"bt_B21_mean\",\n",
    "            \"bt_B31_min\",\"bt_B31_max\",\"bt_B31_mean\"\n",
    "        ])\n",
    "        if not exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "# ==============================================================================\n",
    "#                                     MAIN\n",
    "# ==============================================================================\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Iterate days, process all MODIS L1B/GEO pairs in the manifest, and write products + log.\n",
    "    \"\"\"\n",
    "    aoi = read_aoi_wgs84(AOI_SHP)\n",
    "\n",
    "    for day in day_iter(START_DATE, END_DATE):\n",
    "        daydir   = BASEDIR / day.strftime(\"%Y-%m-%d\")\n",
    "        manifest = daydir / \"manifest_pairs_modis.csv\"\n",
    "        outdir   = daydir / \"bt\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not manifest.exists():\n",
    "            print(f\"[INFO] {day}: no manifest found; skipping.\")\n",
    "            continue\n",
    "\n",
    "        with manifest.open(\"r\", newline=\"\") as f:\n",
    "            rows = list(csv.DictReader(f))\n",
    "\n",
    "        print(f\"[INFO] {day}: processing {len(rows)} MODIS pairs …\")\n",
    "\n",
    "        for row in rows:\n",
    "            l1b_p = Path(row[\"l1b_path\"])\n",
    "            geo_p = Path(row[\"geo_path\"])\n",
    "            plat  = row.get(\"platform\", \"\")\n",
    "            key   = row.get(\"timestamp_key\", \"\")\n",
    "\n",
    "            if not l1b_p.exists() or not geo_p.exists():\n",
    "                print(f\"[WARN] Missing files for {key}; skip.\")\n",
    "                continue\n",
    "\n",
    "            # 1) Radiance for B21/B31 (W·m^-2·sr^-1·µm^-1)\n",
    "            rad_map = read_modis_emissive(l1b_p, want_bands=(21, 31))\n",
    "            rad21, rad31 = rad_map.get(21), rad_map.get(31)\n",
    "            if rad21 is None and rad31 is None:\n",
    "                print(f\"[WARN] {l1b_p.name}: neither B21 nor B31 present; skip.\")\n",
    "                continue\n",
    "\n",
    "            # 2) Brightness temperature via MCST ν_eff-derived (K1, K2)\n",
    "            bt21 = bt_from_radiance_with_bandnum(rad21, \"21\") if rad21 is not None else None\n",
    "            bt31 = bt_from_radiance_with_bandnum(rad31, \"31\") if rad31 is not None else None\n",
    "\n",
    "            # 3) Cloud mask from B31 BT (optional/simple)\n",
    "            cloud_mask = (bt31 < CLOUD_BT_K) if bt31 is not None else None\n",
    "\n",
    "            # 4) Geolocation (MOD03/MYD03)\n",
    "            lat, lon = read_modis_geo(geo_p)\n",
    "\n",
    "            # 5) Output grid = swath ∩ AOI\n",
    "            area = define_area_wgs84_intersection(lat, lon, aoi, res_deg=GRID_RES_DEG)\n",
    "            if area is None:\n",
    "                print(\"[INFO] Swath does not intersect AOI; skipping.\")\n",
    "                continue\n",
    "            area_def, transform, width, height = area\n",
    "\n",
    "            # Helper: apply cloud mask (if present)\n",
    "            def apply_masks(a: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "                if a is None:\n",
    "                    return None\n",
    "                out = a.copy()\n",
    "                if cloud_mask is not None:\n",
    "                    out[cloud_mask] = np.nan\n",
    "                return out\n",
    "\n",
    "            rad21_c = apply_masks(rad21); rad31_c = apply_masks(rad31)\n",
    "            bt21_c  = apply_masks(bt21);  bt31_c  = apply_masks(bt31)\n",
    "\n",
    "            # 6) Resample to grid\n",
    "            if rad21_c is not None:\n",
    "                rad21_g = resample_swath_to_grid(lat, lon, rad21_c, area_def)\n",
    "                bt21_g  = resample_swath_to_grid(lat, lon, bt21_c,  area_def)\n",
    "            if rad31_c is not None:\n",
    "                rad31_g = resample_swath_to_grid(lat, lon, rad31_c, area_def)\n",
    "                bt31_g  = resample_swath_to_grid(lat, lon, bt31_c,  area_def)\n",
    "\n",
    "            # 7) AOI clip mask\n",
    "            mask_aoi = geometry_mask([mapping(aoi)],\n",
    "                                     out_shape=(height, width),\n",
    "                                     transform=transform,\n",
    "                                     invert=True).astype(bool)\n",
    "\n",
    "            stem = l1b_p.with_suffix(\"\").name\n",
    "\n",
    "            # 8) Write products\n",
    "            # -- Band 21\n",
    "            if rad21_c is not None:\n",
    "                rad21_g = np.where(mask_aoi, rad21_g, np.nan)\n",
    "                write_geotiff(outdir / f\"{stem}_B21_Rad.tif\", rad21_g, transform,\n",
    "                              band_tags={\"units\": \"W/m^2/sr/μm\", \"long_name\": \"MODIS Band 21 Radiance\"})\n",
    "                bt21_g = np.where(mask_aoi, bt21_g, np.nan)\n",
    "                write_geotiff(outdir / f\"{stem}_B21_BT_K.tif\", bt21_g, transform,\n",
    "                              band_tags={\"units\": \"K\", \"long_name\": \"MODIS Band 21 Brightness Temperature\"})\n",
    "                print(f\"[OK] wrote B21 radiance & BT for {stem}\")\n",
    "\n",
    "            # -- Band 31\n",
    "            if rad31_c is not None:\n",
    "                rad31_g = np.where(mask_aoi, rad31_g, np.nan)\n",
    "                write_geotiff(outdir / f\"{stem}_B31_Rad.tif\", rad31_g, transform,\n",
    "                              band_tags={\"units\": \"W/m^2/sr/μm\", \"long_name\": \"MODIS Band 31 Radiance\"})\n",
    "                bt31_g = np.where(mask_aoi, bt31_g, np.nan)\n",
    "                write_geotiff(outdir / f\"{stem}_B31_BT_K.tif\", bt31_g, transform,\n",
    "                              band_tags={\"units\": \"K\", \"long_name\": \"MODIS Band 31 Brightness Temperature\"})\n",
    "                print(f\"[OK] wrote B31 radiance & BT for {stem}\")\n",
    "\n",
    "            # 9) Logging (UTC/local time + BT stats)\n",
    "            dt_utc = dt_from_timestamp_key(key)\n",
    "            if dt_utc is not None:\n",
    "                dt_loc = dt_utc.astimezone(LOCAL_TZ)\n",
    "                d_utc, t_utc = dt_utc.strftime(\"%Y-%m-%d\"), dt_utc.strftime(\"%H:%M\")\n",
    "                d_loc, t_loc = dt_loc.strftime(\"%Y-%m-%d\"), dt_loc.strftime(\"%H:%M\")\n",
    "            else:\n",
    "                d_utc = t_utc = d_loc = t_loc = \"\"\n",
    "\n",
    "            def stats(a: Optional[np.ndarray]) -> tuple[float, float, float]:\n",
    "                if a is None:\n",
    "                    return (np.nan, np.nan, np.nan)\n",
    "                vv = a[np.isfinite(a)]\n",
    "                if vv.size == 0:\n",
    "                    return (np.nan, np.nan, np.nan)\n",
    "                return (float(np.nanmin(vv)), float(np.nanmax(vv)), float(np.nanmean(vv)))\n",
    "\n",
    "            s21 = stats(bt21_g if rad21_c is not None else None)\n",
    "            s31 = stats(bt31_g if rad31_c is not None else None)\n",
    "\n",
    "            append_log({\n",
    "                \"l1b_file\": str(l1b_p),\n",
    "                \"geo_file\": str(geo_p),\n",
    "                \"platform\":  plat,\n",
    "                \"timestamp_key\": key,\n",
    "                \"acq_date_utc\":  d_utc,\n",
    "                \"acq_time_utc\":  t_utc,\n",
    "                \"acq_date_local\": d_loc,\n",
    "                \"acq_time_local\": t_loc,\n",
    "                \"local_tz\": LOCAL_TZNAME,\n",
    "                \"cloud_thresh_K\": CLOUD_BT_K,\n",
    "                \"bt_B21_min\": s21[0], \"bt_B21_max\": s21[1], \"bt_B21_mean\": s21[2],\n",
    "                \"bt_B31_min\": s31[0], \"bt_B31_max\": s31[1], \"bt_B31_mean\": s31[2],\n",
    "            })\n",
    "\n",
    "    print(f\"[DONE] Log at: {LOG_CSV}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded03c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ modis_process_af.py =============================\n",
    "\"\"\"\n",
    "MODIS Active Fire (MOD14/MYD14) → GeoTIFFs (FireMask + Detection)\n",
    "==================================================================\n",
    "\n",
    "Per-day outputs in: BASEDIR/YYYY-MM-DD/af/\n",
    "  - <HDF_basename>_AF_FireMask.tif\n",
    "      • float32; FireMask classes (1..9) inside AOI; NaN outside AOI\n",
    "  - <HDF_basename>_AF_Detect.tif\n",
    "      • float32; 1.0 where FireMask ∈ {7,8,9}, 0.0 elsewhere; NaN outside AOI\n",
    "\n",
    "Geolocation\n",
    "-----------\n",
    "• Prefer Latitude/Longitude SDS embedded in AF file.\n",
    "• Otherwise, auto-pair a MOD03/MYD03 HDF by timestamp (.Ayyyyddd.HHMM.) found\n",
    "  within the same day’s raw_modis/ folder.\n",
    "\n",
    "Processing\n",
    "----------\n",
    "1) Read FireMask and lat/lon (internal or paired GEO).\n",
    "2) Build EPSG:4326 output grid as swath∩AOI; resample with nearest-neighbor.\n",
    "3) AOI clip (NaN outside).\n",
    "4) Write FireMask + binary Detect GeoTIFFs.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "• Detection mask uses FireMask classes {7,8,9}.\n",
    "• Dependencies: pyhdf rasterio pyresample fiona shapely pyproj tzdata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ------------------------------ Std lib --------------------------------\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# -------------------------------- NumPy --------------------------------\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------- HDF4 ---------------------------------\n",
    "from pyhdf.SD import SD, SDC  # HDF4 reader\n",
    "\n",
    "# -------------------------------- Raster --------------------------------\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ------------------------------ Vectors --------------------------------\n",
    "import fiona\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "# ----------------------------- Resampling ------------------------------\n",
    "from pyresample import geometry, kd_tree\n",
    "\n",
    "# ------------------------------- CONFIG --------------------------------\n",
    "BASEDIR    = Path(r\"path\\to\\base\\directory\")\n",
    "AOI_SHP    = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\")\n",
    "START_DATE = date(2025, 1, 8)\n",
    "END_DATE   = date(2025, 1, 12)\n",
    "\n",
    "# Output grid resolution in degrees (≈1 km at equator: 0.008333)\n",
    "GRID_RES_DEG = 0.008333\n",
    "\n",
    "# ----------------------------- Utilities -------------------------------\n",
    "def day_iter(d0: date, d1: date):\n",
    "    \"\"\"Inclusive date iterator: d0, d0+1, …, d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "def read_aoi_wgs84(shp_path: Path):\n",
    "    \"\"\"\n",
    "    Read AOI shapefile (any CRS) → unified geometry in EPSG:4326.\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"AOI not found: {shp_path}\")\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(f[\"geometry\"]) for f in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != dst_crs_obj:\n",
    "        tf = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: tf.transform(x, y), aoi)\n",
    "    return aoi\n",
    "\n",
    "def define_area_wgs84_intersection(lat, lon, aoi_geom, res_deg=GRID_RES_DEG, pad=0.0):\n",
    "    \"\"\"\n",
    "    Define EPSG:4326 output grid as intersection of swath extent and AOI extent.\n",
    "    Returns (area_def, transform, width, height) or None if no intersection.\n",
    "    \"\"\"\n",
    "    sw_lon_min = float(np.nanmin(lon)); sw_lon_max = float(np.nanmax(lon))\n",
    "    sw_lat_min = float(np.nanmin(lat)); sw_lat_max = float(np.nanmax(lat))\n",
    "    aoi_lon_min, aoi_lat_min, aoi_lon_max, aoi_lat_max = aoi_geom.bounds\n",
    "\n",
    "    lon_min = max(sw_lon_min, aoi_lon_min) - pad\n",
    "    lon_max = min(sw_lon_max, aoi_lon_max) + pad\n",
    "    lat_min = max(sw_lat_min, aoi_lat_min) - pad\n",
    "    lat_max = min(sw_lat_max, aoi_lat_max) + pad\n",
    "    if not (lon_min < lon_max and lat_min < lat_max):\n",
    "        return None\n",
    "\n",
    "    width  = int(np.ceil((lon_max - lon_min) / res_deg))\n",
    "    height = int(np.ceil((lat_max - lat_min) / res_deg))\n",
    "    transform = from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "    proj_dict = {\"proj\": \"longlat\", \"datum\": \"WGS84\"}\n",
    "    area_def = geometry.AreaDefinition(\n",
    "        \"wgs84\", \"WGS84 latlon\", \"epsg4326\",\n",
    "        proj_dict, width, height, (lon_min, lat_min, lon_max, lat_max)\n",
    "    )\n",
    "    return area_def, transform, width, height\n",
    "\n",
    "def resample_swath_to_grid(lat, lon, data, area_def):\n",
    "    \"\"\"Nearest-neighbor resampling from swath to target grid.\"\"\"\n",
    "    swath_def = geometry.SwathDefinition(lons=lon, lats=lat)\n",
    "    out = kd_tree.resample_nearest(\n",
    "        swath_def, data, area_def,\n",
    "        radius_of_influence=6000, fill_value=np.nan\n",
    "    )\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def write_geotiff(path: Path, arr: np.ndarray, transform, crs=CRS.from_epsg(4326),\n",
    "                  nodata=np.float32(np.nan), band_tags: dict | None = None, dtype=rasterio.float32):\n",
    "    \"\"\"Write a single-band GeoTIFF with LZW compression and NaN nodata.\"\"\"\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\", \"height\": arr.shape[0], \"width\": arr.shape[1], \"count\": 1,\n",
    "        \"dtype\": dtype, \"crs\": crs, \"transform\": transform,\n",
    "        \"nodata\": nodata, \"compress\": \"lzw\", \"tiled\": True\n",
    "    }\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(arr.astype(profile[\"dtype\"]), 1)\n",
    "        if band_tags:\n",
    "            dst.update_tags(1, **band_tags)\n",
    "\n",
    "# ------------------------- HDF4 helpers (robust) ------------------------\n",
    "def _list_sds_names(sd: SD) -> list[str]:\n",
    "    \"\"\"Return all SDS names in this HDF4 file (original case).\"\"\"\n",
    "    info = sd.datasets()  # dict: name -> (idx, rank, dims, type, nattrs)\n",
    "    return list(info.keys())\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    \"\"\"Lowercase alnum-only normalization for forgiving name matches.\"\"\"\n",
    "    return \"\".join(ch for ch in s.lower() if ch.isalnum())\n",
    "\n",
    "def _find_sds(sd: SD, candidates: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Case/punct-insensitive search among SDS names. Returns the original SDS name or None.\n",
    "    \"\"\"\n",
    "    names = _list_sds_names(sd)\n",
    "    names_norm = {_normalize(n): n for n in names}\n",
    "    for cand in candidates:\n",
    "        key = _normalize(cand)\n",
    "        if key in names_norm:\n",
    "            return names_norm[key]\n",
    "    return None\n",
    "\n",
    "def _read_sds_scaled(sd: SD, sds_name: str):\n",
    "    \"\"\"\n",
    "    Read an SDS and apply optional scale_factor/add_offset and _FillValue handling.\n",
    "    Returns (float32 array, attributes dict).\n",
    "    \"\"\"\n",
    "    sds = sd.select(sds_name)\n",
    "    arr = sds.get()  # may raise if inaccessible\n",
    "    attrs = sds.attributes()\n",
    "\n",
    "    arr = arr.astype(np.float64)\n",
    "    fv = attrs.get(\"_FillValue\", None)\n",
    "    if fv is not None:\n",
    "        arr[arr == float(fv)] = np.nan\n",
    "\n",
    "    scale = attrs.get(\"scale_factor\", None)\n",
    "    offs  = attrs.get(\"add_offset\", None)\n",
    "    if scale is not None or offs is not None:\n",
    "        s = float(scale if scale is not None else 1.0)\n",
    "        a = float(offs  if offs  is not None else 0.0)\n",
    "        arr = arr * s + a\n",
    "\n",
    "    return arr.astype(np.float32), attrs\n",
    "\n",
    "# --------------------------- AF readers --------------------------------\n",
    "def timestamp_key_from_name(name: str) -> str | None:\n",
    "    \"\"\"Extract '.Ayyyyddd.HHMM.' from filename, if present.\"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def read_af_latlon_or_pair(af_path: Path, fallback_dirs: list[Path]):\n",
    "    \"\"\"\n",
    "    Try internal AF lat/lon SDS; else auto-pair a MOD03/MYD03 HDF within fallback_dirs\n",
    "    using the timestamp key. Returns (lat, lon) or (None, None).\n",
    "    \"\"\"\n",
    "    sd = SD(str(af_path), SDC.READ)\n",
    "\n",
    "    # Preferred: internal lat/lon\n",
    "    lat_name = _find_sds(sd, [\"Latitude\", \"latitude\"])\n",
    "    lon_name = _find_sds(sd, [\"Longitude\", \"longitude\"])\n",
    "\n",
    "    if lat_name and lon_name:\n",
    "        try:\n",
    "            lat = sd.select(lat_name).get().astype(np.float32)\n",
    "            lon = sd.select(lon_name).get().astype(np.float32)\n",
    "            sd.end()\n",
    "        except Exception:\n",
    "            sd.end()\n",
    "            lat = lon = None\n",
    "    else:\n",
    "        # Fallback: pair MOD03/MYD03 by timestamp\n",
    "        sd.end()\n",
    "        key = timestamp_key_from_name(af_path.name)\n",
    "        if not key:\n",
    "            return None, None\n",
    "        is_aqua = af_path.name.startswith(\"MYD\")\n",
    "        geo_prefix = \"MYD03\" if is_aqua else \"MOD03\"\n",
    "        pat = f\"{geo_prefix}.{key}*.hdf\"\n",
    "        lat = lon = None\n",
    "        for d in fallback_dirs:\n",
    "            for p in d.glob(pat):\n",
    "                try:\n",
    "                    g = SD(str(p), SDC.READ)\n",
    "                    lat = g.select(\"Latitude\").get().astype(np.float32)\n",
    "                    lon = g.select(\"Longitude\").get().astype(np.float32)\n",
    "                    g.end()\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if lat is not None:\n",
    "                break\n",
    "\n",
    "    if lat is None or lon is None:\n",
    "        return None, None\n",
    "\n",
    "    # Basic range hygiene\n",
    "    lat[(lat < -90) | (lat > 90)] = np.nan\n",
    "    lon[(lon < -180) | (lon > 180)] = np.nan\n",
    "    return lat, lon\n",
    "\n",
    "def read_firemask_only(af_path: Path):\n",
    "    \"\"\"\n",
    "    Read the FireMask SDS (2-D) as float32 with NaNs for fill. Returns None if absent.\n",
    "    \"\"\"\n",
    "    sd = SD(str(af_path), SDC.READ)\n",
    "    nm_firemask = _find_sds(sd, [\"FireMask\", \"fire_mask\", \"Fire Mask\", \"PixelFireMask\", \"FP_Mask\", \"Mask\"])\n",
    "    if not nm_firemask:\n",
    "        sd.end()\n",
    "        return None\n",
    "    try:\n",
    "        fm, _ = _read_sds_scaled(sd, nm_firemask)\n",
    "        sd.end()\n",
    "        return fm  # float32; classes ~1..9; NaN for fill\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] FireMask read failed ({nm_firemask}) in {af_path.name}: {e}\")\n",
    "        sd.end()\n",
    "        return None\n",
    "\n",
    "# -------------------------------- Main -----------------------------------\n",
    "def main():\n",
    "    aoi = read_aoi_wgs84(AOI_SHP)\n",
    "\n",
    "    for day in day_iter(START_DATE, END_DATE):\n",
    "        daydir = BASEDIR / day.strftime(\"%Y-%m-%d\")\n",
    "        outdir = daydir / \"af\"\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        manifest_af = daydir / \"manifest_active_fire_modis.csv\"\n",
    "        rawdir = daydir / \"raw_modis\"\n",
    "\n",
    "        # Build list of AF files to process\n",
    "        af_files: list[Path] = []\n",
    "        if manifest_af.exists():\n",
    "            with manifest_af.open(\"r\", newline=\"\") as f:\n",
    "                for row in csv.DictReader(f):\n",
    "                    p = Path(row[\"file_path\"])\n",
    "                    if p.exists():\n",
    "                        af_files.append(p)\n",
    "        else:\n",
    "            # Fallback: scan raw_modis for MOD14/MYD14 HDFs\n",
    "            if rawdir.exists():\n",
    "                af_files += list(rawdir.glob(\"MOD14.A*.hdf\"))\n",
    "                af_files += list(rawdir.glob(\"MYD14.A*.hdf\"))\n",
    "\n",
    "        if not af_files:\n",
    "            print(f\"[INFO] {day}: no AF files found; skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] {day}: processing {len(af_files)} MODIS AF granules …\")\n",
    "\n",
    "        # Search paths for GEO fallback when AF lacks internal lat/lon\n",
    "        fallback_geo_dirs = [rawdir]\n",
    "\n",
    "        for af_path in af_files:\n",
    "            # 1) Geolocation\n",
    "            lat, lon = read_af_latlon_or_pair(af_path, fallback_geo_dirs)\n",
    "            if lat is None or lon is None:\n",
    "                print(f\"[WARN] No geolocation for {af_path.name}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2) FireMask\n",
    "            firemask = read_firemask_only(af_path)\n",
    "            if firemask is None:\n",
    "                print(f\"[WARN] No FireMask in {af_path.name}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Sanity check: shapes must match for resampling input\n",
    "            if firemask.shape != lat.shape:\n",
    "                print(f\"[WARN] FireMask shape {firemask.shape} != geo {lat.shape} in {af_path.name}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 3) Output grid = swath ∩ AOI\n",
    "            area = define_area_wgs84_intersection(lat, lon, aoi, res_deg=GRID_RES_DEG)\n",
    "            if area is None:\n",
    "                print(f\"[INFO] AF swath ({af_path.name}) does not intersect AOI; skipping.\")\n",
    "                continue\n",
    "            area_def, transform, width, height = area\n",
    "\n",
    "            # 4) Detection mask: FireMask ∈ {7,8,9} → 1.0 else 0.0\n",
    "            fm_int = np.rint(firemask).astype(np.int16)\n",
    "            det = np.isin(fm_int, (7, 8, 9)).astype(np.float32)\n",
    "\n",
    "            # 5) Resample to target grid\n",
    "            fm_grid  = resample_swath_to_grid(lat, lon, firemask.astype(np.float32), area_def)\n",
    "            det_grid = resample_swath_to_grid(lat, lon, det,                         area_def)\n",
    "\n",
    "            # 6) AOI clip (NaN outside)\n",
    "            mask_aoi = geometry_mask([mapping(aoi)], out_shape=(height, width),\n",
    "                                     transform=transform, invert=True).astype(bool)\n",
    "            fm_grid  = np.where(mask_aoi, fm_grid,  np.nan)\n",
    "            det_grid = np.where(mask_aoi, det_grid, np.nan)\n",
    "\n",
    "            # 7) Write GeoTIFFs\n",
    "            stem = af_path.with_suffix(\"\").name\n",
    "\n",
    "            out_fm = outdir / f\"{stem}_AF_FireMask.tif\"\n",
    "            write_geotiff(\n",
    "                out_fm, fm_grid, transform,\n",
    "                band_tags={\n",
    "                    \"units\": \"class\",\n",
    "                    \"long_name\": \"MODIS AF FireMask\",\n",
    "                    \"classes\": \"1:obsolete,2:not_processed,3:water,4:cloud,5:land_nonfire,6:unknown,7:fire_low,8:fire_nominal,9:fire_high\"\n",
    "                }\n",
    "            )\n",
    "            print(f\"[OK] wrote {out_fm}\")\n",
    "\n",
    "            out_det = outdir / f\"{stem}_AF_Detect.tif\"\n",
    "            write_geotiff(\n",
    "                out_det, det_grid, transform,\n",
    "                band_tags={\n",
    "                    \"units\": \"binary\",\n",
    "                    \"definition\": \"1=Fire (FireMask in {7,8,9}); 0=No fire; NaN outside AOI\"\n",
    "                }\n",
    "            )\n",
    "            print(f\"[OK] wrote {out_det}\")\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
