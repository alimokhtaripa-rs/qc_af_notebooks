{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- viirs_download.py -----------------------------\n",
    "\"\"\"\n",
    "VIIRS Downloader (dated folders + Active Fire)\n",
    "==============================================\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "• Authenticates to NASA Earthdata (via EARTHDATA_USERNAME / EARTHDATA_PASSWORD env vars,\n",
    "  or secure prompt fallback) and initializes earthaccess.\n",
    "• Reads an AOI shapefile (any CRS), computes an EPSG:4326 bounding box.\n",
    "• For each day in START_DATE..END_DATE (inclusive):\n",
    "    - Searches & downloads VIIRS L1B radiance (VJ202IMG) and GEO (VJ203IMG)\n",
    "      into BASEDIR/YYYY-MM-DD/raw/\n",
    "    - Optionally searches & downloads Active Fire (e.g., VJ214IMG) into the same folder\n",
    "    - Writes per-day manifests in BASEDIR/YYYY-MM-DD/:\n",
    "        * manifest_pairs.csv         — L1B↔GEO pairs + QC flag presence for I04/I05\n",
    "        * manifest_active_fire.csv   — AF files + UTC time + has_internal_geo + paired GEO path (if needed)\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "pip install earthaccess fiona shapely pyproj h5py\n",
    "\n",
    "Notes\n",
    "-----\n",
    "• Product short names default to NOAA-21 (VJ2*). For S-NPP you would switch to VNP*,\n",
    "  and for NOAA-20 to VJ1*.\n",
    "• Network timeouts are set to 20 s (socket.setdefaulttimeout).\n",
    "• The pairing uses the canonical 'Ayyyyddd.HHMM' key; if an exact GEO match is missing,\n",
    "  a same-day fallback is used.\n",
    "\"\"\"\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Standard libs ================================\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "# ============================== Third-party ================================\n",
    "import earthaccess as ea\n",
    "import h5py\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "# ---------------------------- Global settings -------------------------------\n",
    "socket.setdefaulttimeout(20)  # fail fast on flaky networks\n",
    "\n",
    "# =============================== Configuration ==============================\n",
    "# Core radiance + geolocation short names (NOAA-21 defaults). For NOAA-20/21 swap VJ1*/VJ2*;\n",
    "# for S-NPP use VNP* equivalents.\n",
    "SHORT_NAME_L1B = \"VJ202IMG\"   # VIIRS I/M band SDRs (radiances)\n",
    "SHORT_NAME_GEO = \"VJ203IMG\"   # VIIRS geolocation\n",
    "\n",
    "# Active Fire (optional). Add/remove short names as needed.\n",
    "INCLUDE_ACTIVE_FIRE = True\n",
    "AF_SHORTNAMES = [\n",
    "    # \"VNP14IMG\",  # S-NPP (uncomment to include)\n",
    "    # \"VJ114IMG\",  # NOAA-20 (uncomment to include)\n",
    "    \"VJ214IMG\",    # NOAA-21 (kept from original)\n",
    "]\n",
    "\n",
    "# Time window (inclusive by day)\n",
    "START_DATE = date(2025, 1, 1)\n",
    "END_DATE   = date(2025, 1, 11)\n",
    "\n",
    "# Base path; dated subfolders will be created under here (YYYY-MM-DD/{raw, manifests })\n",
    "BASEDIR = Path(\n",
    "    r\"path\\to\\base\\directory\"\n",
    ")\n",
    "\n",
    "# AOI shapefile (any polygon/multipolygon; any CRS)\n",
    "AOI_SHP = Path(\n",
    "    r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================== Auth utilities ==============================\n",
    "\n",
    "def earthdata_login_username_password() -> None:\n",
    "    \"\"\"\n",
    "    Authenticate via environment variables EARTHDATA_USERNAME / EARTHDATA_PASSWORD.\n",
    "    If absent, securely prompt and set them in the environment for this process.\n",
    "    \"\"\"\n",
    "    user = os.getenv(\"EARTHDATA_USERNAME\")\n",
    "    pwd  = os.getenv(\"EARTHDATA_PASSWORD\")\n",
    "    if not user or not pwd:\n",
    "        import getpass\n",
    "        user = user or input(\"Earthdata username: \").strip()\n",
    "        pwd  = pwd  or getpass.getpass(\"Earthdata password: \")\n",
    "        os.environ[\"EARTHDATA_USERNAME\"] = user\n",
    "        os.environ[\"EARTHDATA_PASSWORD\"] = pwd\n",
    "    ea.login(strategy=\"environment\")\n",
    "\n",
    "\n",
    "# ============================== AOI & helpers ===============================\n",
    "\n",
    "def read_aoi_bbox_wgs84(shp_path: Path) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Read the AOI shapefile and return a clamped WGS84 bounding box (minx, miny, maxx, maxy).\n",
    "\n",
    "    • Accepts any input CRS; reprojects to EPSG:4326.\n",
    "    • Clamps to valid lon/lat bounds ([-180,180], [-90,90]).\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"AOI not found: {shp_path}\")\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(feat[\"geometry\"]) for feat in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    wgs84 = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != wgs84:\n",
    "        transformer = Transformer.from_crs(src_crs_obj, wgs84, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: transformer.transform(x, y), aoi)\n",
    "\n",
    "    minx, miny, maxx, maxy = aoi.bounds\n",
    "    return (\n",
    "        max(minx, -180.0),\n",
    "        max(miny,  -90.0),\n",
    "        min(maxx,  180.0),\n",
    "        min(maxy,   90.0),\n",
    "    )\n",
    "\n",
    "\n",
    "def date_iter(d0: date, d1: date) -> Iterable[date]:\n",
    "    \"\"\"Inclusive date iterator from d0 to d1 (step=1 day).\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def timestamp_key(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extract '.Ayyyyddd.HHMM.' from the filename if present (canonical key for pairing).\n",
    "    Falls back to stem if not found.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", p.name)\n",
    "    return m.group(1) if m else p.stem\n",
    "\n",
    "\n",
    "def pair_l1b_geo(l1b_paths: Sequence[Path] | Sequence[str],\n",
    "                 geo_paths: Sequence[Path] | Sequence[str]) -> list[tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Pair L1B with GEO using the canonical time key. If no exact GEO exists,\n",
    "    fall back to a GEO from the same day (best effort).\n",
    "    \"\"\"\n",
    "    gmap = {timestamp_key(Path(g)): Path(g) for g in geo_paths}\n",
    "    pairs: list[tuple[Path, Path]] = []\n",
    "    for l in l1b_paths:\n",
    "        l = Path(l)\n",
    "        k = timestamp_key(l)\n",
    "        if k in gmap:\n",
    "            pairs.append((l, gmap[k]))\n",
    "        else:\n",
    "            day = k.split(\".\")[0]  # 'Ayyyyddd'\n",
    "            cands = [g for kk, g in gmap.items() if kk.startswith(day)]\n",
    "            if cands:\n",
    "                pairs.append((l, cands[0]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def has_quality_flags_in_l1b(h5_path: Path, band: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if '/observation_data/<BAND>_quality_flags' exists in the HDF5 L1B file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            return f.get(f\"/observation_data/{band}_quality_flags\") is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def acquisition_dt_from_name(name: str) -> datetime | None:\n",
    "    \"\"\"Parse '.Ayyyyddd.HHMM.' → aware UTC datetime; returns None if not found.\"\"\"\n",
    "    m = re.search(r\"\\.A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})\\.\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "def af_has_internal_geo(h5_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Active Fire HDF5 may include internal geolocation arrays.\n",
    "    Returns True if both '/geolocation_data/latitude' and '/geolocation_data/longitude' exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            return (\"/geolocation_data/latitude\" in f) and (\"/geolocation_data/longitude\" in f)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ================================== Main =====================================\n",
    "\n",
    "def main() -> None:\n",
    "    # ---- AOI & auth ----\n",
    "    bbox = read_aoi_bbox_wgs84(AOI_SHP)\n",
    "    earthdata_login_username_password()\n",
    "\n",
    "    # ---- Iterate days ----\n",
    "    for day in date_iter(START_DATE, END_DATE):\n",
    "        day_str = day.strftime(\"%Y-%m-%d\")\n",
    "        DAYDIR = BASEDIR / day_str\n",
    "        RAWDIR = DAYDIR / \"raw\"\n",
    "        RAWDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        MANIFEST_L1_GEO = DAYDIR / \"manifest_pairs.csv\"\n",
    "        MANIFEST_AF     = DAYDIR / \"manifest_active_fire.csv\"\n",
    "\n",
    "        # Temporal window for Earthdata search: [day, day+1)\n",
    "        t0, t1 = day, day + timedelta(days=1)\n",
    "\n",
    "        print(f\"\\n=== {day_str} ===\")\n",
    "        print(f\"[INFO] Searching {SHORT_NAME_L1B}/{SHORT_NAME_GEO} for {t0}..{t1} in bbox {bbox} …\")\n",
    "\n",
    "        # ---- Search & download core L1B/GEO ----\n",
    "        l1b_items = ea.search_data(short_name=SHORT_NAME_L1B, temporal=(t0, t1), bounding_box=bbox)\n",
    "        geo_items = ea.search_data(short_name=SHORT_NAME_GEO, temporal=(t0, t1), bounding_box=bbox)\n",
    "\n",
    "        if not l1b_items:\n",
    "            print(\"[WARN] No L1B granules for this day.\")\n",
    "        if not geo_items:\n",
    "            print(\"[WARN] No GEO granules for this day.\")\n",
    "\n",
    "        l1b_paths = ea.download(l1b_items, RAWDIR.as_posix()) if l1b_items else []\n",
    "        geo_paths = ea.download(geo_items, RAWDIR.as_posix()) if geo_items else []\n",
    "        print(f\"[OK] Downloaded: {len(l1b_paths)} L1B, {len(geo_paths)} GEO into {RAWDIR}\")\n",
    "\n",
    "        # ---- Pair L1B↔GEO and write manifest ----\n",
    "        if l1b_paths and geo_paths:\n",
    "            pairs = pair_l1b_geo(l1b_paths, geo_paths)\n",
    "            with MANIFEST_L1_GEO.open(\"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"l1b_path\", \"geo_path\", \"timestamp_key\", \"has_qc_I04\", \"has_qc_I05\"])\n",
    "                for l1b_p, geo_p in pairs:\n",
    "                    h4 = has_quality_flags_in_l1b(Path(l1b_p), \"I04\")\n",
    "                    h5 = has_quality_flags_in_l1b(Path(l1b_p), \"I05\")\n",
    "                    w.writerow([str(l1b_p), str(geo_p), timestamp_key(Path(l1b_p)), int(h4), int(h5)])\n",
    "            print(f\"[OK] {MANIFEST_L1_GEO}\")\n",
    "\n",
    "        # ---- Active Fire (optional) ----\n",
    "        if INCLUDE_ACTIVE_FIRE and AF_SHORTNAMES:\n",
    "            af_rows: list[list[str | int]] = []\n",
    "\n",
    "            # Pre-map GEO by timestamp for pairing AF that lack internal geolocation\n",
    "            geo_map = {timestamp_key(Path(g)): Path(g) for g in geo_paths}\n",
    "\n",
    "            for sn in AF_SHORTNAMES:\n",
    "                print(f\"[INFO] Searching AF {sn} for {t0}..{t1} …\")\n",
    "                items = ea.search_data(short_name=sn, temporal=(t0, t1), bounding_box=bbox)\n",
    "                if not items:\n",
    "                    print(f\"[INFO] No AF granules for {sn} on {day_str}.\")\n",
    "                    continue\n",
    "\n",
    "                paths = ea.download(items, RAWDIR.as_posix())\n",
    "                for p in paths:\n",
    "                    p = Path(p)\n",
    "                    key = timestamp_key(p)\n",
    "                    dt  = acquisition_dt_from_name(p.name)\n",
    "                    acq_date = dt.strftime(\"%Y-%m-%d\") if dt else \"\"\n",
    "                    acq_time = dt.strftime(\"%H:%M\")     if dt else \"\"\n",
    "                    has_geo  = af_has_internal_geo(p)\n",
    "                    paired_geo = str(geo_map.get(key, \"\")) if not has_geo else \"\"\n",
    "                    af_rows.append([sn, str(p), key, acq_date, acq_time, int(has_geo), paired_geo])\n",
    "\n",
    "            if af_rows:\n",
    "                with MANIFEST_AF.open(\"w\", newline=\"\") as f:\n",
    "                    w = csv.writer(f)\n",
    "                    w.writerow([\n",
    "                        \"product\", \"file_path\", \"timestamp_key\",\n",
    "                        \"acq_date_utc\", \"acq_time_utc\",\n",
    "                        \"has_internal_geo\", \"paired_geo_path\"\n",
    "                    ])\n",
    "                    w.writerows(af_rows)\n",
    "                print(f\"[OK] {MANIFEST_AF}\")\n",
    "\n",
    "    print(\"\\n[DONE] All requested days downloaded into dated folders under:\", BASEDIR)\n",
    "\n",
    "\n",
    "# ================================== CLI ======================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc01dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- viirs_process_rad_bt.py ---------------------------\n",
    "\"\"\"\n",
    "VIIRS L1B Processor (date range; Radiance + Planck-derived BT)\n",
    "================================================================\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "• Processes all days from START_DATE to END_DATE (inclusive).\n",
    "• Expects dated folders from a downloader:\n",
    "\n",
    "    BASEDIR/YYYY-MM-DD/\n",
    "        raw/                    # HDF5 swaths: L1B (VJ202IMG) + GEO (VJ203IMG)\n",
    "        manifest_pairs.csv      # (optional) L1B↔GEO pairs; auto-fallback scans raw/\n",
    "        bt/                     # created here\n",
    "\n",
    "• Outputs per day to BASEDIR/YYYY-MM-DD/bt/:\n",
    "    *_I04_Rad.tif, *_I04_BT_K.tif, *_I05_Rad.tif, *_I05_BT_K.tif\n",
    "\n",
    "• Maintains CSV log per day:\n",
    "    BASEDIR/YYYY-MM-DD/processing_log.csv\n",
    "\n",
    "Assumptions / Notes\n",
    "-------------------\n",
    "• Brightness temperature is computed via Planck inversion using nominal λ for I04/I05.\n",
    "• A simple cold-cloud mask is applied from I05 BT (< CLOUD_BT_K = 265 °K) and I05 invalids.\n",
    "• AOI is used for both gridding extent (intersection) and final mask (clip to AOI).\n",
    "• Grid is EPSG:4326 with uniform spacing GRID_RES_DEG.\n",
    "Notes\n",
    "-----\n",
    "• Cloud mask is deliberately simple (cold-cloud threshold on Band 31 BT).\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "pip install h5py numpy rasterio pyresample fiona shapely pyproj tzdata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Imports ======================================\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from zoneinfo import ZoneInfo\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "from pyresample import geometry, kd_tree\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "\n",
    "# ============================ Configuration ==================================\n",
    "# Range to process (inclusive)\n",
    "BASEDIR     = Path(r\"path\\to\\base\\directory\")\n",
    "START_DATE  = date(2025, 1, 1)\n",
    "END_DATE    = date(2025, 1, 11)\n",
    "\n",
    "# AOI shapefile used for intersection + clipping\n",
    "AOI_SHP     = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\")\n",
    "\n",
    "# Grid and radiometry parameters\n",
    "GRID_RES_DEG = 0.0036  # ~400 m at equator (deg per pixel)\n",
    "LAM_I4_UM    = 3.74\n",
    "LAM_I5_UM    = 11.45\n",
    "CLOUD_BT_K   = 265.0   # simple cold-cloud threshold (Kelvin)\n",
    "\n",
    "# Local time zone for logging human-readable times\n",
    "LOCAL_TZNAME = \"America/Chicago\"\n",
    "\n",
    "# =============================== Utilities ===================================\n",
    "def date_iter(d0: date, d1: date):\n",
    "    \"\"\"Inclusive date iterator: d0, d0+1, …, d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def read_aoi_wgs84(shp_path: Path):\n",
    "    \"\"\"\n",
    "    Read AOI shapefile and return a valid, unioned MultiPolygon in EPSG:4326.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shapely.geometry.base.BaseGeometry\n",
    "        AOI geometry in WGS84 (lon/lat).\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"AOI not found: {shp_path}\")\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(feat[\"geometry\"]) for feat in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    dst_crs_obj = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != dst_crs_obj:\n",
    "        transformer = Transformer.from_crs(src_crs_obj, dst_crs_obj, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: transformer.transform(x, y), aoi)\n",
    "    return aoi\n",
    "\n",
    "\n",
    "def timestamp_key(p: Path) -> str:\n",
    "    \"\"\"Extract '.Ayyyyddd.HHMM.' key from filename; fallback to stem.\"\"\"\n",
    "    m = re.search(r\"\\.(A\\d{7}\\.\\d{4})\\.\", p.name)\n",
    "    return m.group(1) if m else p.stem\n",
    "\n",
    "\n",
    "def read_pairs_from_manifest(manifest_path: Path, rawdir: Path) -> list[tuple[Path, Path]]:\n",
    "    \"\"\"\n",
    "    Read L1B↔GEO pairs from manifest if present. Otherwise, build pairs by scanning raw/.\n",
    "    \"\"\"\n",
    "    if manifest_path.exists():\n",
    "        pairs = []\n",
    "        with manifest_path.open(\"r\", newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                pairs.append((Path(row[\"l1b_path\"]), Path(row[\"geo_path\"])))\n",
    "        return pairs\n",
    "\n",
    "    # Fallback: scan for common product names (S-NPP/NOAA-20/21)\n",
    "    l1b = sorted(rawdir.glob(\"VNP02IMG*.h5\")) + sorted(rawdir.glob(\"VJ102IMG*.h5\")) + sorted(rawdir.glob(\"VJ202IMG*.h5\"))\n",
    "    geo = sorted(rawdir.glob(\"VNP03IMG*.h5\")) + sorted(rawdir.glob(\"VJ103IMG*.h5\")) + sorted(rawdir.glob(\"VJ203IMG*.h5\"))\n",
    "    gmap = {timestamp_key(p): p for p in geo}\n",
    "    pairs = []\n",
    "    for p in l1b:\n",
    "        k = timestamp_key(p)\n",
    "        if k in gmap:\n",
    "            pairs.append((p, gmap[k]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def acquisition_dt_from_name(name: str) -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    Parse '.Ayyyyddd.HHMM.' from a filename, returning an aware UTC datetime.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\.A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})\\.\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, doy, hh, mm = map(int, m.groups())\n",
    "    return datetime(year, 1, 1, tzinfo=timezone.utc) + timedelta(days=doy - 1, hours=hh, minutes=mm)\n",
    "\n",
    "\n",
    "# ============================= Planck inversion ===============================\n",
    "def planck_bt_from_radiance(L_um, lam_um: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Invert Planck’s law to obtain brightness temperature (Kelvin) from spectral radiance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    L_um : array-like\n",
    "        Spectral radiance in W m^-2 sr^-1 µm^-1.\n",
    "    lam_um : float\n",
    "        Effective wavelength in µm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray (float32)\n",
    "        Brightness temperature (K).\n",
    "    \"\"\"\n",
    "    h = 6.62607015e-34\n",
    "    c = 2.99792458e8\n",
    "    k = 1.380649e-23\n",
    "    lam = lam_um * 1e-6\n",
    "    K1 = (2 * h * c**2) / (lam**5) * 1e-6              # (W m^-2 sr^-1 µm^-1)\n",
    "    K2 = (h * c) / (k * lam)                            # (K)\n",
    "    L  = np.array(L_um, dtype=np.float64)\n",
    "    L  = np.clip(L, 1e-9, np.inf)                       # avoid division/log issues\n",
    "    return (K2 / np.log1p(K1 / L)).astype(np.float32)\n",
    "\n",
    "\n",
    "# ========================= Band read / decode (L1B) ===========================\n",
    "def read_band_rad_bt_and_mask(f: h5py.File, band: str, lam_um: float):\n",
    "    \"\"\"\n",
    "    Read radiance for a VIIRS band and compute BT. Also derive validity mask.\n",
    "\n",
    "    Logic\n",
    "    -----\n",
    "    • Scales/calibrates using scale_factor/add_offset.\n",
    "    • Invalid if equals _FillValue or any of flag_values.\n",
    "    • If <band>_quality_flags exists: require == 0 to be valid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (rad, bt, valid_mask, has_qc) or (None, None, None, False) if band missing.\n",
    "    \"\"\"\n",
    "    dset = f\"/observation_data/{band}\"\n",
    "    if dset not in f:\n",
    "        return None, None, None, False\n",
    "\n",
    "    v = f[dset]\n",
    "    si = v[...].astype(np.uint32)\n",
    "    scale = float(v.attrs[\"scale_factor\"])\n",
    "    offs  = float(v.attrs[\"add_offset\"])\n",
    "    fill  = int(v.attrs[\"_FillValue\"])\n",
    "    flag_vals = list(v.attrs.get(\"flag_values\", []))\n",
    "\n",
    "    valid = ~(si == fill)\n",
    "    for fv in flag_vals:\n",
    "        valid &= ~(si == fv)\n",
    "\n",
    "    qname  = f\"/observation_data/{band}_quality_flags\"\n",
    "    has_qc = qname in f\n",
    "    if has_qc:\n",
    "        q = f[qname][...]\n",
    "        valid &= (q == 0)\n",
    "\n",
    "    rad = np.full(si.shape, np.nan, dtype=np.float32)\n",
    "    rad[valid] = si[valid] * scale + offs\n",
    "    bt = planck_bt_from_radiance(rad, lam_um=lam_um)\n",
    "    return rad, bt, valid, has_qc\n",
    "\n",
    "\n",
    "# ========================== Geolocation / gridding ============================\n",
    "def read_geo(geo_path: Path) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Read swath latitude/longitude arrays and clamp to valid ranges.\"\"\"\n",
    "    with h5py.File(geo_path, \"r\") as g:\n",
    "        lat = g[\"/geolocation_data/latitude\"][...].astype(np.float32)\n",
    "        lon = g[\"/geolocation_data/longitude\"][...].astype(np.float32)\n",
    "    lat[(lat < -90) | (lat > 90)] = np.nan\n",
    "    lon[(lon < -180) | (lon > 180)] = np.nan\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def define_area_wgs84_intersection(lat: np.ndarray,\n",
    "                                   lon: np.ndarray,\n",
    "                                   aoi_geom,\n",
    "                                   res_deg: float = GRID_RES_DEG,\n",
    "                                   pad: float = 0.0):\n",
    "    \"\"\"\n",
    "    Define an EPSG:4326 AreaDefinition that is the intersection of the swath bounds and AOI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (area_def, transform, width, height) or None if no intersection.\n",
    "    \"\"\"\n",
    "    sw_lon_min, sw_lon_max = float(np.nanmin(lon)), float(np.nanmax(lon))\n",
    "    sw_lat_min, sw_lat_max = float(np.nanmin(lat)), float(np.nanmax(lat))\n",
    "    aoi_lon_min, aoi_lat_min, aoi_lon_max, aoi_lat_max = aoi_geom.bounds\n",
    "\n",
    "    lon_min = max(sw_lon_min, aoi_lon_min) - pad\n",
    "    lon_max = min(sw_lon_max, aoi_lon_max) + pad\n",
    "    lat_min = max(sw_lat_min, aoi_lat_min) - pad\n",
    "    lat_max = min(sw_lat_max, aoi_lat_max) + pad\n",
    "    if not (lon_min < lon_max and lat_min < lat_max):\n",
    "        return None\n",
    "\n",
    "    width  = int(np.ceil((lon_max - lon_min) / res_deg))\n",
    "    height = int(np.ceil((lat_max - lat_min) / res_deg))\n",
    "    transform = from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "\n",
    "    proj_dict = {\"proj\": \"longlat\", \"datum\": \"WGS84\"}  # pyresample longlat dictionary\n",
    "    area_def = geometry.AreaDefinition(\n",
    "        \"wgs84\", \"WGS84 latlon\", \"epsg4326\",\n",
    "        proj_dict, width, height,\n",
    "        (lon_min, lat_min, lon_max, lat_max)\n",
    "    )\n",
    "    return area_def, transform, width, height\n",
    "\n",
    "\n",
    "def resample_swath_to_grid(lat: np.ndarray,\n",
    "                           lon: np.ndarray,\n",
    "                           data: np.ndarray,\n",
    "                           area_def: geometry.AreaDefinition) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nearest-neighbor resampling of swath data to the target grid.\n",
    "    \"\"\"\n",
    "    swath_def = geometry.SwathDefinition(lons=lon, lats=lat)\n",
    "    out = kd_tree.resample_nearest(\n",
    "        swath_def, data, area_def,\n",
    "        radius_of_influence=5000,  # meters\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def write_geotiff(path: Path,\n",
    "                  arr: np.ndarray,\n",
    "                  transform,\n",
    "                  crs=CRS.from_epsg(4326),\n",
    "                  nodata=np.float32(np.nan),\n",
    "                  band_tags: dict | None = None,\n",
    "                  dtype=rasterio.float32) -> None:\n",
    "    \"\"\"\n",
    "    Write a single-band GeoTIFF with LZW compression and AOI nodata.\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\", \"height\": arr.shape[0], \"width\": arr.shape[1], \"count\": 1,\n",
    "        \"dtype\": dtype, \"crs\": crs, \"transform\": transform,\n",
    "        \"nodata\": nodata, \"compress\": \"lzw\", \"tiled\": True\n",
    "    }\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(arr.astype(profile[\"dtype\"]), 1)\n",
    "        if band_tags:\n",
    "            dst.update_tags(1, **band_tags)\n",
    "\n",
    "\n",
    "def append_log_l1b(log_csv: Path, row: dict) -> None:\n",
    "    \"\"\"\n",
    "    Append one record to the per-day processing_log.csv, creating headers if needed.\n",
    "    \"\"\"\n",
    "    exists = log_csv.exists()\n",
    "    with log_csv.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\n",
    "            \"l1b_file\",\"geo_file\",\"timestamp_key\",\n",
    "            \"acq_date_utc\",\"acq_time_utc\",\"acq_date_local\",\"acq_time_local\",\"local_tz\",\n",
    "            \"has_qc_I04\",\"has_qc_I05\",\"cloud_thresh_K\",\n",
    "            \"bt_I04_min\",\"bt_I04_max\",\"bt_I04_mean\",\n",
    "            \"bt_I05_min\",\"bt_I05_max\",\"bt_I05_mean\",\n",
    "            \"valid_px_I04\",\"valid_px_I05\"\n",
    "        ])\n",
    "        if not exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "\n",
    "# ============================== Per-day worker ================================\n",
    "def process_day(day: date, aoi) -> None:\n",
    "    \"\"\"\n",
    "    Process one day: read pairs, compute/rasterize I04/I05 radiance + BT,\n",
    "    apply cloud/AOI masks, write GeoTIFFs, append CSV log.\n",
    "    \"\"\"\n",
    "    daydir   = BASEDIR / day.strftime(\"%Y-%m-%d\")\n",
    "    rawdir   = daydir / \"raw\"\n",
    "    outbt    = daydir / \"bt\"\n",
    "    outbt.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    manifest = daydir / \"manifest_pairs.csv\"\n",
    "    log_csv  = daydir / \"processing_log.csv\"\n",
    "\n",
    "    if not rawdir.exists():\n",
    "        print(f\"[SKIP] No raw folder for {day}: {rawdir}\")\n",
    "        return\n",
    "\n",
    "    pairs = read_pairs_from_manifest(manifest, rawdir)\n",
    "    if not pairs:\n",
    "        print(f\"[SKIP] No L1B+GEO pairs for {day}.\")\n",
    "        return\n",
    "\n",
    "    tz = ZoneInfo(LOCAL_TZNAME)\n",
    "    print(f\"[INFO] {day} — processing {len(pairs)} pairs …\")\n",
    "\n",
    "    for l1b_p, geo_p in pairs:\n",
    "        # ---- Read band radiances + BT + validity ----\n",
    "        with h5py.File(l1b_p, \"r\") as f:\n",
    "            rad_i4, bt_i4, valid_i4, has_qc_i4 = read_band_rad_bt_and_mask(f, \"I04\", lam_um=LAM_I4_UM)\n",
    "            rad_i5, bt_i5, valid_i5, has_qc_i5 = read_band_rad_bt_and_mask(f, \"I05\", lam_um=LAM_I5_UM)\n",
    "\n",
    "        if bt_i4 is None and bt_i5 is None:\n",
    "            print(f\"[WARN] No I04/I05 in {Path(l1b_p).name}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # ---- GEO lat/lon ----\n",
    "        lat, lon = read_geo(geo_p)\n",
    "\n",
    "        # ---- Area / transform from swath∩AOI ----\n",
    "        area = define_area_wgs84_intersection(lat, lon, aoi, res_deg=GRID_RES_DEG)\n",
    "        if area is None:\n",
    "            print(\"[INFO] Swath does not intersect AOI; skipping L1B.\")\n",
    "            continue\n",
    "        area_def, transform, width, height = area\n",
    "\n",
    "        # ---- Cloud/invalid mask (from I05) ----\n",
    "        cloud_mask = None\n",
    "        if bt_i5 is not None:\n",
    "            cloud_mask = (bt_i5 < CLOUD_BT_K)\n",
    "            if valid_i5 is not None:\n",
    "                cloud_mask |= ~valid_i5\n",
    "\n",
    "        def apply_masks(arr: Optional[np.ndarray], valid: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "            \"\"\"Apply validity and cloud masks to a band array (in swath geometry).\"\"\"\n",
    "            if arr is None:\n",
    "                return None\n",
    "            out = arr.copy()\n",
    "            if valid is not None:\n",
    "                out[~valid] = np.nan\n",
    "            if cloud_mask is not None:\n",
    "                out[cloud_mask] = np.nan\n",
    "            return out\n",
    "\n",
    "        # Apply masks in swath space\n",
    "        rad_i4_c = apply_masks(rad_i4, valid_i4)\n",
    "        rad_i5_c = apply_masks(rad_i5, valid_i5)\n",
    "        bt_i4_c  = apply_masks(bt_i4,  valid_i4)\n",
    "        bt_i5_c  = apply_masks(bt_i5,  valid_i5)\n",
    "\n",
    "        stem = Path(l1b_p).with_suffix(\"\").name  # base name without \".h5\"\n",
    "\n",
    "        # ---- Resample to grid (only if present) ----\n",
    "        if rad_i4_c is not None:\n",
    "            rad_i4_g = resample_swath_to_grid(lat, lon, rad_i4_c, area_def)\n",
    "            bt_i4_g  = resample_swath_to_grid(lat, lon, bt_i4_c,  area_def)\n",
    "        if rad_i5_c is not None:\n",
    "            rad_i5_g = resample_swath_to_grid(lat, lon, rad_i5_c, area_def)\n",
    "            bt_i5_g  = resample_swath_to_grid(lat, lon, bt_i5_c,  area_def)\n",
    "\n",
    "        # ---- AOI mask on gridded arrays ----\n",
    "        mask = geometry_mask([mapping(aoi)], out_shape=(height, width), transform=transform, invert=True).astype(bool)\n",
    "\n",
    "        n_valid_i4 = n_valid_i5 = 0\n",
    "        i4_min = i4_max = i4_mean = np.nan\n",
    "        i5_min = i5_max = i5_mean = np.nan\n",
    "\n",
    "        # ---- Write I04 outputs + stats ----\n",
    "        if rad_i4_c is not None:\n",
    "            rad_i4_g = np.where(mask, rad_i4_g, np.nan)\n",
    "            out4r = outbt / f\"{stem}_I04_Rad.tif\"\n",
    "            write_geotiff(\n",
    "                out4r, rad_i4_g, transform,\n",
    "                band_tags={\"units\": \"W/m^2/sr/μm\", \"long_name\": \"VIIRS I04 Radiance\"}\n",
    "            )\n",
    "\n",
    "            bt_i4_g = np.where(mask, bt_i4_g, np.nan)\n",
    "            n_valid_i4 = int(np.count_nonzero(~np.isnan(bt_i4_g)))\n",
    "            if n_valid_i4 > 0:\n",
    "                i4_min = float(np.nanmin(bt_i4_g))\n",
    "                i4_max = float(np.nanmax(bt_i4_g))\n",
    "                i4_mean = float(np.nanmean(bt_i4_g))\n",
    "\n",
    "            out4b = outbt / f\"{stem}_I04_BT_K.tif\"\n",
    "            write_geotiff(\n",
    "                out4b, bt_i4_g, transform,\n",
    "                band_tags={\"units\": \"K\", \"long_name\": \"VIIRS I04 Brightness Temperature\"}\n",
    "            )\n",
    "            print(f\"[OK] wrote {out4r} and {out4b}\")\n",
    "\n",
    "        # ---- Write I05 outputs + stats ----\n",
    "        if rad_i5_c is not None:\n",
    "            rad_i5_g = np.where(mask, rad_i5_g, np.nan)\n",
    "            out5r = outbt / f\"{stem}_I05_Rad.tif\"\n",
    "            write_geotiff(\n",
    "                out5r, rad_i5_g, transform,\n",
    "                band_tags={\"units\": \"W/m^2/sr/μm\", \"long_name\": \"VIIRS I05 Radiance\"}\n",
    "            )\n",
    "\n",
    "            bt_i5_g = np.where(mask, bt_i5_g, np.nan)\n",
    "            n_valid_i5 = int(np.count_nonzero(~np.isnan(bt_i5_g)))\n",
    "            if n_valid_i5 > 0:\n",
    "                i5_min = float(np.nanmin(bt_i5_g))\n",
    "                i5_max = float(np.nanmax(bt_i5_g))\n",
    "                i5_mean = float(np.nanmean(bt_i5_g))\n",
    "\n",
    "            out5b = outbt / f\"{stem}_I05_BT_K.tif\"\n",
    "            write_geotiff(\n",
    "                out5b, bt_i5_g, transform,\n",
    "                band_tags={\"units\": \"K\", \"long_name\": \"VIIRS I05 Brightness Temperature\"}\n",
    "            )\n",
    "            print(f\"[OK] wrote {out5r} and {out5b}\")\n",
    "\n",
    "        # ---- Append log record ----\n",
    "        dt_utc = acquisition_dt_from_name(Path(l1b_p).name)\n",
    "        if dt_utc is not None:\n",
    "            dt_local = dt_utc.astimezone(tz)\n",
    "            acq_date_utc   = dt_utc.strftime(\"%Y-%m-%d\")\n",
    "            acq_time_utc   = dt_utc.strftime(\"%H:%M\")\n",
    "            acq_date_local = dt_local.strftime(\"%Y-%m-%d\")\n",
    "            acq_time_local = dt_local.strftime(\"%H:%M\")\n",
    "        else:\n",
    "            acq_date_utc = acq_time_utc = acq_date_local = acq_time_local = \"\"\n",
    "\n",
    "        append_log_l1b(log_csv, {\n",
    "            \"l1b_file\": str(l1b_p), \"geo_file\": str(geo_p),\n",
    "            \"timestamp_key\": timestamp_key(Path(l1b_p)),\n",
    "            \"acq_date_utc\": acq_date_utc, \"acq_time_utc\": acq_time_utc,\n",
    "            \"acq_date_local\": acq_date_local, \"acq_time_local\": acq_time_local, \"local_tz\": LOCAL_TZNAME,\n",
    "            \"has_qc_I04\": int(bool(valid_i4 is not None)),  # preserves original semantics\n",
    "            \"has_qc_I05\": int(bool(valid_i5 is not None)),\n",
    "            \"cloud_thresh_K\": CLOUD_BT_K,\n",
    "            \"bt_I04_min\": i4_min, \"bt_I04_max\": i4_max, \"bt_I04_mean\": i4_mean,\n",
    "            \"bt_I05_min\": i5_min, \"bt_I05_max\": i5_max, \"bt_I05_mean\": i5_mean,\n",
    "            \"valid_px_I04\": n_valid_i4, \"valid_px_I05\": n_valid_i5\n",
    "        })\n",
    "\n",
    "\n",
    "# ================================ Main =======================================\n",
    "def main() -> None:\n",
    "    \"\"\"Entry point: iterate date range and process each day.\"\"\"\n",
    "    aoi = read_aoi_wgs84(AOI_SHP)\n",
    "    for day in date_iter(START_DATE, END_DATE):\n",
    "        process_day(day, aoi)\n",
    "    print(\"[DONE] Range processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- viirs_process_af.py ---------------------------\n",
    "\"\"\"\n",
    "VIIRS Active Fire Processor (date range; FireMask-only detection)\n",
    "=================================================================\n",
    "\n",
    "What this script does\n",
    "---------------------\n",
    "• Processes all days in START_DATE..END_DATE (inclusive).\n",
    "• Expects dated folders created by a downloader:\n",
    "\n",
    "    BASEDIR/YYYY-MM-DD/\n",
    "        raw/\n",
    "        manifest_active_fire.csv   # produced by the downloader\n",
    "        af/                        # created here\n",
    "\n",
    "• For each AF granule, outputs to BASEDIR/YYYY-MM-DD/af/:\n",
    "    *_AF_DetectMask.tif   # float32, 1.0 where FireMask >= 7, NaN elsewhere (AOI-clipped)\n",
    "    *_AF_FireMask.tif     # float32 FireMask values (AOI-clipped), for reference\n",
    "\n",
    "Important\n",
    "---------\n",
    "• Detection uses FireMask ≥ 7 only.\n",
    "• When AF lacks internal geolocation, the paired GEO file path from the manifest is used.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "pip install h5py numpy rasterio pyresample fiona shapely pyproj tzdata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ============================== Standard libs ================================\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from typing import Optional, Iterable, Tuple, List\n",
    "\n",
    "# ============================== Third-party =================================\n",
    "import numpy as np\n",
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.features import geometry_mask\n",
    "from pyresample import geometry, kd_tree\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import CRS as PJCRS, Transformer\n",
    "\n",
    "\n",
    "# =============================== Configuration ===============================\n",
    "BASEDIR     = Path(r\"path\\to\\base\\directory\")\n",
    "START_DATE  = date(2025, 1, 1)\n",
    "END_DATE    = date(2025, 1, 11)\n",
    "\n",
    "# AOI shapefile (any CRS); used for intersection window + final clip\n",
    "AOI_SHP     = Path(r\"path\\to\\F002_L1__IR__L2L1M0__2025-01-10T215412.018348Z_2025-04-10T154832.806087Z_97706189_MWIR_Boundary.shp\")\n",
    "\n",
    "# Output grid resolution in degrees (EPSG:4326)\n",
    "GRID_RES_DEG = 0.0036  # ~400 m at equator\n",
    "\n",
    "\n",
    "# ================================ Utilities ==================================\n",
    "def date_iter(d0: date, d1: date) -> Iterable[date]:\n",
    "    \"\"\"Inclusive date iterator: d0, d0+1, …, d1.\"\"\"\n",
    "    cur = d0\n",
    "    while cur <= d1:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "\n",
    "def read_aoi_wgs84(shp_path: Path):\n",
    "    \"\"\"\n",
    "    Read an AOI shapefile, union geometries, return as WGS84 geometry.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shapely geometry in EPSG:4326\n",
    "    \"\"\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"AOI not found: {shp_path}\")\n",
    "    with fiona.open(shp_path, \"r\") as src:\n",
    "        geoms = [shape(feat[\"geometry\"]) for feat in src]\n",
    "        if not geoms:\n",
    "            raise ValueError(\"AOI shapefile has no geometries.\")\n",
    "        src_crs = src.crs\n",
    "    if not src_crs:\n",
    "        raise ValueError(\"AOI shapefile has no CRS.\")\n",
    "    aoi = unary_union(geoms).buffer(0)\n",
    "    src_crs_obj = PJCRS.from_user_input(src_crs)\n",
    "    wgs84 = PJCRS.from_epsg(4326)\n",
    "    if src_crs_obj != wgs84:\n",
    "        transformer = Transformer.from_crs(src_crs_obj, wgs84, always_xy=True)\n",
    "        aoi = shp_transform(lambda x, y, z=None: transformer.transform(x, y), aoi)\n",
    "    return aoi\n",
    "\n",
    "\n",
    "def find_dataset_path_case_insensitive(f: h5py.File, candidates: list[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return the first dataset path in 'f' whose basename loosely matches any candidate name.\n",
    "    Matching ignores case and non-alphanumerics.\n",
    "    \"\"\"\n",
    "    import re as _re\n",
    "\n",
    "    def _norm(s: str) -> str:\n",
    "        return _re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "\n",
    "    target = {_norm(c): c for c in candidates}\n",
    "    found: Optional[str] = None\n",
    "\n",
    "    def visitor(name, obj):\n",
    "        nonlocal found\n",
    "        if found is not None:\n",
    "            return\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            base = name.split(\"/\")[-1]\n",
    "            if _norm(base) in target:\n",
    "                found = name\n",
    "\n",
    "    f.visititems(visitor)\n",
    "    return found\n",
    "\n",
    "\n",
    "def read_af_latlon(af_path: Path, paired_geo: Optional[Path]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Read latitude/longitude from AF HDF5 if available; otherwise, from paired GEO.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (lat, lon) arrays or (None, None) if not available.\n",
    "    \"\"\"\n",
    "    with h5py.File(af_path, \"r\") as f:\n",
    "        has_latlon = (\"/geolocation_data/latitude\" in f) and (\"/geolocation_data/longitude\" in f)\n",
    "        if has_latlon:\n",
    "            lat = f[\"/geolocation_data/latitude\"][...].astype(np.float32)\n",
    "            lon = f[\"/geolocation_data/longitude\"][...].astype(np.float32)\n",
    "        else:\n",
    "            if paired_geo is None or not paired_geo.exists():\n",
    "                return None, None\n",
    "            with h5py.File(paired_geo, \"r\") as g:\n",
    "                lat = g[\"/geolocation_data/latitude\"][...].astype(np.float32)\n",
    "                lon = g[\"/geolocation_data/longitude\"][...].astype(np.float32)\n",
    "\n",
    "    # Clamp to valid ranges\n",
    "    lat[(lat < -90) | (lat > 90)] = np.nan\n",
    "    lon[(lon < -180) | (lon > 180)] = np.nan\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def read_firemask_only(af_path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Read FireMask (float32) from an AF product, handling varied dataset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    FireMask array (float32) with fill values set to NaN, or None if missing.\n",
    "    \"\"\"\n",
    "    candidates = [\"FireMask\", \"fire_mask\", \"fire mask\", \"FP_Mask\", \"Mask\"]\n",
    "    with h5py.File(af_path, \"r\") as f:\n",
    "        p_mask = find_dataset_path_case_insensitive(f, candidates)\n",
    "        if not p_mask:\n",
    "            return None\n",
    "        d = f[p_mask]\n",
    "        fm = d[...].astype(np.float32)\n",
    "        fm_fill = d.attrs.get(\"_FillValue\", None)\n",
    "        if fm_fill is not None:\n",
    "            fm[fm == fm_fill] = np.nan\n",
    "        return fm\n",
    "\n",
    "\n",
    "def _maybe_transpose_to_match(target_shape: tuple[int, int], arr: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    \"\"\"If 'arr' is 2D and transposing matches 'target_shape', return transposed.\"\"\"\n",
    "    if arr is None:\n",
    "        return None\n",
    "    if arr.shape == target_shape:\n",
    "        return arr\n",
    "    if arr.ndim == 2 and arr.T.shape == target_shape:\n",
    "        return arr.T\n",
    "    return arr\n",
    "\n",
    "\n",
    "def coerce_same_shape(lat: np.ndarray, lon: np.ndarray, *arrays: np.ndarray) -> Tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Ensure lat/lon and provided arrays share the same 2D shape.\n",
    "    • Transposes arrays that match lat.shape when transposed.\n",
    "    • Crops all to the minimal common (rows, cols).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (lat2, lon2, arrays2)\n",
    "    \"\"\"\n",
    "    arrays_t = [_maybe_transpose_to_match(lat.shape, a) for a in arrays]\n",
    "    shapes = [lat.shape, lon.shape] + [a.shape for a in arrays_t if a is not None]\n",
    "    if any(len(s) != 2 for s in shapes):\n",
    "        raise ValueError(\"All AF arrays must be 2D for resampling.\")\n",
    "    min_r = min(s[0] for s in shapes)\n",
    "    min_c = min(s[1] for s in shapes)\n",
    "\n",
    "    def crop(a: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "        return a[:min_r, :min_c] if (a is not None and a.shape != (min_r, min_c)) else a\n",
    "\n",
    "    lat2, lon2 = crop(lat), crop(lon)\n",
    "    arrays2 = [crop(a) for a in arrays_t]\n",
    "    return lat2, lon2, arrays2\n",
    "\n",
    "\n",
    "def define_area_wgs84_intersection(lat: np.ndarray,\n",
    "                                   lon: np.ndarray,\n",
    "                                   aoi_geom,\n",
    "                                   res_deg: float = GRID_RES_DEG,\n",
    "                                   pad: float = 0.0):\n",
    "    \"\"\"\n",
    "    Define a WGS84 (EPSG:4326) grid covering (swath ∩ AOI). Returns None if empty.\n",
    "    \"\"\"\n",
    "    sw_lon_min, sw_lon_max = float(np.nanmin(lon)), float(np.nanmax(lon))\n",
    "    sw_lat_min, sw_lat_max = float(np.nanmin(lat)), float(np.nanmax(lat))\n",
    "    aoi_lon_min, aoi_lat_min, aoi_lon_max, aoi_lat_max = aoi_geom.bounds\n",
    "\n",
    "    lon_min = max(sw_lon_min, aoi_lon_min) - pad\n",
    "    lon_max = min(sw_lon_max, aoi_lon_max) + pad\n",
    "    lat_min = max(sw_lat_min, aoi_lat_min) - pad\n",
    "    lat_max = min(sw_lat_max, aoi_lat_max) + pad\n",
    "    if not (lon_min < lon_max and lat_min < lat_max):\n",
    "        return None\n",
    "\n",
    "    width  = int(np.ceil((lon_max - lon_min) / res_deg))\n",
    "    height = int(np.ceil((lat_max - lat_min) / res_deg))\n",
    "    transform = from_bounds(lon_min, lat_min, lon_max, lat_max, width, height)\n",
    "\n",
    "    proj_dict = {\"proj\": \"longlat\", \"datum\": \"WGS84\"}  # pyresample expects dict\n",
    "    area_def = geometry.AreaDefinition(\n",
    "        \"wgs84\", \"WGS84 latlon\", \"epsg4326\",\n",
    "        proj_dict, width, height, (lon_min, lat_min, lon_max, lat_max)\n",
    "    )\n",
    "    return area_def, transform, width, height\n",
    "\n",
    "\n",
    "def resample_swath_to_grid(lat: np.ndarray,\n",
    "                           lon: np.ndarray,\n",
    "                           data: np.ndarray,\n",
    "                           area_def: geometry.AreaDefinition) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nearest-neighbor resampling of swath data to the target grid.\n",
    "    \"\"\"\n",
    "    swath_def = geometry.SwathDefinition(lons=lon, lats=lat)\n",
    "    out = kd_tree.resample_nearest(\n",
    "        swath_def, data, area_def,\n",
    "        radius_of_influence=5000,  # meters\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def write_geotiff(path: Path,\n",
    "                  arr: np.ndarray,\n",
    "                  transform,\n",
    "                  crs=CRS.from_epsg(4326),\n",
    "                  nodata=np.float32(np.nan),\n",
    "                  band_tags: dict | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Write a single-band GeoTIFF (float32) with LZW compression, tiled, with optional band tags.\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": arr.shape[0],\n",
    "        \"width\": arr.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": rasterio.float32,\n",
    "        \"crs\": crs,\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": nodata,\n",
    "        \"compress\": \"lzw\",\n",
    "        \"tiled\": True,\n",
    "    }\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(arr.astype(profile[\"dtype\"]), 1)\n",
    "        if band_tags:\n",
    "            dst.update_tags(1, **band_tags)\n",
    "\n",
    "\n",
    "# ============================== Per-day worker ================================\n",
    "def process_day(day: date, aoi) -> None:\n",
    "    \"\"\"\n",
    "    Process one calendar day of AF granules:\n",
    "    • Read manifest_active_fire.csv\n",
    "    • For each granule: read geolocation (internal or paired GEO), read FireMask,\n",
    "      compute detect mask (FireMask>=7), resample to AOI grid, AOI-clip, write GeoTIFFs.\n",
    "    \"\"\"\n",
    "    daydir    = BASEDIR / day.strftime(\"%Y-%m-%d\")\n",
    "    rawdir    = daydir / \"raw\"\n",
    "    afout_dir = daydir / \"af\"\n",
    "    afout_dir.mkdir(parents=True, exist_ok=True)\n",
    "    manifest_af = daydir / \"manifest_active_fire.csv\"\n",
    "\n",
    "    if not rawdir.exists():\n",
    "        print(f\"[SKIP] No raw folder for {day}: {rawdir}\")\n",
    "        return\n",
    "    if not manifest_af.exists():\n",
    "        print(f\"[SKIP] No AF manifest for {day}: {manifest_af}\")\n",
    "        return\n",
    "\n",
    "    with manifest_af.open(\"r\", newline=\"\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "    if not rows:\n",
    "        print(f\"[SKIP] Empty AF manifest for {day}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] {day} — processing {len(rows)} AF granules …\")\n",
    "    for row in rows:\n",
    "        product  = row[\"product\"]\n",
    "        af_path  = Path(row[\"file_path\"])\n",
    "        has_geo  = int(row[\"has_internal_geo\"]) == 1\n",
    "        geo_path = Path(row[\"paired_geo_path\"]) if row.get(\"paired_geo_path\") else None\n",
    "\n",
    "        if not af_path.exists():\n",
    "            print(f\"[WARN] AF file missing: {af_path}\")\n",
    "            continue\n",
    "\n",
    "        # --- Geolocation (internal or paired GEO) ---\n",
    "        lat, lon = read_af_latlon(af_path, geo_path if not has_geo else None)\n",
    "        if lat is None or lon is None:\n",
    "            print(f\"[WARN] No geolocation for AF {af_path.name}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- FireMask ---\n",
    "        firemask = read_firemask_only(af_path)\n",
    "        if firemask is None:\n",
    "            print(f\"[WARN] No FireMask in {af_path.name}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Detection rule: FireMask >= 7  (binary mask as float32)\n",
    "        fm_clean = firemask.copy()\n",
    "        det_mask = np.isfinite(fm_clean) & (fm_clean >= 7)\n",
    "\n",
    "        # Align shapes (lat/lon & arrays must match before resampling)\n",
    "        lat2, lon2, (det2, fm2) = coerce_same_shape(\n",
    "            lat, lon, det_mask.astype(np.float32), fm_clean\n",
    "        )\n",
    "\n",
    "        # --- Define output grid = (swath ∩ AOI) ---\n",
    "        area = define_area_wgs84_intersection(lat2, lon2, aoi, res_deg=GRID_RES_DEG)\n",
    "        if area is None:\n",
    "            print(\"[INFO] AF swath does not intersect AOI; skipping.\")\n",
    "            continue\n",
    "        area_def, transform, width, height = area\n",
    "\n",
    "        # --- Resample to grid ---\n",
    "        det_grid = resample_swath_to_grid(lat2, lon2, det2, area_def)\n",
    "        fm_grid  = resample_swath_to_grid(lat2, lon2, fm2,  area_def)\n",
    "\n",
    "        # --- AOI mask/clip ---\n",
    "        mask_aoi = geometry_mask([mapping(aoi)], out_shape=(height, width), transform=transform, invert=True).astype(bool)\n",
    "        det_grid = np.where(mask_aoi, det_grid, np.nan)\n",
    "        fm_grid  = np.where(mask_aoi, fm_grid,  np.nan)\n",
    "\n",
    "        # --- Write outputs ---\n",
    "        stem    = af_path.with_suffix(\"\").name\n",
    "        out_det = afout_dir / f\"{stem}_AF_DetectMask.tif\"\n",
    "        out_fm  = afout_dir / f\"{stem}_AF_FireMask.tif\"\n",
    "\n",
    "        write_geotiff(\n",
    "            out_det, det_grid, transform,\n",
    "            band_tags={\"units\": \"binary\", \"long_name\": f\"{product} Detection Mask (1=FireMask>=7)\"}\n",
    "        )\n",
    "        print(f\"[OK] wrote {out_det}\")\n",
    "\n",
    "        write_geotiff(\n",
    "            out_fm, fm_grid, transform,\n",
    "            band_tags={\"units\": \"class\", \"long_name\": f\"{product} FireMask\"}\n",
    "        )\n",
    "        print(f\"[OK] wrote {out_fm}\")\n",
    "\n",
    "\n",
    "# ================================== Main =====================================\n",
    "def main() -> None:\n",
    "    aoi = read_aoi_wgs84(AOI_SHP)\n",
    "    for day in date_iter(START_DATE, END_DATE):\n",
    "        process_day(day, aoi)\n",
    "    print(\"[DONE] AF range processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
